diff --git b/Makefile a/Makefile
index 57d918069..4da396560 100644
--- b/Makefile
+++ a/Makefile
@@ -6,6 +6,7 @@ CORE_IMAGES=./cmd/activator ./cmd/autoscaler ./cmd/autoscaler-hpa ./cmd/controll
 TEST_IMAGES=$(shell find ./test/test_images ./test/test_images/multicontainer -mindepth 1 -maxdepth 1 -type d)
 # Exclude wrapper images like multicontainer and initcontainers as those are just ko convenience wrappers used upstream. The openshift serverless tests use other images to run.
 TEST_IMAGES_WITHOUT_WRAPPERS=$(shell find ./test/test_images ./test/test_images/multicontainer -mindepth 1 -maxdepth 1 -type d -not -name multicontainer -not -name initcontainers)
+PERF_IMAGES=$(shell find ./test/performance/benchmarks -mindepth 1 -maxdepth 1)
 BRANCH=
 TEST=
 IMAGE=
@@ -26,6 +27,12 @@ test-install:
 	done
 .PHONY: test-install

+perf-install:
+	for img in $(PERF_IMAGES); do \
+		go install $$img ; \
+	done
+.PHONY: perf-install
+
 test-e2e:
 	./openshift/e2e-tests.sh
 .PHONY: test-e2e
@@ -64,8 +71,9 @@ test-e2e-local:
 
 # Generate Dockerfiles for core and test images used by ci-operator. The files need to be committed manually.
 generate-dockerfiles:
-	./openshift/ci-operator/generate-dockerfiles.sh openshift/ci-operator/knative-images $(CORE_IMAGES)
-	./openshift/ci-operator/generate-dockerfiles.sh openshift/ci-operator/knative-test-images $(TEST_IMAGES_WITHOUT_WRAPPERS)
+	./openshift/ci-operator/generate-dockerfiles.sh openshift/ci-operator/knative-images $(CORE_IMAGES); \
+	./openshift/ci-operator/generate-dockerfiles.sh openshift/ci-operator/knative-test-images $(TEST_IMAGES_WITHOUT_WRAPPERS); \
+    ./openshift/ci-operator/generate-dockerfiles.sh openshift/ci-operator/knative-perf-images $(PERF_IMAGES)
 .PHONY: generate-dockerfiles
 
 # Generate an aggregated knative yaml file with replaced image references
diff --git a/hack/tools.go b/hack/tools.go
index 13464e22b..5cc2e3fbe 100644
--- a/hack/tools.go
+++ b/hack/tools.go
@@ -34,9 +34,6 @@ import (
 	// Migration job.
 	_ "knative.dev/pkg/apiextensions/storageversion/cmd/migrate"
 
-	// Mako stub
-	_ "knative.dev/pkg/test/mako/stub-sidecar"
-
 	_ "k8s.io/code-generator/cmd/client-gen"
 	_ "k8s.io/code-generator/cmd/deepcopy-gen"
 	_ "k8s.io/code-generator/cmd/defaulter-gen"
diff --git a/openshift/ci-operator/Dockerfile.in b/openshift/ci-operator/Dockerfile.in
index 1c31865cc..7bd47161b 100644
--- a/openshift/ci-operator/Dockerfile.in
+++ b/openshift/ci-operator/Dockerfile.in
@@ -2,7 +2,7 @@
 FROM registry.ci.openshift.org/openshift/release:golang-1.21 as builder

 COPY . .
-RUN make install test-install
+RUN make install test-install perf-install
 
 FROM registry.access.redhat.com/ubi8/ubi-minimal
 USER 65532
diff --git a/openshift/ci-operator/Dockerfile_with_kodata.in b/openshift/ci-operator/Dockerfile_with_kodata.in
index 9d84e5ecc..572d91c25 100644
--- a/openshift/ci-operator/Dockerfile_with_kodata.in
+++ b/openshift/ci-operator/Dockerfile_with_kodata.in
@@ -2,7 +2,7 @@
 FROM registry.ci.openshift.org/openshift/release:golang-1.21 as builder

 COPY . .
-RUN make install test-install
+RUN make install test-install perf-install
 
 FROM registry.access.redhat.com/ubi8/ubi-minimal
 USER 65532
diff --git a/test/performance/profiling.md b/test/performance/PROFILING.md
similarity index 100%
rename from test/performance/profiling.md
rename to test/performance/PROFILING.md
diff --git a/test/performance/README.md b/test/performance/README.md
index a59bced77..f17f7cffc 100644
--- a/test/performance/README.md
+++ b/test/performance/README.md
@@ -3,7 +3,7 @@
 Knative performance tests are tests geared towards producing useful performance
 metrics of the knative system. As such they can choose to take a closed-box
 point-of-view of the system and use it just like an end-user might see it. They
-can also go more open-boxy to narrow down the components under test.
+can also go more open-box to narrow down the components under test.
 
 ## Load Generator
 
@@ -15,32 +15,156 @@ different rate, you can write your own pacer by implementing
 interface. Custom pacer implementations used in Knative tests are under
 [pacers](https://github.com/knative/pkg/tree/main/test/vegeta/pacers).
 
-## Benchmarking using Mako
 
-The benchmarks were originally built to use [mako](https://github.com/google/mako), but currently
-running without connecting to the Mako backend, and collecting the data using
-a Mako sidecar stub.
+## Testing architecture
 
-### Run without Mako
+The performance tests are based on Kubernetes Jobs running Golang code based on different [benchmarks](#benchmarks).
+The script [performance-tests.sh](./performance-tests.sh) first creates a cluster in GKE, installs Serving with specific settings
+for the performance tests. Then it installs the required Knative Services and runs the testing jobs.
 
-To run a benchmark once, and use the result from `mako-stub` for plotting:
+The results are written to:
+* Stdout
+* A logfile in a folder defined in env: `$ARTIFACTS`
+* To an InfluxDB hosted in the knative-community GKE project: https://github.com/knative/infra/tree/main/infra/k8s/shared
 
-1. Start the benchmarking job:
 
-   `ko apply -f test/performance/benchmarks/deployment-probe/continuous/benchmark-direct.yaml`
+## Grafana
 
-1. Wait until all the pods with the selector equal to the job name are completed.
+To better visualize the test results, Grafana is used to show the results in a dashboard.
+The dashboard is defined in [grafana-dashboard.json](./visualization/grafana-dashboard.json) and
+hosted on [grafana.knative.dev](https://grafana.knative.dev/d/igHJ5-fdk/knative-serving-performance-tests?orgId=1)
 
-1. Retrieve results from mako-stub using the script in where `pod_name` is the name of the pod from the previous step.
 
-  `read_results.sh "$pod_name" "$pod_namespace" ${mako_port:-10001} ${timeout:-120} ${retries:-100} ${retries_interval:-10} "$output_file"`
+## Benchmarks
 
-   This will download a CSV with all raw results. Alternatively you can remove
-   the port argument `-p` in `mako-stub` container to dump the output to
-   container log directly.
+Knative Serving has different benchmarking scenarios:
 
-**Note:** Running `performance-tests-mako.sh` creates a cluster and runs all the benchmarks in sequence. Results are downloaded in a temp folder
+* [dataplane-probe](./benchmarks/dataplane-probe): Measures the overhead Knative has compared to a `Deployment`
+* [load-test](./benchmarks/load-test): Measures request metrics for Knative Services under load in different scenarios (Activator always in path, Activator only in path at zero, Activator moving out of path on high-load)
+* [real-traffic-test](./benchmarks/real-traffic-test): Simulates realistic traffic with random request latency, service startup latency and payload sizes.
+* [reconciliation-delay](./benchmarks/reconciliation-delay): Measures the time it takes to reconcile a `KnativeService` and its child CRs.
+* [rollout-probe](./benchmarks/rollout-probe): Measures request metrics for a rolling update of a scaled `KnativeService`.
+* [scale-from-zero](./benchmarks/scale-from-zero): Measures the latency of scaling 1, 5, 25 and 100 Knative Services from zero in parallel.
 
-### Benchmarking using Kperf
 
-Running `performance-tests.sh` runs performance tests using [kperf](https://github.com/knative-sandbox/kperf)
+## Running the benchmarks
+
+### Local InfluxDB setup
+
+You first need a local running instance of InfluxDB.
+
+Note: if you don't have or don't want to use helm, you can also [install InfluxDB with YAMLs](https://docs.influxdata.com/influxdb/v2.7/install/?t=Kubernetes).
+
+```bash
+# Create an InfluxDB with helm
+helm repo add influxdata https://helm.influxdata.com/
+kubectl create ns influx
+helm upgrade --install -n influx local-influx --set persistence.enabled=true,persistence.size=50Gi influxdata/influxdb2
+echo "Admin password"
+echo $(kubectl get secret local-influx-influxdb2-auth -o "jsonpath={.data['admin-password']}" --namespace influx | base64 --decode)
+echo "Admin token"
+echo $(kubectl get secret local-influx-influxdb2-auth -o "jsonpath={.data['admin-token']}" --namespace influx | base64 --decode)
+
+# Forward the InfluxDB service to your laptop if you want to access the UI:
+kubectl port-forward -n influx svc/local-influx-influxdb2 8080:80
+
+# Set up the expected influxdb config
+export INFLUX_URL=http://localhost:8080
+export INFLUX_TOKEN=$(kubectl get secret local-influx-influxdb2-auth -o "jsonpath={.data['admin-token']}" --namespace influx | base64 --decode)
+
+# Run the script to initialize the Organization + Buckets in InfluxDB
+./visualization/setup-influx-db.sh
+```
+
+### Elastic Search / Opensearch
+
+```bash
+helm repo add opensearch https://opensearch-project.github.io/helm-charts/
+helm repo update
+helm search repo opensearch
+helm install my-deployment opensearch/opensearch
+helm install dashboards opensearch/opensearch-dashboards
+
+export ES_URL=https://localhost:9200
+oc port-forward svc/opensearch-cluster-master 9200:9200
+export ES_USERNAME=admin
+export ES_PASSWORD=admin
+
+# Creates an index template
+./test/performance/visualization/setup-es-index.sh
+
+# Sample log entry to create:
+
+curl -u elastic:Y*A_Ce=+0wbsV8C-b+u* -k -X POST "https://localhost:9200/knative-serving-data-plane/_doc" -H 'Content-Type: application/json' -d'
+{
+  "@timestamp": "2023-09-12T11:23:23+03:00",
+  "_measurement": "Knative Serving dataplane probe",
+  "activator-pod-count": "2",
+  "tags": [{"JOB_NAME": "custom", "t21":"t2"}]
+}
+'
+
+SYSTEM_NAMESPACE=knative-serving
+JOB_NAME="local"
+BUILD_ID="local"
+USE_OPEN_SEARCH=true
+export ES_URL=https://admin:admin@opensearch-cluster-master.default.svc.cluster.local:9200
+
+
+kubectl create secret generic performance-test-config -n "default" \
+  --from-literal=esurl="${ES_URL}" \
+  --from-literal=esusername="${ES_USERNAME}" \
+  --from-literal=espassword="${ES_PASSWORD}" \
+  --from-literal=jobname="${JOB_NAME}" \
+  --from-literal=buildid="${BUILD_ID}"
+
+ko apply -f ./test/performance/benchmarks/dataplane-probe/dataplane-probe-setup.yaml
+sed "s|@SYSTEM_NAMESPACE@|$SYSTEM_NAMESPACE|g" ./test/performance/benchmarks/dataplane-probe/dataplane-probe-deployment.yaml | sed "s|@KO_DOCKER_REPO@|$KO_DOCKER_REPO|g" | sed "s|@USE_OPEN_SEARCH@|\"$USE_OPEN_SEARCH\"|g" | sed "s|@USE_ES@|'false'|g" | ko apply --sbom=none -Bf -
+```
+
+### Local grafana dashboards
+
+Use an existing grafana instance or create one on your cluster, [see docs](https://grafana.com/docs/grafana/latest/setup-grafana/installation/kubernetes/).
+
+To use our InfluxDB as a datasource for Grafana
+* Navigate to Grafana UI and log in using the user from the installation
+* Create a new datasource for InfluxDB
+* Select the flux query language
+* Server-URL: http://local-influx-influxdb2.influx:80  (Note: this could be different if your grafana instance is hosted outside the cluster)
+* Organization: Knativetest
+* Bucket: knative-serving
+* Token: <your influx-db token>
+
+
+### Local development
+
+You can run all the benchmarks directly by calling the `main()` method in `main.go` in the respective [benchmarks](./benchmarks) folders.
+
+#### Environment and Secret
+
+The tests expect to be configured with certain environment variables:
+
+* KO_DOCKER_REPO = What you have set for `ko`
+* SYSTEM_NAMESPACE = Where knative-serving is installed, typically `knative-serving`
+* INFLUX_URL = http://local-influx-influxdb2.influx:80
+* INFLUX_TOKEN = as outputted from the command above
+* BUILD_ID=local
+* JOB_NAME=local
+
+and a `Secret` in the `default` namespace with the following contents:
+
+```bash
+kubectl create secret generic performance-test-config -n default \
+  --from-literal=influxurl="${INFLUX_URL}" \
+  --from-literal=influxtoken="${INFLUX_TOKEN}" \
+  --from-literal=jobname="${JOB_NAME}" \
+  --from-literal=buildid="${BUILD_ID}"
+```
+
+### Running them on cluster
+
+Check out what the [script](./performance-tests.sh) does. Basically just run:
+
+```bash
+  envsubst < your-benchmark-job.yaml | ko apply --sbom=none -Bf -
+```
diff --git a/test/performance/benchmarks/dataplane-probe/continuous/dataplane-probe-direct.yaml b/test/performance/benchmarks/dataplane-probe/continuous/dataplane-probe-direct.yaml
deleted file mode 100644
index aa3fe6e09..000000000
--- a/test/performance/benchmarks/dataplane-probe/continuous/dataplane-probe-direct.yaml
+++ /dev/null
@@ -1,201 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
----
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: prober
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: prober
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: prober
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: dataplane-probe-deployment
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      restartPolicy: Never
-      containers:
-        - name: dataplane-probe
-          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe/continuous
-          args: ["-target=deployment", "--duration=3m"]
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-        - name: config-mako
-          configMap:
-            name: config-mako
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: dataplane-probe-istio
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      restartPolicy: Never
-      containers:
-        - name: dataplane-probe
-          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe/continuous
-          args: ["-target=istio", "--duration=3m"]
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-        - name: config-mako
-          configMap:
-            name: config-mako
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: dataplane-probe-queue
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      restartPolicy: Never
-      containers:
-        - name: dataplane-probe
-          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe/continuous
-          args: ["-target=queue", "--duration=3m"]
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-        - name: config-mako
-          configMap:
-            name: config-mako
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: dataplane-probe-activator
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      restartPolicy: Never
-      containers:
-        - name: dataplane-probe
-          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe/continuous
-          args: ["-target=activator", "--duration=3m"]
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-        - name: config-mako
-          configMap:
-            name: config-mako
diff --git a/test/performance/benchmarks/dataplane-probe/continuous/dataplane-probe-setup.yaml b/test/performance/benchmarks/dataplane-probe/continuous/dataplane-probe-setup.yaml
deleted file mode 100644
index c124f0afd..000000000
--- a/test/performance/benchmarks/dataplane-probe/continuous/dataplane-probe-setup.yaml
+++ /dev/null
@@ -1,228 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: activator
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "5"
-        autoscaling.knative.dev/maxScale: "5"
-        # Always hook the activator in.
-        autoscaling.knative.dev/targetBurstCapacity: "-1"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 0 # Explicitly set the default, since it might be overridden in CM.
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: activator-with-cc
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "5"
-        autoscaling.knative.dev/maxScale: "5"
-        # Always hook the activator in.
-        autoscaling.knative.dev/targetBurstCapacity: "-1"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 100
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: activator-with-cc-10
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "15"
-        autoscaling.knative.dev/maxScale: "15"
-        # Always hook the activator in.
-        autoscaling.knative.dev/targetBurstCapacity: "-1"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 10
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: activator-with-cc-1
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "150"
-        autoscaling.knative.dev/maxScale: "150"
-        # Always hook the activator in.
-        autoscaling.knative.dev/targetBurstCapacity: "-1"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 1
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: queue-proxy
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "5"
-        autoscaling.knative.dev/maxScale: "5"
-        # Only hook the activator in when scaled to zero.
-        autoscaling.knative.dev/targetBurstCapacity: "0"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 0 # Explicitly set the default, since it might be overridden in CM.
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: queue-proxy-with-cc
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "5"
-        autoscaling.knative.dev/maxScale: "5"
-        # Only hook the activator in when scaled to zero.
-        autoscaling.knative.dev/targetBurstCapacity: "0"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 100
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: queue-proxy-with-cc-10
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "15"
-        autoscaling.knative.dev/maxScale: "15"
-        # Only hook the activator in when scaled to zero.
-        autoscaling.knative.dev/targetBurstCapacity: "0"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 10
----
-apiVersion: serving.knative.dev/v1
-kind: Service
-metadata:
-  name: queue-proxy-with-cc-1
-  namespace: default
-spec:
-  template:
-    metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "150"
-        autoscaling.knative.dev/maxScale: "150"
-        # Only hook the activator in when scaled to zero.
-        autoscaling.knative.dev/targetBurstCapacity: "0"
-    spec:
-      containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-      containerConcurrency: 1
----
-apiVersion: v1
-kind: Service
-metadata:
-  name: deployment
-  namespace: default
-spec:
-  ports:
-    - port: 80
-      targetPort: 8080
-  selector:
-    app: app1
----
-apiVersion: apps/v1
-kind: Deployment
-metadata:
-  name: deployment
-  namespace: default
-spec:
-  replicas: 5
-  selector:
-    matchLabels:
-      app: app1
-  template:
-    metadata:
-      labels:
-        app: app1
-    spec:
-      containers:
-      - name: user-container
-        image: ko://knative.dev/serving/test/test_images/autoscale
-        ports:
-        - name: app1
-          containerPort: 8080
----
-apiVersion: v1
-kind: Service
-metadata:
-  name: istio
-  namespace: default
-spec:
-  externalName: istio-ingressgateway.istio-system.svc.cluster.local
-  sessionAffinity: None
-  type: ExternalName
----
-apiVersion: networking.istio.io/v1alpha3
-kind: VirtualService
-metadata:
-  name: dataplane-probe
-  namespace: default
-spec:
-  gateways:
-  - knative-ingress-gateway.knative-serving.svc.cluster.local
-  hosts:
-  - istio.default
-  - istio.default.svc
-  - istio.default.svc.cluster.local
-  http:
-  - match:
-    - authority:
-        regex: ^istio\.default(?::\d{1,5})?$
-    - authority:
-        regex: ^istio\.default\.svc(?::\d{1,5})?$
-    - authority:
-        regex: ^istio\.default\.svc\.cluster\.local(?::\d{1,5})?$
-    route:
-    - destination:
-        host: deployment.default.svc.cluster.local
-        port:
-          number: 80
-      weight: 100
diff --git a/test/performance/benchmarks/dataplane-probe/continuous/kodata/dev.config b/test/performance/benchmarks/dataplane-probe/continuous/kodata/dev.config
deleted file mode 120000
index ee953f22c..000000000
--- a/test/performance/benchmarks/dataplane-probe/continuous/kodata/dev.config
+++ /dev/null
@@ -1 +0,0 @@
-../../dev.config
\ No newline at end of file
diff --git a/test/performance/benchmarks/dataplane-probe/continuous/main.go b/test/performance/benchmarks/dataplane-probe/continuous/main.go
deleted file mode 100644
index a7279f349..000000000
--- a/test/performance/benchmarks/dataplane-probe/continuous/main.go
+++ /dev/null
@@ -1,147 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"context"
-	"flag"
-	"log"
-	"time"
-
-	vegeta "github.com/tsenart/vegeta/v12/lib"
-
-	"knative.dev/pkg/signals"
-	"knative.dev/pkg/system"
-	"knative.dev/pkg/test/mako"
-
-	"knative.dev/serving/test/performance"
-	"knative.dev/serving/test/performance/metrics"
-
-	_ "knative.dev/pkg/system/testing"
-)
-
-const (
-	benchmarkName = "Development - Serving dataplane probe"
-)
-
-var (
-	target   = flag.String("target", "", "The target to attack.")
-	duration = flag.Duration("duration", 5*time.Minute, "The duration of the probe")
-)
-
-func main() {
-	flag.Parse()
-
-	// We want this for properly handling Kubernetes container lifecycle events.
-	ctx := signals.NewContext()
-
-	// We cron quite often, so make sure that we don't severely overrun to
-	// limit how noisy a neighbor we can be.
-	ctx, cancel := context.WithTimeout(ctx, *duration+time.Minute)
-	defer cancel()
-
-	// Use the benchmark key created
-	mc, err := mako.Setup(ctx)
-	if err != nil {
-		log.Fatal("Failed to setup mako: ", err)
-	}
-	q, qclose, ctx := mc.Quickstore, mc.ShutDownFunc, mc.Context
-	// Use a fresh context here so that our RPC to terminate the sidecar
-	// isn't subject to our timeout (or we won't shut it down when we time out)
-	defer qclose(context.Background())
-
-	// Wrap fatalf in a helper or our sidecar will live forever.
-	fatalf := func(f string, args ...interface{}) {
-		qclose(context.Background())
-		log.Fatalf(f, args...)
-	}
-
-	// Validate flags after setting up "fatalf" or our sidecar will run forever.
-	if *target == "" {
-		fatalf("Missing flag: -target")
-	}
-
-	// Based on the "target" flag, load up our target benchmark.
-	// We only run one variation per run to avoid the runs being noisy neighbors,
-	// which in early iterations of the benchmark resulted in latency bleeding
-	// across the different workload types.
-	t, ok := targets[*target]
-	if !ok {
-		fatalf("Unrecognized target: %s", *target)
-	}
-
-	// Make sure the target is ready before sending the large amount of requests.
-	if err := performance.ProbeTargetTillReady(t.target.URL, *duration); err != nil {
-		fatalf("Failed to get target ready for attacking: %v", err)
-	}
-
-	// Set up the threshold analyzers for the selected benchmark.  This will
-	// cause Mako/Quickstore to analyze the results we are storing and flag
-	// things that are outside of expected bounds.
-	q.Input.ThresholdInputs = append(q.Input.ThresholdInputs, t.analyzers...)
-
-	// Send 1000 QPS (1 per ms) for the given duration with a 30s request timeout.
-	rate := vegeta.Rate{Freq: 1, Per: time.Millisecond}
-	targeter := vegeta.NewStaticTargeter(t.target)
-	attacker := vegeta.NewAttacker(vegeta.Timeout(30 * time.Second))
-
-	// Create a new aggregateResult to accumulate the results.
-	ar := metrics.NewAggregateResult(int(duration.Seconds()))
-
-	// Start the attack!
-	results := attacker.Attack(targeter, rate, *duration, "load-test")
-	deploymentStatus := metrics.FetchDeploymentStatus(ctx, system.Namespace(), "activator", time.Second)
-LOOP:
-	for {
-		select {
-		case <-ctx.Done():
-			// If we timeout or the pod gets shutdown via SIGTERM then start to
-			// clean thing up.
-			break LOOP
-
-		case ds := <-deploymentStatus:
-			// Report number of ready activators.
-			q.AddSamplePoint(mako.XTime(ds.Time), map[string]float64{
-				"ap": float64(ds.ReadyReplicas),
-			})
-			performance.AddInfluxPoint(benchmarkName, map[string]interface{}{"ap": float64(ds.ReadyReplicas)})
-		case res, ok := <-results:
-			if !ok {
-				// Once we have read all of the request results, break out of
-				// our loop.
-				break LOOP
-			}
-			// Handle the result for this request
-			metrics.HandleResult(q, benchmarkName, *res, t.stat, ar)
-		}
-	}
-
-	// Walk over our accumulated per-second error rates and report them as
-	// sample points.  The key is seconds since the Unix epoch, and the value
-	// is the number of errors observed in that second.
-	for ts, count := range ar.ErrorRates {
-		q.AddSamplePoint(mako.XTime(time.Unix(ts, 0)), map[string]float64{
-			t.estat: float64(count),
-		})
-		performance.AddInfluxPoint(benchmarkName, map[string]interface{}{t.estat: float64(count)})
-	}
-
-	// Commit data to Mako and handle the result.
-	if err := mc.StoreAndHandleResult(); err != nil {
-		fatalf("Failed to store and handle benchmarking result: %v", err)
-	}
-}
diff --git a/test/performance/benchmarks/dataplane-probe/continuous/sla.go b/test/performance/benchmarks/dataplane-probe/continuous/sla.go
deleted file mode 100644
index 37e254c21..000000000
--- a/test/performance/benchmarks/dataplane-probe/continuous/sla.go
+++ /dev/null
@@ -1,190 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"net/http"
-	"time"
-
-	tpb "github.com/google/mako/clients/proto/analyzers/threshold_analyzer_go_proto"
-	mpb "github.com/google/mako/spec/proto/mako_go_proto"
-	vegeta "github.com/tsenart/vegeta/v12/lib"
-	"knative.dev/pkg/ptr"
-	"knative.dev/pkg/test/mako"
-)
-
-var (
-	minDefault = 100 * time.Millisecond
-)
-
-// This function constructs an analyzer that validates the p95 aggregate value of the given metric.
-func new95PercentileLatency(name, valueKey string, min, max time.Duration) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String(name),
-		Configs: []*tpb.ThresholdConfig{{
-			Min: bound(min),
-			Max: bound(max),
-			DataFilter: &mpb.DataFilter{
-				DataType:            mpb.DataFilter_METRIC_AGGREGATE_PERCENTILE.Enum(),
-				PercentileMilliRank: ptr.Int32(95000),
-				ValueKey:            ptr.String(valueKey),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10),
-	}
-}
-
-// This analyzer validates that the p95 latency talking to pods through a Kubernetes
-// Service falls in the +5ms range.  This does not have Knative or Istio components
-// on the dataplane, and so it is intended as a canary to flag environmental
-// problems that might be causing contemporaneous Knative or Istio runs to fall out of SLA.
-func newKubernetes95PercentileLatency(valueKey string) *tpb.ThresholdAnalyzerInput {
-	return new95PercentileLatency("Kubernetes baseline", valueKey, minDefault, 105*time.Millisecond)
-}
-
-// This analyzer validates that the p95 latency talking to pods through Istio
-// falls in the +8ms range.  This does not actually have Knative components
-// on the dataplane, and so it is intended as a canary to flag environmental
-// problems that might be causing contemporaneous Knative runs to fall out of SLA.
-func newIstio95PercentileLatency(valueKey string) *tpb.ThresholdAnalyzerInput {
-	return new95PercentileLatency("Istio baseline", valueKey, minDefault, 108*time.Millisecond)
-}
-
-// This analyzer validates that the p95 latency hitting a Knative Service
-// going through JUST the queue-proxy falls in the +10ms range.
-func newQueue95PercentileLatency(valueKey string) *tpb.ThresholdAnalyzerInput {
-	return new95PercentileLatency("Queue p95 latency", valueKey, minDefault, 110*time.Millisecond)
-}
-
-// This analyzer validates that the p95 latency hitting a Knative Service
-// going through BOTH the activator and queue-proxy falls in the +10ms range.
-func newActivator95PercentileLatency(valueKey string) *tpb.ThresholdAnalyzerInput {
-	return new95PercentileLatency("Activator p95 latency", valueKey, minDefault, 110*time.Millisecond)
-}
-
-var (
-	// Map the above to our benchmark targets.
-	targets = map[string]struct {
-		target    vegeta.Target
-		stat      string
-		estat     string
-		analyzers []*tpb.ThresholdAnalyzerInput
-	}{
-		"deployment": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://deployment.default.svc.cluster.local?sleep=100",
-			},
-			stat:      "kd",
-			estat:     "ke",
-			analyzers: []*tpb.ThresholdAnalyzerInput{newKubernetes95PercentileLatency("kd")},
-		},
-		"istio": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://istio.default.svc.cluster.local?sleep=100",
-			},
-			stat:      "id",
-			estat:     "ie",
-			analyzers: []*tpb.ThresholdAnalyzerInput{newIstio95PercentileLatency("id")},
-		},
-		"queue": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://queue-proxy.default.svc.cluster.local?sleep=100",
-			},
-			stat:      "qp",
-			estat:     "qe",
-			analyzers: []*tpb.ThresholdAnalyzerInput{newQueue95PercentileLatency("qp")},
-		},
-		"queue-with-cc": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://queue-proxy-with-cc.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "qc",
-			estat: "re",
-			// We use the same threshold analyzer, since we want Breaker to exert minimal latency impact.
-			analyzers: []*tpb.ThresholdAnalyzerInput{newQueue95PercentileLatency("qc")},
-		},
-		"queue-with-cc-10": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://queue-proxy-with-cc-10.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "qct",
-			estat: "ret",
-			// TODO(vagababov): determine values here.
-			analyzers: []*tpb.ThresholdAnalyzerInput{},
-		},
-		"queue-with-cc-1": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://queue-proxy-with-cc-1.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "qc1",
-			estat: "re1",
-			// TODO(vagababov): determine values here.
-			analyzers: []*tpb.ThresholdAnalyzerInput{},
-		},
-		"activator": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator.default.svc.cluster.local?sleep=100",
-			},
-			stat:      "a",
-			estat:     "ae",
-			analyzers: []*tpb.ThresholdAnalyzerInput{newActivator95PercentileLatency("a")},
-		},
-		"activator-with-cc": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator-with-cc.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "ac",
-			estat: "be",
-			// We use the same threshold analyzer, since we want Throttler/Breaker to exert minimal latency impact.
-			analyzers: []*tpb.ThresholdAnalyzerInput{newActivator95PercentileLatency("ac")},
-		},
-		"activator-with-cc-10": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator-with-cc-10.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "act",
-			estat: "bet",
-			// TODO(vagababov): determine values here.
-			analyzers: []*tpb.ThresholdAnalyzerInput{},
-		},
-		"activator-with-cc-1": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator-with-cc-1.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "ac1",
-			estat: "be1",
-			// TODO(vagababov): determine values here.
-			analyzers: []*tpb.ThresholdAnalyzerInput{},
-		},
-	}
-)
-
-// bound is a helper for making the inline SLOs more readable by expressing
-// them as durations.
-func bound(d time.Duration) *float64 {
-	return ptr.Float64(d.Seconds())
-}
diff --git a/test/performance/benchmarks/dataplane-probe/dataplane-probe-activator.yaml b/test/performance/benchmarks/dataplane-probe/dataplane-probe-activator.yaml
new file mode 100644
index 000000000..9c2b2da4d
--- /dev/null
+++ b/test/performance/benchmarks/dataplane-probe/dataplane-probe-activator.yaml
@@ -0,0 +1,113 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: prober
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: prober
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: prober
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: dataplane-probe-activator
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: prober
+      restartPolicy: Never
+      containers:
+        - name: dataplane-probe
+          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe
+          args: ["-target=activator", "--duration=3m"]
+          env:
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1000m
+              memory: 3Gi
+            limits:
+              cpu: 1000m
+              memory: 3Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
diff --git a/test/performance/benchmarks/dataplane-probe/dataplane-probe-deployment.yaml b/test/performance/benchmarks/dataplane-probe/dataplane-probe-deployment.yaml
new file mode 100644
index 000000000..a9fc2ba58
--- /dev/null
+++ b/test/performance/benchmarks/dataplane-probe/dataplane-probe-deployment.yaml
@@ -0,0 +1,113 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: prober
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: prober
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: prober
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: dataplane-probe-deployment
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: prober
+      restartPolicy: Never
+      containers:
+        - name: dataplane-probe
+          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe
+          args: ["-target=deployment", "--duration=3m"]
+          env:
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1000m
+              memory: 3Gi
+            limits:
+              cpu: 1000m
+              memory: 3Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
diff --git a/test/performance/benchmarks/dataplane-probe/dataplane-probe-queue.yaml b/test/performance/benchmarks/dataplane-probe/dataplane-probe-queue.yaml
new file mode 100644
index 000000000..6d5e4b74a
--- /dev/null
+++ b/test/performance/benchmarks/dataplane-probe/dataplane-probe-queue.yaml
@@ -0,0 +1,113 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+---
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: prober
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: prober
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: prober
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: dataplane-probe-queue
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: prober
+      restartPolicy: Never
+      containers:
+        - name: dataplane-probe
+          image: ko://knative.dev/serving/test/performance/benchmarks/dataplane-probe
+          args: ["-target=queue", "--duration=3m"]
+          env:
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1000m
+              memory: 3Gi
+            limits:
+              cpu: 1000m
+              memory: 3Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
\ No newline at end of file
diff --git a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-setup.yaml b/test/performance/benchmarks/dataplane-probe/dataplane-probe-setup.yaml
similarity index 53%
rename from test/performance/benchmarks/rollout-probe/continuous/rollout-probe-setup.yaml
rename to test/performance/benchmarks/dataplane-probe/dataplane-probe-setup.yaml
index 58995b653..4846af44c 100644
--- a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-setup.yaml
+++ b/test/performance/benchmarks/dataplane-probe/dataplane-probe-setup.yaml
@@ -15,76 +15,70 @@
 apiVersion: serving.knative.dev/v1
 kind: Service
 metadata:
-  name: activator-with-cc
+  name: activator
   namespace: default
 spec:
   template:
     metadata:
       annotations:
-        autoscaling.knative.dev/minScale: "100"
-        autoscaling.knative.dev/maxScale: "150"
+        autoscaling.knative.dev/minScale: "5"
+        autoscaling.knative.dev/maxScale: "5"
         # Always hook the activator in.
         autoscaling.knative.dev/targetBurstCapacity: "-1"
-        autoscaling.knative.dev/metricAggregationAlgorithm: "weightedExponential"
     spec:
       containers:
       - image: ko://knative.dev/serving/test/test_images/autoscale
-        resources:
-          requests:
-            cpu: 20m
-            memory: 20Mi
-          limits:
-            cpu: 50m
-            memory: 50Mi
-      containerConcurrency: 5
+      containerConcurrency: 0 # Explicitly set the default, since it might be overridden in CM.
 ---
 apiVersion: serving.knative.dev/v1
 kind: Service
 metadata:
-  name: activator-with-cc-lin
+  name: queue-proxy
   namespace: default
 spec:
   template:
     metadata:
       annotations:
-        autoscaling.knative.dev/minScale: "100"
-        autoscaling.knative.dev/maxScale: "150"
-        # Always hook the activator in.
-        autoscaling.knative.dev/targetBurstCapacity: "-1"
+        autoscaling.knative.dev/minScale: "5"
+        autoscaling.knative.dev/maxScale: "5"
+        # Only hook the activator in when scaled to zero.
+        autoscaling.knative.dev/targetBurstCapacity: "0"
     spec:
       containers:
       - image: ko://knative.dev/serving/test/test_images/autoscale
-        resources:
-          requests:
-            cpu: 20m
-            memory: 20Mi
-          limits:
-            cpu: 50m
-            memory: 50Mi
-      containerConcurrency: 5
+      containerConcurrency: 0 # Explicitly set the default, since it might be overridden in CM.
 ---
-apiVersion: serving.knative.dev/v1
+apiVersion: v1
 kind: Service
 metadata:
-  name: queue-proxy-with-cc
+  name: deployment
   namespace: default
 spec:
+  ports:
+    - port: 80
+      targetPort: 8080
+  selector:
+    app: app1
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: deployment
+  namespace: default
+spec:
+  replicas: 5
+  selector:
+    matchLabels:
+      app: app1
   template:
     metadata:
-      annotations:
-        autoscaling.knative.dev/minScale: "100"
-        autoscaling.knative.dev/maxScale: "150"
-        # Only hook the activator in when scaled to zero.
-        autoscaling.knative.dev/targetBurstCapacity: "0"
+      labels:
+        app: app1
     spec:
       containers:
-      - image: ko://knative.dev/serving/test/test_images/autoscale
-        resources:
-          requests:
-            cpu: 20m
-            memory: 20Mi
-          limits:
-            cpu: 50m
-            memory: 50Mi
-      containerConcurrency: 5
+      - name: user-container
+        image: ko://knative.dev/serving/test/test_images/autoscale
+        ports:
+        - name: app1
+          containerPort: 8080
 ---
diff --git a/test/performance/benchmarks/dataplane-probe/dev.config b/test/performance/benchmarks/dataplane-probe/dev.config
deleted file mode 100644
index 971a9ff23..000000000
--- a/test/performance/benchmarks/dataplane-probe/dev.config
+++ /dev/null
@@ -1,109 +0,0 @@
-# Creating this benchmark:
-# mako create_benchmark \
-#   test/performance/benchmarks/dataplane-probe/dev.config
-# Updating this benchmark
-# mako update_benchmark \
-#   test/performance/benchmarks/dataplane-probe/dev.config
-project_name: "Knative"
-benchmark_name: "Development - Serving dataplane probe"
-description: "Measure dataplane component latency and reliability."
-benchmark_key: '6316266134437888'
-
-# Human owners for manual benchmark adjustments.
-
-# Anyone can add their IAM robot here to publish to this benchmark.
-owner_list: "mako-job@knative-performance.iam.gserviceaccount.com"
-
-# Define the name and type for x-axis of run charts
-input_value_info: {
-  value_key: "t"
-  label: "time"
-  type: TIMESTAMP
-}
-
-# Note: value_key is stored repeatedly and should be very short (ideally one or two characters).
-metric_info_list: {
-  value_key: "kd"
-  label: "kube-deployment"
-}
-metric_info_list: {
-  value_key: "id"
-  label: "istio-deployment"
-}
-metric_info_list: {
-  value_key: "qp"
-  label: "queue-proxy"
-}
-metric_info_list: {
-  value_key: "a"
-  label: "activator"
-}
-metric_info_list: {
-  value_key: "qc"
-  label: "queue-proxy-with-cc-100"
-}
-metric_info_list: {
-  value_key: "qc1"
-  label: "queue-proxy-with-cc-1"
-}
-metric_info_list: {
-  value_key: "qct"
-  label: "queue-proxy-with-cc-10"
-}
-metric_info_list: {
-  value_key: "ac"
-  label: "activator-with-cc-100"
-}
-metric_info_list: {
-  value_key: "act"
-  label: "activator-with-cc-10"
-}
-metric_info_list: {
-  value_key: "ac1"
-  label: "activator-with-cc-1"
-}
-
-metric_info_list: {
-  value_key: "ke"
-  label: "kube-errors"
-}
-metric_info_list: {
-  value_key: "ie"
-  label: "istio-errors"
-}
-metric_info_list: {
-  value_key: "qe"
-  label: "queue-errors"
-}
-metric_info_list: {
-  value_key: "ae"
-  label: "activator-errors"
-}
-metric_info_list: {
-  value_key: "re"
-  label: "queue-errors-with-cc-100"
-}
-metric_info_list: {
-  value_key: "ret"
-  label: "queue-errors-with-cc-10"
-}
-metric_info_list: {
-  value_key: "re1"
-  label: "queue-errors-with-cc-1"
-}
-metric_info_list: {
-  value_key: "be"
-  label: "activator-errors-with-cc-100"
-}
-metric_info_list: {
-  value_key: "bet"
-  label: "activator-errors-with-cc-10"
-}
-metric_info_list: {
-  value_key: "be1"
-  label: "activator-errors-with-cc-1"
-}
-metric_info_list: {
-  value_key: "ap"
-  label: "activator-pod-count"
-}
diff --git a/test/performance/benchmarks/dataplane-probe/main.go b/test/performance/benchmarks/dataplane-probe/main.go
new file mode 100644
index 000000000..09e9fcadf
--- /dev/null
+++ b/test/performance/benchmarks/dataplane-probe/main.go
@@ -0,0 +1,181 @@
+/*
+Copyright 2022 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"log"
+	"net/http"
+	"os"
+	"time"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	"knative.dev/pkg/injection"
+	"knative.dev/serving/test/performance/performance"
+
+	"knative.dev/pkg/signals"
+	"knative.dev/pkg/system"
+)
+
+const (
+	benchmarkName = "Knative Serving dataplane probe"
+)
+
+var (
+	target     = flag.String("target", "", "The target to attack.")
+	duration   = flag.Duration("duration", 5*time.Minute, "The duration of the probe")
+	minDefault = 100 * time.Millisecond
+)
+
+var (
+	// Map the above to our benchmark targets.
+	targets = map[string]struct {
+		target vegeta.Target
+		slaMin time.Duration
+		slaMax time.Duration
+	}{
+		"deployment": {
+			target: vegeta.Target{
+				Method: http.MethodGet,
+				URL:    "http://deployment.default.svc.cluster.local?sleep=100",
+			},
+			// vanilla deployment falls in the +5ms range. This does not have Knative or Istio components
+			// on the dataplane, and so it is intended as a canary to flag environmental
+			// problems that might be causing contemporaneous Knative or Istio runs to fall out of SLA.
+			slaMin: minDefault,
+			slaMax: 105 * time.Millisecond,
+		},
+		"queue": {
+			target: vegeta.Target{
+				Method: http.MethodGet,
+				URL:    "http://queue-proxy.default.svc.cluster.local?sleep=100",
+			},
+			// hitting a Knative Service
+			// going through JUST the queue-proxy falls in the +10ms range.
+			slaMin: minDefault,
+			slaMax: 110 * time.Millisecond,
+		},
+		"activator": {
+			target: vegeta.Target{
+				Method: http.MethodGet,
+				URL:    "http://activator.default.svc.cluster.local?sleep=100",
+			},
+			// hitting a Knative Service
+			// going through BOTH the activator and queue-proxy falls in the +10ms range.
+			slaMin: minDefault,
+			slaMax: 110 * time.Millisecond,
+		},
+	}
+)
+
+func main() {
+	ctx := signals.NewContext()
+	cfg := injection.ParseAndGetRESTConfigOrDie()
+	ctx, startInformers := injection.EnableInjectionOrDie(ctx, cfg)
+	startInformers()
+
+	if *target == "" {
+		log.Fatalf("-target is a required flag.")
+	}
+
+	log.Println("Starting dataplane probe for target:", *target)
+
+	ctx, cancel := context.WithTimeout(ctx, *duration+time.Minute)
+	defer cancel()
+
+	// Based on the "target" flag, load up our target benchmark.
+	// We only run one variation per run to avoid the runs being noisy neighbors,
+	// which in early iterations of the benchmark resulted in latency bleeding
+	// across the different workload types.
+	t, ok := targets[*target]
+	if !ok {
+		log.Fatalf("Unrecognized target: %s", *target)
+	}
+
+	// Make sure the target is ready before sending the large amount of requests.
+	if err := performance.ProbeTargetTillReady(t.target.URL, *duration); err != nil {
+		log.Fatalf("Failed to get target ready for attacking: %v", err)
+	}
+
+	// Send 1000 QPS (1 per ms) for the given duration with a 30s request timeout.
+	rate := vegeta.Rate{Freq: 1, Per: time.Millisecond}
+	targeter := vegeta.NewStaticTargeter(t.target)
+	attacker := vegeta.NewAttacker(vegeta.Timeout(30 * time.Second))
+
+	reporter, err := performance.NewDataPointReporterFactory(map[string]string{"target": *target}, benchmarkName)
+	if err != nil {
+		log.Fatalf("failed to create reporter: %v", err.Error())
+	}
+	defer reporter.FlushAndShutdown()
+
+	// Start the attack!
+	results := attacker.Attack(targeter, rate, *duration, "load-test")
+	deploymentStatus := performance.FetchDeploymentStatus(ctx, system.Namespace(), "activator", time.Second)
+
+	metricResults := &vegeta.Metrics{}
+
+LOOP:
+	for {
+		select {
+		case <-ctx.Done():
+			// If we time out or the pod gets shutdown via SIGTERM then start to
+			// clean thing up.
+			break LOOP
+
+		case ds := <-deploymentStatus:
+			// Report number of ready activators.
+			reporter.AddDataPoint(benchmarkName, map[string]interface{}{"activator-pod-count": float64(ds.ReadyReplicas)})
+
+		case res, ok := <-results:
+			if ok {
+				metricResults.Add(res)
+			} else {
+				// If there are no more results, then we're done!
+				break LOOP
+			}
+		}
+	}
+
+	// Compute latency percentiles
+	metricResults.Close()
+
+	// Report the results
+	reporter.AddDataPointsForMetrics(metricResults, benchmarkName)
+	_ = vegeta.NewTextReporter(metricResults).Report(os.Stdout)
+
+	if err := checkSLA(metricResults, t.slaMin, t.slaMax); err != nil {
+		// make sure to still write the stats
+		reporter.FlushAndShutdown()
+		log.Fatalf(err.Error())
+	}
+
+	log.Println("Dataplane probe test finished")
+}
+
+func checkSLA(results *vegeta.Metrics, slaMin time.Duration, slaMax time.Duration) error {
+	// SLA 1: The p95 latency hitting the target has to be between the range defined
+	// in the target map on top.
+	if results.Latencies.P95 >= slaMin && results.Latencies.P95 <= slaMax {
+		log.Printf("SLA 1 passed. P95 latency is in %d-%dms time range", slaMin, slaMax)
+	} else {
+		return fmt.Errorf("SLA 1 failed. P95 latency is not in %d-%dms time range: %s", slaMin, slaMax, results.Latencies.P95)
+	}
+
+	return nil
+}
diff --git a/test/performance/benchmarks/deployment-probe/continuous/benchmark-direct.yaml b/test/performance/benchmarks/deployment-probe/continuous/benchmark-direct.yaml
deleted file mode 100644
index 64ce68f4e..000000000
--- a/test/performance/benchmarks/deployment-probe/continuous/benchmark-direct.yaml
+++ /dev/null
@@ -1,86 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: prober
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: service-creator
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: prober
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: deployment-probe
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      containers:
-      - name: probe
-        image: ko://knative.dev/serving/test/performance/benchmarks/deployment-probe/continuous
-        args:
-        - "-template=basic"
-        - "-duration=35m"
-        - "-frequency=5s"
-        resources:
-          requests:
-            cpu: 100m
-        env:
-        - name: POD_NAME
-          valueFrom:
-            fieldRef:
-              fieldPath: metadata.name
-        - name: POD_UID
-          valueFrom:
-            fieldRef:
-              fieldPath: metadata.uid
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/deployment-probe/continuous/kodata/dev.config b/test/performance/benchmarks/deployment-probe/continuous/kodata/dev.config
deleted file mode 120000
index ee953f22c..000000000
--- a/test/performance/benchmarks/deployment-probe/continuous/kodata/dev.config
+++ /dev/null
@@ -1 +0,0 @@
-../../dev.config
\ No newline at end of file
diff --git a/test/performance/benchmarks/deployment-probe/continuous/main.go b/test/performance/benchmarks/deployment-probe/continuous/main.go
deleted file mode 100644
index db5ff24c8..000000000
--- a/test/performance/benchmarks/deployment-probe/continuous/main.go
+++ /dev/null
@@ -1,304 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"context"
-	"flag"
-	"log"
-	"os"
-	"path/filepath"
-	"time"
-
-	"github.com/google/mako/go/quickstore"
-	"sigs.k8s.io/yaml"
-
-	corev1 "k8s.io/api/core/v1"
-	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/apimachinery/pkg/types"
-	"k8s.io/apimachinery/pkg/util/sets"
-	"k8s.io/apimachinery/pkg/watch"
-	"knative.dev/pkg/apis"
-
-	duckv1 "knative.dev/pkg/apis/duck/v1"
-	"knative.dev/pkg/kmeta"
-	"knative.dev/pkg/ptr"
-	"knative.dev/pkg/signals"
-
-	netv1alpha1 "knative.dev/networking/pkg/apis/networking/v1alpha1"
-	networkingclient "knative.dev/networking/pkg/client/injection/client"
-	"knative.dev/pkg/test/mako"
-	autoscalingv1alpha1 "knative.dev/serving/pkg/apis/autoscaling/v1alpha1"
-	v1 "knative.dev/serving/pkg/apis/serving/v1"
-	servingclient "knative.dev/serving/pkg/client/injection/client"
-	"knative.dev/serving/test/performance"
-)
-
-const (
-	benchmarkName = "Development - Serving deployment probe"
-)
-
-var (
-	template  = flag.String("template", "", "The service template to load from kodata/")
-	duration  = flag.Duration("duration", 5*time.Minute, "The duration of the benchmark to run.")
-	frequency = flag.Duration("frequency", 5*time.Second, "The frequency at which to create services.")
-)
-
-func readTemplate() (*v1.Service, error) {
-	path := filepath.Join(os.Getenv("KO_DATA_PATH"), *template+"-template.yaml")
-	b, err := os.ReadFile(path)
-	if err != nil {
-		return nil, err
-	}
-	svc := &v1.Service{}
-	if err := yaml.Unmarshal(b, svc); err != nil {
-		return nil, err
-	}
-
-	svc.OwnerReferences = []metav1.OwnerReference{{
-		APIVersion:         "v1",
-		Kind:               "Pod",
-		Name:               os.Getenv("POD_NAME"),
-		UID:                types.UID(os.Getenv("POD_UID")),
-		Controller:         ptr.Bool(true),
-		BlockOwnerDeletion: ptr.Bool(true),
-	}}
-
-	return svc, nil
-}
-
-func handle(q *quickstore.Quickstore, svc kmeta.Accessor, status duckv1.Status,
-	seen sets.String, metric string) {
-	if seen.Has(svc.GetName()) {
-		return
-	}
-	cc := status.GetCondition(apis.ConditionReady)
-	if cc == nil || cc.Status == corev1.ConditionUnknown {
-		return
-	}
-	seen.Insert(svc.GetName())
-	created := svc.GetCreationTimestamp().Time
-	ready := cc.LastTransitionTime.Inner.Time
-	elapsed := ready.Sub(created)
-
-	if cc.Status == corev1.ConditionTrue {
-		q.AddSamplePoint(mako.XTime(created), map[string]float64{
-			metric: elapsed.Seconds(),
-		})
-		log.Print("Ready: ", svc.GetName())
-		performance.AddInfluxPoint(benchmarkName, map[string]interface{}{metric: elapsed.Seconds()})
-	} else if cc.Status == corev1.ConditionFalse {
-		q.AddError(mako.XTime(created), cc.Message)
-		log.Printf("Not Ready: %s; %s: %s", svc.GetName(), cc.Reason, cc.Message)
-	}
-}
-
-func main() {
-	flag.Parse()
-
-	// We want this for properly handling Kubernetes container lifecycle events.
-	ctx := signals.NewContext()
-
-	tmpl, err := readTemplate()
-	if err != nil {
-		log.Fatalf("Unable to read template %s: %v", *template, err)
-	}
-
-	// We cron every 30 minutes, so make sure that we don't severely overrun to
-	// limit how noisy a neighbor we can be.
-	ctx, cancel := context.WithTimeout(ctx, *duration)
-	defer cancel()
-
-	// Tag this run with the various flag values.
-	tags := []string{
-		"template=" + *template,
-		"duration=" + duration.String(),
-		"frequency=" + frequency.String(),
-	}
-	mc, err := mako.Setup(ctx, tags...)
-	if err != nil {
-		log.Fatal("Failed to setup mako: ", err)
-	}
-	q, qclose, ctx := mc.Quickstore, mc.ShutDownFunc, mc.Context
-	// Use a fresh context here so that our RPC to terminate the sidecar
-	// isn't subject to our timeout (or we won't shut it down when we time out)
-	defer qclose(context.Background())
-
-	sc := servingclient.Get(ctx)
-	cleanup := func() error {
-		return sc.ServingV1().Services(tmpl.Namespace).DeleteCollection(
-			context.Background(), metav1.DeleteOptions{}, metav1.ListOptions{})
-	}
-	defer cleanup()
-
-	// Wrap fatalf in a helper or our sidecar will live forever.
-	fatalf := func(f string, args ...interface{}) {
-		qclose(context.Background())
-		cleanup()
-		log.Fatalf(f, args...)
-	}
-
-	// Set up the threshold analyzers for the selected benchmark.  This will
-	// cause Mako/Quickstore to analyze the results we are storing and flag
-	// things that are outside of expected bounds.
-	q.Input.ThresholdInputs = append(q.Input.ThresholdInputs,
-		newDeploy95PercentileLatency(tags...),
-		newReadyDeploymentCount(tags...),
-	)
-
-	if err := cleanup(); err != nil {
-		fatalf("Error cleaning up services: %v", err)
-	}
-
-	lo := metav1.ListOptions{TimeoutSeconds: ptr.Int64(int64(duration.Seconds()))}
-
-	// TODO(mattmoor): We could maybe use a duckv1.KResource to eliminate this boilerplate.
-
-	serviceWI, err := sc.ServingV1().Services(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch services: %v", err)
-	}
-	defer serviceWI.Stop()
-	serviceSeen := sets.String{}
-
-	configurationWI, err := sc.ServingV1().Configurations(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch configurations: %v", err)
-	}
-	defer configurationWI.Stop()
-	configurationSeen := sets.String{}
-
-	routeWI, err := sc.ServingV1().Routes(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch routes: %v", err)
-	}
-	defer routeWI.Stop()
-	routeSeen := sets.String{}
-
-	revisionWI, err := sc.ServingV1().Revisions(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch revisions: %v", err)
-	}
-	defer revisionWI.Stop()
-	revisionSeen := sets.String{}
-
-	nc := networkingclient.Get(ctx)
-	ingressWI, err := nc.NetworkingV1alpha1().Ingresses(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch ingresss: %v", err)
-	}
-	defer ingressWI.Stop()
-	ingressSeen := sets.String{}
-
-	sksWI, err := nc.NetworkingV1alpha1().ServerlessServices(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch skss: %v", err)
-	}
-	defer sksWI.Stop()
-	sksSeen := sets.String{}
-
-	paWI, err := sc.AutoscalingV1alpha1().PodAutoscalers(tmpl.Namespace).Watch(ctx, lo)
-	if err != nil {
-		fatalf("Unable to watch pas: %v", err)
-	}
-	defer paWI.Stop()
-	paSeen := sets.String{}
-
-	tick := time.NewTicker(*frequency)
-	func() {
-		for {
-			select {
-			case <-ctx.Done():
-				// If we timeout or the pod gets shutdown via SIGTERM then start to
-				// clean thing up.
-				return
-
-			case ts := <-tick.C:
-				svc, err := sc.ServingV1().Services(tmpl.Namespace).Create(ctx, tmpl, metav1.CreateOptions{})
-				if err != nil {
-					q.AddError(mako.XTime(ts), err.Error())
-					log.Println("Error creating service:", err)
-					break
-				}
-				log.Println("Created:", svc.Name)
-
-			case event := <-serviceWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				svc := event.Object.(*v1.Service)
-				handle(q, svc, svc.Status.Status, serviceSeen, "dl")
-
-			case event := <-configurationWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				cfg := event.Object.(*v1.Configuration)
-				handle(q, cfg, cfg.Status.Status, configurationSeen, "cl")
-
-			case event := <-routeWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				rt := event.Object.(*v1.Route)
-				handle(q, rt, rt.Status.Status, routeSeen, "rl")
-
-			case event := <-revisionWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				rev := event.Object.(*v1.Revision)
-				handle(q, rev, rev.Status.Status, revisionSeen, "rvl")
-
-			case event := <-ingressWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				ing := event.Object.(*netv1alpha1.Ingress)
-				handle(q, ing, ing.Status.Status, ingressSeen, "il")
-
-			case event := <-sksWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				ing := event.Object.(*netv1alpha1.ServerlessService)
-				handle(q, ing, ing.Status.Status, sksSeen, "sksl")
-
-			case event := <-paWI.ResultChan():
-				if event.Type != watch.Modified {
-					// Skip events other than modifications
-					break
-				}
-				pa := event.Object.(*autoscalingv1alpha1.PodAutoscaler)
-				handle(q, pa, pa.Status.Status, paSeen, "pal")
-			}
-		}
-	}()
-
-	// Commit this benchmark run to Mako!
-	out, err := q.Store()
-	if err != nil {
-		fatalf("q.Store error: %v: %v", out, err)
-	}
-	log.Printf("Done! Run: %s\n", out.GetRunChartLink())
-}
diff --git a/test/performance/benchmarks/deployment-probe/continuous/sla.go b/test/performance/benchmarks/deployment-probe/continuous/sla.go
deleted file mode 100644
index 2a0f92a5f..000000000
--- a/test/performance/benchmarks/deployment-probe/continuous/sla.go
+++ /dev/null
@@ -1,70 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"time"
-
-	tpb "github.com/google/mako/clients/proto/analyzers/threshold_analyzer_go_proto"
-	mpb "github.com/google/mako/spec/proto/mako_go_proto"
-
-	"knative.dev/pkg/ptr"
-	"knative.dev/pkg/test/mako"
-)
-
-// This analyzer validates that the p95 latency deploying a new service takes up
-// to 25 seconds.
-func newDeploy95PercentileLatency(tags ...string) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String("Deploy p95 latency"),
-		Configs: []*tpb.ThresholdConfig{{
-			Min: bound(0 * time.Second),
-			Max: bound(25 * time.Second),
-			DataFilter: &mpb.DataFilter{
-				DataType:            mpb.DataFilter_METRIC_AGGREGATE_PERCENTILE.Enum(),
-				PercentileMilliRank: ptr.Int32(95000),
-				ValueKey:            ptr.String("dl"),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10, tags...),
-	}
-}
-
-// This analyzer validates that the number of services deployed to "Ready=True".
-// Configured to run for 35m with a frequency of 5s, the theoretical limit is 420
-// if deployments take 0s.  Factoring in deployment latency, we will miss a
-// handful of the trailing deployments, so we relax this to 410.
-func newReadyDeploymentCount(tags ...string) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String("Ready deployment count"),
-		Configs: []*tpb.ThresholdConfig{{
-			Min: ptr.Float64(410),
-			Max: ptr.Float64(420),
-			DataFilter: &mpb.DataFilter{
-				DataType: mpb.DataFilter_METRIC_AGGREGATE_COUNT.Enum(),
-				ValueKey: ptr.String("dl"),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10, tags...),
-	}
-}
-
-// bound is a helper for making the inline SLOs more readable by expressing
-// them as durations.
-func bound(d time.Duration) *float64 {
-	return ptr.Float64(d.Seconds())
-}
diff --git a/test/performance/benchmarks/deployment-probe/dev.config b/test/performance/benchmarks/deployment-probe/dev.config
deleted file mode 100644
index 97460b393..000000000
--- a/test/performance/benchmarks/deployment-probe/dev.config
+++ /dev/null
@@ -1,52 +0,0 @@
-# Creating this benchmark:
-# mako create_benchmark \
-#   test/performance/benchmarks/deployment-probe/dev.config
-# Updating this benchmark
-# mako update_benchmark \
-#   test/performance/benchmarks/deployment-probe/dev.config
-project_name: "Knative"
-benchmark_name: "Development - Serving deployment probe"
-description: "Measure deployment latency."
-benchmark_key: '5915474038620160'
-
-# Human owners for manual benchmark adjustments.
-
-# Anyone can add their IAM robot here to publish to this benchmark.
-owner_list: "mako-job@knative-performance.iam.gserviceaccount.com"
-
-# Define the name and type for x-axis of run charts
-input_value_info: {
-  value_key: "t"
-  label: "time"
-  type: TIMESTAMP
-}
-
-# Note: value_key is stored repeatedly and should be very short (ideally one or two characters).
-metric_info_list: {
-  value_key: "dl"
-  label: "deployment-latency"
-}
-metric_info_list: {
-  value_key: "cl"
-  label: "configuration-latency"
-}
-metric_info_list: {
-  value_key: "rl"
-  label: "route-latency"
-}
-metric_info_list: {
-  value_key: "rvl"
-  label: "revision-latency"
-}
-metric_info_list: {
-  value_key: "il"
-  label: "ingress-latency"
-}
-metric_info_list: {
-  value_key: "sksl"
-  label: "sks-latency"
-}
-metric_info_list: {
-  value_key: "pal"
-  label: "podautoscaler-latency"
-}
diff --git a/test/performance/benchmarks/load-test/continuous/kodata/dev.config b/test/performance/benchmarks/load-test/continuous/kodata/dev.config
deleted file mode 120000
index ee953f22c..000000000
--- a/test/performance/benchmarks/load-test/continuous/kodata/dev.config
+++ /dev/null
@@ -1 +0,0 @@
-../../dev.config
\ No newline at end of file
diff --git a/test/performance/benchmarks/load-test/continuous/load-test-0-direct.yaml b/test/performance/benchmarks/load-test/continuous/load-test-0-direct.yaml
deleted file mode 100644
index e8ce6c75c..000000000
--- a/test/performance/benchmarks/load-test/continuous/load-test-0-direct.yaml
+++ /dev/null
@@ -1,76 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: loader
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: load-testing-loader
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: loader
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: load-test-zero
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: loader
-      containers:
-      - name: load-test
-        image: ko://knative.dev/serving/test/performance/benchmarks/load-test/continuous
-        args:
-        - "-flavor=zero"
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/load-test/continuous/load-test-200-direct.yaml b/test/performance/benchmarks/load-test/continuous/load-test-200-direct.yaml
deleted file mode 100644
index ecc0492cc..000000000
--- a/test/performance/benchmarks/load-test/continuous/load-test-200-direct.yaml
+++ /dev/null
@@ -1,76 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: loader
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: load-testing-loader
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: loader
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: load-test-200
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: loader
-      containers:
-      - name: load-test
-        image: ko://knative.dev/serving/test/performance/benchmarks/load-test/continuous
-        args:
-        - "-flavor=200"
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/load-test/continuous/load-test-always-direct.yaml b/test/performance/benchmarks/load-test/continuous/load-test-always-direct.yaml
deleted file mode 100644
index ecd16d949..000000000
--- a/test/performance/benchmarks/load-test/continuous/load-test-always-direct.yaml
+++ /dev/null
@@ -1,76 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: loader
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: load-testing-loader
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: loader
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: load-test-always
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: loader
-      containers:
-      - name: load-test
-        image: ko://knative.dev/serving/test/performance/benchmarks/load-test/continuous
-        args:
-        - "-flavor=always"
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/load-test/continuous/main.go b/test/performance/benchmarks/load-test/continuous/main.go
deleted file mode 100644
index 099b8a69c..000000000
--- a/test/performance/benchmarks/load-test/continuous/main.go
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"context"
-	"flag"
-	"fmt"
-	"log"
-	"net/http"
-	"time"
-
-	"github.com/google/mako/go/quickstore"
-	vegeta "github.com/tsenart/vegeta/v12/lib"
-	"k8s.io/apimachinery/pkg/labels"
-
-	netv1alpha1 "knative.dev/networking/pkg/apis/networking/v1alpha1"
-	"knative.dev/pkg/signals"
-	"knative.dev/pkg/test/mako"
-	pkgpacers "knative.dev/pkg/test/vegeta/pacers"
-	"knative.dev/serving/pkg/apis/serving"
-	"knative.dev/serving/test/performance"
-	"knative.dev/serving/test/performance/metrics"
-)
-
-const (
-	namespace     = "default"
-	benchmarkName = "Development - Serving load testing"
-)
-
-var (
-	flavor   = flag.String("flavor", "", "The flavor of the benchmark to run.")
-	selector labels.Selector
-)
-
-func processResults(ctx context.Context, q *quickstore.Quickstore, results <-chan *vegeta.Result) {
-	// Create a new aggregateResult to accumulate the results.
-	ar := metrics.NewAggregateResult(0)
-
-	// When the benchmark completes, iterate over the accumulated rates
-	// and add them as sample points.
-	defer func() {
-		for t, req := range ar.RequestRates {
-			q.AddSamplePoint(mako.XTime(time.Unix(t, 0)), map[string]float64{
-				"rs": float64(req),
-			})
-			performance.AddInfluxPoint(benchmarkName, map[string]interface{}{"rs": float64(req)})
-		}
-		for t, err := range ar.ErrorRates {
-			q.AddSamplePoint(mako.XTime(time.Unix(t, 0)), map[string]float64{
-				"es": float64(err),
-			})
-			performance.AddInfluxPoint(benchmarkName, map[string]interface{}{"es": float64(err)})
-		}
-	}()
-
-	ctx, cancel := context.WithCancel(ctx)
-	deploymentStatus := metrics.FetchDeploymentsStatus(ctx, namespace, selector, time.Second)
-	sksMode := metrics.FetchSKSStatus(ctx, namespace, selector, time.Second)
-	defer cancel()
-
-	for {
-		select {
-		case res, ok := <-results:
-			// If there are no more results, then we're done!
-			if !ok {
-				return
-			}
-			// Handle the result for this request
-			metrics.HandleResult(q, benchmarkName, *res, "l", ar)
-		case ds := <-deploymentStatus:
-			// Add a sample point for the deployment status
-			q.AddSamplePoint(mako.XTime(ds.Time), map[string]float64{
-				"dp": float64(ds.DesiredReplicas),
-				"ap": float64(ds.ReadyReplicas),
-			})
-			performance.AddInfluxPoint(benchmarkName,
-				map[string]interface{}{"ap": float64(ds.ReadyReplicas), "dp": float64(ds.DesiredReplicas)})
-		case sksm := <-sksMode:
-			// Add a sample point for the serverless service mode
-			mode := float64(0)
-			if sksm.Mode == netv1alpha1.SKSOperationModeProxy {
-				mode = 1.0
-			}
-			q.AddSamplePoint(mako.XTime(sksm.Time), map[string]float64{
-				"sks": mode,
-				"na":  float64(sksm.NumActivators),
-			})
-			performance.AddInfluxPoint(benchmarkName,
-				map[string]interface{}{"sks": mode, "na": float64(sksm.NumActivators)})
-		}
-	}
-}
-
-func main() {
-	flag.Parse()
-
-	if *flavor == "" {
-		log.Fatalf("-flavor is a required flag.")
-	}
-	selector = labels.SelectorFromSet(labels.Set{
-		serving.ServiceLabelKey: "load-test-" + *flavor,
-	})
-
-	// We want this for properly handling Kubernetes container lifecycle events.
-	ctx := signals.NewContext()
-
-	// We cron every 10 minutes, so give ourselves 8 minutes to complete.
-	ctx, cancel := context.WithTimeout(ctx, 8*time.Minute)
-	defer cancel()
-
-	// Use the benchmark key created.
-	tbcTag := "tbc=" + *flavor
-	mc, err := mako.Setup(ctx, tbcTag)
-	if err != nil {
-		log.Fatal("Failed to setup mako: ", err)
-	}
-	q, qclose, ctx := mc.Quickstore, mc.ShutDownFunc, mc.Context
-	// Use a fresh context here so that our RPC to terminate the sidecar
-	// isn't subject to our timeout (or we won't shut it down when we time out)
-	defer qclose(context.Background())
-
-	// Wrap fatalf in a helper or our sidecar will live forever.
-	fatalf := func(f string, args ...interface{}) {
-		qclose(context.Background())
-		log.Fatalf(f, args...)
-	}
-
-	q.Input.ThresholdInputs = append(q.Input.ThresholdInputs,
-		newLoadTest95PercentileLatency(tbcTag),
-		newLoadTestMaximumLatency(tbcTag),
-		newLoadTestMaximumErrorRate(tbcTag))
-
-	log.Print("Starting the load test.")
-	// Ramp up load from 1k to 3k in 2 minute steps.
-	const duration = 2 * time.Minute
-	url := fmt.Sprintf("http://load-test-%s.default.svc.cluster.local?sleep=100", *flavor)
-	targeter := vegeta.NewStaticTargeter(vegeta.Target{
-		Method: http.MethodGet,
-		URL:    url,
-	})
-
-	// Make sure the target is ready before sending the large amount of requests.
-	if err := performance.ProbeTargetTillReady(url, duration); err != nil {
-		fatalf("Failed to get target ready for attacking: %v", err)
-	}
-	// Wait for scale back to 0
-	if err := performance.WaitForScaleToZero(ctx, namespace, selector, 2*time.Minute); err != nil {
-		fatalf("Failed to wait for scale-to-0: %v", err)
-	}
-
-	pacers := make([]vegeta.Pacer, 3)
-	durations := make([]time.Duration, 3)
-	for i := 1; i < 4; i++ {
-		pacers[i-1] = vegeta.Rate{Freq: i, Per: time.Millisecond}
-		durations[i-1] = duration
-	}
-	pacer, err := pkgpacers.NewCombined(pacers, durations)
-	if err != nil {
-		fatalf("Error creating the pacer: %v", err)
-	}
-	results := vegeta.NewAttacker().Attack(targeter, pacer, 3*duration, "load-test")
-	processResults(ctx, q, results)
-
-	if err := mc.StoreAndHandleResult(); err != nil {
-		fatalf("Failed to store and handle benchmarking result: %v", err)
-	}
-}
diff --git a/test/performance/benchmarks/load-test/continuous/sla.go b/test/performance/benchmarks/load-test/continuous/sla.go
deleted file mode 100644
index be9a308c6..000000000
--- a/test/performance/benchmarks/load-test/continuous/sla.go
+++ /dev/null
@@ -1,87 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"time"
-
-	tpb "github.com/google/mako/clients/proto/analyzers/threshold_analyzer_go_proto"
-	mpb "github.com/google/mako/spec/proto/mako_go_proto"
-	"knative.dev/pkg/ptr"
-	"knative.dev/pkg/test/mako"
-)
-
-// This analyzer validates that the p95 latency over the 0->3k stepped burst
-// falls in the +15ms range.   This includes a mix of cold-starts and steady
-// state (once the autoscaling decisions have leveled off).
-func newLoadTest95PercentileLatency(tags ...string) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String("95p latency"),
-		Configs: []*tpb.ThresholdConfig{{
-			Min: bound(100 * time.Millisecond),
-			Max: bound(115 * time.Millisecond),
-			DataFilter: &mpb.DataFilter{
-				DataType:            mpb.DataFilter_METRIC_AGGREGATE_PERCENTILE.Enum(),
-				PercentileMilliRank: ptr.Int32(95000),
-				ValueKey:            ptr.String("l"),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10, tags...),
-	}
-}
-
-// This analyzer validates that the maximum request latency observed over the 0->3k
-// stepped burst is no more than +10 seconds.  This is not strictly a cold-start
-// metric, but it is a superset that includes steady state latency and the latency
-// of non-cold-start overload requests.
-func newLoadTestMaximumLatency(tags ...string) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String("Maximum latency"),
-		Configs: []*tpb.ThresholdConfig{{
-			Min: bound(100 * time.Millisecond),
-			Max: bound(100*time.Millisecond + 10*time.Second),
-			DataFilter: &mpb.DataFilter{
-				DataType: mpb.DataFilter_METRIC_AGGREGATE_MAX.Enum(),
-				ValueKey: ptr.String("l"),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10, tags...),
-	}
-}
-
-// This analyzer validates that the mean error rate observed over the 0->3k
-// stepped burst is 0.
-func newLoadTestMaximumErrorRate(tags ...string) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String("Mean error rate"),
-		Configs: []*tpb.ThresholdConfig{{
-			// TODO(chizhg): set the error rate check back to 0 after https://github.com/knative/serving/issues/10074 is fixed.
-			Max: ptr.Float64(1),
-			DataFilter: &mpb.DataFilter{
-				DataType: mpb.DataFilter_METRIC_AGGREGATE_MEAN.Enum(),
-				ValueKey: ptr.String("es"),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10, tags...),
-	}
-}
-
-// bound is a helper for making the inline SLOs more readable by expressing
-// them as durations.
-func bound(d time.Duration) *float64 {
-	return ptr.Float64(d.Seconds())
-}
diff --git a/test/performance/benchmarks/load-test/dev.config b/test/performance/benchmarks/load-test/dev.config
deleted file mode 100644
index fd613436b..000000000
--- a/test/performance/benchmarks/load-test/dev.config
+++ /dev/null
@@ -1,56 +0,0 @@
-# Creating this benchmark:
-# mako create_benchmark \
-#   test/performance/benchmarks/load-test/dev.config
-# Updating this benchmark:
-# mako update_benchmark \
-#   test/performance/benchmarks/load-test/dev.config
-project_name: "Knative"
-benchmark_name: "Development - Serving load testing"
-description: "Load test 0->1k->2k->3k against a ksvc (with several TBC values)."
-benchmark_key: '6297841731371008'
-
-# Human owners for manual benchmark adjustments.
-
-# Anyone can add their IAM robot here to publish to this benchmark.
-owner_list: "mako-job@knative-performance.iam.gserviceaccount.com"
-
-# Define the name and type for x-axis of run charts
-input_value_info: {
-  value_key: "t"
-  label: "time"
-  type: TIMESTAMP
-}
-
-# Note: value_key is stored repeatedly and should be very short (ideally one or two characters).
-metric_info_list: {
-  value_key: "l"
-  label: "latency"
-}
-
-# Used to track errors/sec and requests/sec alongside latency
-metric_info_list: {
-  value_key: "es"
-  label: "errs-sec"
-}
-metric_info_list: {
-  value_key: "rs"
-  label: "requests-sec"
-}
-
-# Used to track desired and actual pod counts alongside latency
-metric_info_list: {
-  value_key: "dp"
-  label: "desired-pods"
-}
-metric_info_list: {
-  value_key: "ap"
-  label: "available-pods"
-}
-metric_info_list: {
-  value_key: "sks"
-  label: "sks-proxy"
-}
-metric_info_list: {
-  value_key: "na"
-  label: "num-activators"
-}
diff --git a/test/performance/benchmarks/load-test/load-test-0-direct.yaml b/test/performance/benchmarks/load-test/load-test-0-direct.yaml
new file mode 100644
index 000000000..c0190a69f
--- /dev/null
+++ b/test/performance/benchmarks/load-test/load-test-0-direct.yaml
@@ -0,0 +1,116 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: loader
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: load-testing-loader
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: loader
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: load-test-zero
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: loader
+      containers:
+      - name: load-test
+        image: ko://knative.dev/serving/test/performance/benchmarks/load-test
+        args:
+        - "-flavor=zero"
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 3Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/load-test/load-test-200-direct.yaml b/test/performance/benchmarks/load-test/load-test-200-direct.yaml
new file mode 100644
index 000000000..da4f8fb30
--- /dev/null
+++ b/test/performance/benchmarks/load-test/load-test-200-direct.yaml
@@ -0,0 +1,116 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: loader
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: load-testing-loader
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: loader
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: load-test-200
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: loader
+      containers:
+      - name: load-test
+        image: ko://knative.dev/serving/test/performance/benchmarks/load-test
+        args:
+        - "-flavor=200"
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 3Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/load-test/load-test-always-direct.yaml b/test/performance/benchmarks/load-test/load-test-always-direct.yaml
new file mode 100644
index 000000000..2c7b6295d
--- /dev/null
+++ b/test/performance/benchmarks/load-test/load-test-always-direct.yaml
@@ -0,0 +1,116 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: loader
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: load-testing-loader
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: loader
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: load-test-always
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: loader
+      containers:
+      - name: load-test
+        image: ko://knative.dev/serving/test/performance/benchmarks/load-test
+        args:
+        - "-flavor=always"
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 3Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/load-test/continuous/load-test-setup.yaml b/test/performance/benchmarks/load-test/load-test-setup.yaml
similarity index 100%
rename from test/performance/benchmarks/load-test/continuous/load-test-setup.yaml
rename to test/performance/benchmarks/load-test/load-test-setup.yaml
diff --git a/test/performance/benchmarks/load-test/main.go b/test/performance/benchmarks/load-test/main.go
new file mode 100644
index 000000000..f5b18ea28
--- /dev/null
+++ b/test/performance/benchmarks/load-test/main.go
@@ -0,0 +1,187 @@
+/*
+Copyright 2022 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"log"
+	"net/http"
+	"os"
+	"time"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	"k8s.io/apimachinery/pkg/labels"
+	"knative.dev/pkg/injection"
+	"knative.dev/serving/test/performance/performance"
+
+	netv1alpha1 "knative.dev/networking/pkg/apis/networking/v1alpha1"
+	"knative.dev/pkg/signals"
+	pkgpacers "knative.dev/pkg/test/vegeta/pacers"
+	"knative.dev/serving/pkg/apis/serving"
+)
+
+const (
+	namespace     = "default"
+	benchmarkName = "Knative Serving load test"
+)
+
+var (
+	flavor = flag.String("flavor", "", "The flavor of the benchmark to run.")
+)
+
+func main() {
+	ctx := signals.NewContext()
+	cfg := injection.ParseAndGetRESTConfigOrDie()
+	ctx, startInformers := injection.EnableInjectionOrDie(ctx, cfg)
+	startInformers()
+
+	if *flavor == "" {
+		log.Fatalf("-flavor is a required flag.")
+	}
+	selector := labels.SelectorFromSet(labels.Set{
+		serving.ServiceLabelKey: "load-test-" + *flavor,
+	})
+
+	// We cron every 10 minutes, so give ourselves 8 minutes to complete.
+	ctx, cancel := context.WithTimeout(ctx, 8*time.Minute)
+	defer cancel()
+
+	reporter, err := performance.NewDataPointReporterFactory(map[string]string{"flavor": *flavor}, benchmarkName)
+	if err != nil {
+		log.Fatalf("failed to create data point reporter: %v", err.Error())
+	}
+	defer reporter.FlushAndShutdown()
+
+	log.Print("Starting the load test.")
+	// Ramp up load from 1k to 3k in 2 minute steps.
+	const duration = 2 * time.Minute
+	url := fmt.Sprintf("http://load-test-%s.default.svc.cluster.local?sleep=100", *flavor)
+	targeter := vegeta.NewStaticTargeter(vegeta.Target{
+		Method: http.MethodGet,
+		URL:    url,
+	})
+
+	// Make sure the target is ready before sending the large amount of requests.
+	if err := performance.ProbeTargetTillReady(url, duration); err != nil {
+		log.Fatalf("Failed to get target ready for attacking: %v", err)
+	}
+	// Wait for scale back to 0
+	if err := performance.WaitForScaleToZero(ctx, namespace, selector, 2*time.Minute); err != nil {
+		log.Fatalf("Failed to wait for scale-to-0: %v", err)
+	}
+
+	// Run vegeta attack with custom pacers and process results from channel
+	pacers := make([]vegeta.Pacer, 3)
+	durations := make([]time.Duration, 3)
+	for i := 1; i < 4; i++ {
+		pacers[i-1] = vegeta.Rate{Freq: i, Per: time.Millisecond}
+		durations[i-1] = duration
+	}
+	pacer, err := pkgpacers.NewCombined(pacers, durations)
+	if err != nil {
+		log.Fatalf("Error creating the pacer: %v", err)
+	}
+	resultsChan := vegeta.NewAttacker().Attack(targeter, pacer, 3*duration, "load-test")
+	metricResults := processResults(ctx, resultsChan, &reporter, selector)
+
+	// Report the results
+	reporter.AddDataPointsForMetrics(metricResults, benchmarkName)
+	_ = vegeta.NewTextReporter(metricResults).Report(os.Stdout)
+
+	if err := checkSLA(metricResults); err != nil {
+		// make sure to still write the stats
+		reporter.FlushAndShutdown()
+		log.Fatalf(err.Error())
+	}
+
+	log.Println("Load test finished")
+}
+
+func processResults(ctx context.Context, results <-chan *vegeta.Result, reporter *performance.DataPointReporter, selector labels.Selector) *vegeta.Metrics {
+	ctx, cancel := context.WithCancel(ctx)
+	deploymentStatus := performance.FetchDeploymentsStatus(ctx, namespace, selector, time.Second)
+	sksMode := performance.FetchSKSStatus(ctx, namespace, selector, time.Second)
+	defer cancel()
+
+	metricResults := &vegeta.Metrics{}
+
+	for {
+		select {
+		case <-ctx.Done():
+			// If we time out or the pod gets shutdown via SIGTERM then start to
+			// clean thing up.
+			return nil
+
+		case res, ok := <-results:
+			if ok {
+				metricResults.Add(res)
+			} else {
+				// If there are no more results, then we're done!
+				// Compute latency percentiles
+				metricResults.Close()
+
+				return metricResults
+			}
+
+		case ds := <-deploymentStatus:
+			// Add a sample point for the deployment status
+			(*reporter).AddDataPoint(benchmarkName,
+				map[string]interface{}{"ready-replicas": float64(ds.ReadyReplicas), "desired-replicas": float64(ds.DesiredReplicas)})
+
+		case sksm := <-sksMode:
+			// Add a sample point for the serverless service mode
+			mode := float64(0)
+			if sksm.Mode == netv1alpha1.SKSOperationModeProxy {
+				mode = 1.0
+			}
+			(*reporter).AddDataPoint(benchmarkName,
+				map[string]interface{}{"sks": mode, "num-activators": float64(sksm.NumActivators)})
+		}
+	}
+}
+
+func checkSLA(results *vegeta.Metrics) error {
+	// SLA 1: the p95 latency has to be over the 0->3k stepped burst
+	// falls in the +15ms range (we sleep 100 ms, so 100-115ms).
+	// This includes a mix of cold-starts and steady state (once the autoscaling decisions have leveled off).
+	if results.Latencies.P95 >= 100*time.Millisecond && results.Latencies.P95 <= 115*time.Millisecond {
+		log.Println("SLA 1 passed. P95 latency is in 100-115ms time range")
+	} else {
+		return fmt.Errorf("SLA 1 failed. P95 latency is not in 100-115ms time range: %s", results.Latencies.P95)
+	}
+
+	// SLA 2: the maximum request latency observed over the 0->3k
+	// stepped burst is no more than +10 seconds. This is not strictly a cold-start
+	// metric, but it is a superset that includes steady state latency and the latency
+	// of non-cold-start overload requests.
+	if results.Latencies.Max <= 10*time.Second {
+		log.Println("SLA 2 passed. Max latency is below 10s")
+	} else {
+		return fmt.Errorf("SLA 2 failed. Max latency is above 10s: %s", results.Latencies.Max)
+	}
+
+	// SLA 3: The mean error rate observed over the 0->3k stepped burst is 0.
+	if len(results.Errors) == 0 {
+		log.Println("SLA 3 passed. No errors occurred")
+	} else {
+		return fmt.Errorf("SLA 3 failed. Errors occurred: %d", len(results.Errors))
+	}
+
+	return nil
+}
diff --git a/test/performance/benchmarks/real-traffic-test/main.go b/test/performance/benchmarks/real-traffic-test/main.go
new file mode 100644
index 000000000..db18a12f6
--- /dev/null
+++ b/test/performance/benchmarks/real-traffic-test/main.go
@@ -0,0 +1,301 @@
+/*
+Copyright 2022 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"log"
+	"math/rand"
+	"net/http"
+	"os"
+	"strconv"
+	"testing"
+	"time"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	"golang.org/x/sync/errgroup"
+	corev1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/resource"
+	netapi "knative.dev/networking/pkg/apis/networking"
+	"knative.dev/pkg/environment"
+	"knative.dev/pkg/injection"
+	pkgTest "knative.dev/pkg/test"
+	"knative.dev/serving/pkg/apis/autoscaling"
+	"knative.dev/serving/pkg/apis/serving"
+	ktest "knative.dev/serving/pkg/testing/v1"
+	"knative.dev/serving/test"
+	"knative.dev/serving/test/performance/performance"
+	v1test "knative.dev/serving/test/v1"
+
+	"knative.dev/pkg/signals"
+
+	crytporand "crypto/rand"
+)
+
+const (
+	namespace     = "default"
+	benchmarkName = "Knative Serving real traffic test"
+	serviceName   = "perftest"
+
+	duration = 5 * time.Minute
+
+	// Test configuration
+	// Defines the latency of target
+	minLatency = 0 * time.Second
+	maxLatency = 5 * time.Second
+
+	// Defines the delay that a Knative Service has for startup (init-container causes the delay)
+	minStartupLatency = 0 * time.Second
+	maxStartupLatency = 10 * time.Second
+
+	// Defines the payloads that are sent on the vegeta requests
+	minPayloadSizeBytes = 10
+	maxPayloadSizeBytes = 50_000
+)
+
+var (
+	numberOfServices = flag.Int("number-of-services", 10, "The number of Knative Services to create")
+	rps              = flag.Int("requests-per-second", 300, "The of requests per second to send")
+)
+
+type serviceConfig struct {
+	resourceObjects *v1test.ResourceObjects
+
+	activatorAlwaysInPath bool
+	latency               int64
+	startupLatency        int64
+	payload               []byte
+}
+
+func main() {
+	if *rps >= 2500 {
+		log.Fatal("One test container cannot create more than 2500 RPS without errors. Consider starting this test in parallel.")
+	}
+
+	ctx := signals.NewContext()
+	ctx, cancel := context.WithTimeout(ctx, 30*time.Minute)
+	defer cancel()
+
+	// To make testing.T work properly
+	testing.Init()
+
+	env := environment.ClientConfig{}
+
+	// The local domain is directly resolvable by the test
+	flag.Set("resolvabledomain", "true")
+
+	// manually parse flags to avoid conflicting flags
+	flag.Parse()
+
+	cfg, err := env.GetRESTConfig()
+	if err != nil {
+		log.Fatalf("failed to get kubeconfig %s", err)
+	}
+
+	ctx, _ = injection.EnableInjectionOrDie(ctx, cfg)
+
+	clients, err := test.NewClients(cfg, namespace)
+	if err != nil {
+		log.Fatal("Failed to setup clients: ", err)
+	}
+
+	reporter, err := performance.NewDataPointReporterFactory(map[string]string{"number-of-services": strconv.Itoa(*numberOfServices)}, benchmarkName)
+	if err != nil {
+		log.Fatalf("failed to create reporter: %v", err.Error())
+	}
+	defer reporter.FlushAndShutdown()
+
+	log.Printf("Creating %d Knative Services", *numberOfServices)
+	services, cleanup, err := createServices(clients, *numberOfServices)
+	if err != nil {
+		log.Fatalf("Failed to create services: %v", err)
+	}
+	defer cleanup()
+
+	log.Print("Creating vegeta targets")
+
+	targets := []vegeta.Target{}
+	for _, svc := range services {
+		t := vegeta.Target{
+			Method: http.MethodPost,
+			URL:    fmt.Sprintf("http://%s.default.svc.cluster.local?sleep=%d", svc.resourceObjects.Service.Name, svc.latency),
+			Body:   svc.payload,
+		}
+		targets = append(targets, t)
+
+		log.Printf("Name: %s, startup-latency: %ds, latency: %dms, payload: %dKb, activator-always-in-path: %v",
+			svc.resourceObjects.Service.Name, svc.startupLatency, svc.latency, len(svc.payload)/1000, svc.activatorAlwaysInPath)
+	}
+
+	// Send configured RPS round-robin over all services,
+	// while the request timeout is based on the max delays + 20 seconds
+	log.Printf("Starting vegeta attack for with %v RPS for duration: %v", *rps, duration)
+	rate := vegeta.Rate{Freq: *rps, Per: time.Second}
+	attacker := vegeta.NewAttacker(vegeta.Timeout(maxLatency + maxStartupLatency + 20*time.Second))
+	targeter := vegeta.NewStaticTargeter(targets...)
+
+	results := attacker.Attack(targeter, rate, duration, "real-traffic-test")
+
+	metricResults := &vegeta.Metrics{}
+
+LOOP:
+	for {
+		select {
+		case <-ctx.Done():
+			// If we time out or the pod gets shutdown via SIGTERM then start to
+			// clean thing up.
+			break LOOP
+
+		case res, ok := <-results:
+			if ok {
+				if res.Error != "" {
+					log.Printf("error occurred calling target. Err: %s, url: %s, method: %s", res.Error, res.URL, res.Method)
+				}
+				metricResults.Add(res)
+			} else {
+				// If there are no more results, then we're done!
+				break LOOP
+			}
+		}
+	}
+
+	// Compute latency percentiles
+	metricResults.Close()
+
+	// Report the results
+	reporter.AddDataPointsForMetrics(metricResults, benchmarkName)
+	_ = vegeta.NewTextReporter(metricResults).Report(os.Stdout)
+
+	if err := checkSLA(metricResults); err != nil {
+		cleanup()
+		reporter.FlushAndShutdown()
+		log.Fatal(err.Error())
+	}
+
+	log.Println("Real traffic test finished")
+}
+
+func createServices(clients *test.Clients, count int) ([]*serviceConfig, func(), error) {
+	testNames := make([]*test.ResourceNames, count)
+
+	// Initialize our service names.
+	for i := 0; i < count; i++ {
+		testNames[i] = &test.ResourceNames{
+			Service: test.AppendRandomString(fmt.Sprintf("%s-%02d", serviceName, i)),
+			// The crd.go helpers will convert to the actual image path.
+			Image: test.Runtime,
+		}
+	}
+
+	cleanupNames := func() {
+		log.Println("Cleaning up all created services")
+		for i := 0; i < count; i++ {
+			test.TearDown(clients, testNames[i])
+		}
+	}
+
+	objs := make([]*serviceConfig, count)
+	begin := time.Now()
+	sos := []ktest.ServiceOption{
+		ktest.WithResourceRequirements(corev1.ResourceRequirements{
+			// We set a small resource alloc so that we can pack more pods into the cluster,
+			// also we do not set limits, as buffering in QP will take memory, and we'd be OOMKilled.
+			Requests: corev1.ResourceList{
+				corev1.ResourceCPU:    resource.MustParse("10m"),
+				corev1.ResourceMemory: resource.MustParse("20Mi"),
+			},
+		}),
+		ktest.WithServiceLabel(netapi.VisibilityLabelKey, serving.VisibilityClusterLocal),
+	}
+
+	g := errgroup.Group{}
+	for i := 0; i < count; i++ {
+		ndx := i
+		g.Go(func() error {
+			activatorInPath := getRandomBool()
+			if activatorInPath {
+				annotations := map[string]string{autoscaling.TargetBurstCapacityKey: "-1"}
+				sos = append(sos, ktest.WithConfigAnnotations(annotations))
+			}
+
+			startupLatency := getRandomValue(int64(minStartupLatency.Seconds()), int64(maxStartupLatency.Seconds()))
+			if startupLatency > 0 {
+				sos = append(sos, ktest.WithInitContainer(corev1.Container{
+					Name:  "slow-startup",
+					Image: pkgTest.ImagePath(test.SlowStart),
+					Args:  []string{"-sleep", strconv.FormatInt(startupLatency, 10)},
+				}))
+			}
+
+			createdService, err := v1test.CreateServiceReady(&testing.T{}, clients, testNames[ndx], sos...)
+			if err != nil {
+				return fmt.Errorf("%02d: failed to create Ready service: %w", ndx, err)
+			}
+			latency := getRandomValue(minLatency.Milliseconds(), maxLatency.Milliseconds())
+			payload, err := getRandomPayload(minPayloadSizeBytes, maxPayloadSizeBytes)
+			if err != nil {
+				return fmt.Errorf("%02d: failed to generate random payload: %w", ndx, err)
+			}
+			objs[ndx] = &serviceConfig{
+				resourceObjects:       createdService,
+				activatorAlwaysInPath: activatorInPath,
+				latency:               latency,
+				payload:               payload,
+				startupLatency:        startupLatency,
+			}
+			return nil
+		})
+	}
+	if err := g.Wait(); err != nil {
+		return nil, nil, err
+	}
+	log.Print("Created all the services in ", time.Since(begin))
+	return objs, cleanupNames, nil
+}
+
+func getRandomPayload(min int, max int) ([]byte, error) {
+	num := getRandomValue(int64(min), int64(max))
+
+	buf := make([]byte, num)
+	_, err := crytporand.Read(buf)
+	if err != nil {
+		return nil, err
+	}
+
+	return buf, nil
+}
+
+func getRandomValue(min, max int64) int64 {
+	return rand.Int63n(max-min) + min
+}
+
+func getRandomBool() bool {
+	return rand.Intn(2) == 1
+}
+
+func checkSLA(results *vegeta.Metrics) error {
+	// SLA 1: All requests should pass successfully.
+	if len(results.Errors) == 0 {
+		log.Println("SLA 1 passed. No errors occurred")
+	} else {
+		return fmt.Errorf("SLA 1 failed. Errors occurred: %d", len(results.Errors))
+	}
+
+	return nil
+}
diff --git a/test/performance/benchmarks/real-traffic-test/real-traffic-test.yaml b/test/performance/benchmarks/real-traffic-test/real-traffic-test.yaml
new file mode 100644
index 000000000..f4f47ec5a
--- /dev/null
+++ b/test/performance/benchmarks/real-traffic-test/real-traffic-test.yaml
@@ -0,0 +1,118 @@
+# Copyright 2023 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: loader
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: load-testing-loader
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: loader
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: real-traffic-test
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: loader
+      containers:
+      - name: load-test
+        image: ko://knative.dev/serving/test/performance/benchmarks/real-traffic-test
+        args:
+          - "-number-of-services=50"
+          - "-requests-per-second=2000"
+          - $IMAGE_TEMPLATE_REPLACE
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 2Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/reconciliation-delay/main.go b/test/performance/benchmarks/reconciliation-delay/main.go
new file mode 100644
index 000000000..4c7dcdd91
--- /dev/null
+++ b/test/performance/benchmarks/reconciliation-delay/main.go
@@ -0,0 +1,331 @@
+/*
+Copyright 2022 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"log"
+	"math"
+	"os"
+	"testing"
+	"time"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	corev1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/resource"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/sets"
+	"k8s.io/apimachinery/pkg/watch"
+	netapi "knative.dev/networking/pkg/apis/networking"
+	netv1alpha1 "knative.dev/networking/pkg/apis/networking/v1alpha1"
+	networkingclient "knative.dev/networking/pkg/client/injection/client"
+	"knative.dev/pkg/apis"
+	duckv1 "knative.dev/pkg/apis/duck/v1"
+	"knative.dev/pkg/environment"
+	"knative.dev/pkg/injection"
+	"knative.dev/pkg/kmeta"
+	"knative.dev/pkg/ptr"
+	"knative.dev/pkg/signals"
+	autoscalingv1alpha1 "knative.dev/serving/pkg/apis/autoscaling/v1alpha1"
+	"knative.dev/serving/pkg/apis/serving"
+	v1 "knative.dev/serving/pkg/apis/serving/v1"
+	servingclient "knative.dev/serving/pkg/client/injection/client"
+	ktest "knative.dev/serving/pkg/testing/v1"
+	"knative.dev/serving/test"
+	"knative.dev/serving/test/performance/performance"
+	v1test "knative.dev/serving/test/v1"
+)
+
+const (
+	namespace     = "default"
+	benchmarkName = "Knative Serving reconciliation delay"
+)
+
+var (
+	duration  = flag.Duration("duration", 1*time.Minute, "The duration of the benchmark to run.")
+	frequency = flag.Duration("frequency", 5*time.Second, "The frequency at which to create services.")
+)
+
+func main() {
+	ctx := signals.NewContext()
+
+	// To make testing.T work properly
+	testing.Init()
+
+	env := environment.ClientConfig{}
+
+	// manually parse flags to avoid conflicting flags
+	flag.Parse()
+
+	ctx, cancel := context.WithTimeout(ctx, *duration)
+	defer cancel()
+
+	cfg, err := env.GetRESTConfig()
+	if err != nil {
+		log.Fatalf("failed to get kubeconfig %s", err)
+	}
+
+	ctx, startInformers := injection.EnableInjectionOrDie(ctx, cfg)
+	startInformers()
+
+	sc := servingclient.Get(ctx)
+	cleanupServices := func() error {
+		return sc.ServingV1().Services(namespace).DeleteCollection(
+			context.Background(), metav1.DeleteOptions{}, metav1.ListOptions{})
+	}
+	defer cleanupServices()
+
+	// Wrap fatalf to make sure we clean up our created resources
+	fatalf := func(f string, args ...interface{}) {
+		cleanupServices()
+		log.Fatalf(f, args...)
+	}
+
+	if err := cleanupServices(); err != nil {
+		fatalf("Error cleaning up services: %v", err)
+	}
+
+	lo := metav1.ListOptions{TimeoutSeconds: ptr.Int64(int64(duration.Seconds()))}
+
+	serviceWI, err := sc.ServingV1().Services(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch services: %v", err)
+	}
+	defer serviceWI.Stop()
+	serviceSeen := make(sets.String)
+
+	configurationWI, err := sc.ServingV1().Configurations(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch configurations: %v", err)
+	}
+	defer configurationWI.Stop()
+	configurationSeen := make(sets.String)
+
+	routeWI, err := sc.ServingV1().Routes(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch routes: %v", err)
+	}
+	defer routeWI.Stop()
+	routeSeen := make(sets.String)
+
+	revisionWI, err := sc.ServingV1().Revisions(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch revisions: %v", err)
+	}
+	defer revisionWI.Stop()
+	revisionSeen := make(sets.String)
+
+	nc := networkingclient.Get(ctx)
+	ingressWI, err := nc.NetworkingV1alpha1().Ingresses(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch ingresss: %v", err)
+	}
+	defer ingressWI.Stop()
+	ingressSeen := make(sets.String)
+
+	sksWI, err := nc.NetworkingV1alpha1().ServerlessServices(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch skss: %v", err)
+	}
+	defer sksWI.Stop()
+	sksSeen := make(sets.String)
+
+	paWI, err := sc.AutoscalingV1alpha1().PodAutoscalers(namespace).Watch(ctx, lo)
+	if err != nil {
+		fatalf("Unable to watch pas: %v", err)
+	}
+	defer paWI.Stop()
+	paSeen := make(sets.String)
+
+	tick := time.NewTicker(*frequency)
+	metricResults := func() *vegeta.Metrics {
+		reporter, err := performance.NewDataPointReporterFactory(map[string]string{}, benchmarkName)
+		if err != nil {
+			fatalf(fmt.Sprintf("failed to create data point reporter: %v", err.Error()))
+		}
+		defer reporter.FlushAndShutdown()
+
+		// We use vegeta.Metrics here as a metrics collector because it already contains logic to calculate percentiles
+		mr := &vegeta.Metrics{}
+		for {
+			select {
+			case <-ctx.Done():
+				// If we time out or the pod gets shutdown via SIGTERM then start to clean thing up.
+				// Compute latency percentiles
+				mr.Close()
+				return mr
+
+			case <-tick.C:
+				_, err := sc.ServingV1().Services(namespace).Create(ctx, getService(), metav1.CreateOptions{})
+				if err != nil {
+					log.Println("Error creating service:", err)
+					break
+				}
+
+			case event := <-serviceWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				svc := event.Object.(*v1.Service)
+				handleEvent(&reporter, mr, svc, svc.Status.Status, serviceSeen, "Service")
+
+			case event := <-configurationWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				cfg := event.Object.(*v1.Configuration)
+				handleEvent(&reporter, mr, cfg, cfg.Status.Status, configurationSeen, "Configuration")
+
+			case event := <-routeWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				rt := event.Object.(*v1.Route)
+				handleEvent(&reporter, mr, rt, rt.Status.Status, routeSeen, "Route")
+
+			case event := <-revisionWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				rev := event.Object.(*v1.Revision)
+				handleEvent(&reporter, mr, rev, rev.Status.Status, revisionSeen, "Revision")
+
+			case event := <-ingressWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				ing := event.Object.(*netv1alpha1.Ingress)
+				handleEvent(&reporter, mr, ing, ing.Status.Status, ingressSeen, "Ingress")
+
+			case event := <-sksWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				ing := event.Object.(*netv1alpha1.ServerlessService)
+				handleEvent(&reporter, mr, ing, ing.Status.Status, sksSeen, "ServerlessService")
+
+			case event := <-paWI.ResultChan():
+				if event.Type != watch.Modified {
+					// Skip events other than modifications
+					break
+				}
+				pa := event.Object.(*autoscalingv1alpha1.PodAutoscaler)
+				handleEvent(&reporter, mr, pa, pa.Status.Status, paSeen, "PodAutoscaler")
+			}
+		}
+	}()
+
+	// Report to stdout
+	_ = vegeta.NewTextReporter(metricResults).Report(os.Stdout)
+
+	expectedServices := duration.Seconds() / frequency.Seconds()
+	if err := checkSLA(metricResults, expectedServices); err != nil {
+		fatalf(err.Error())
+	}
+
+	log.Println("Reconciliation delay run finished")
+}
+
+func getService() *v1.Service {
+	rn := test.ResourceNames{
+		Service: test.AppendRandomString("runtime"),
+		// The crd.go helpers will convert to the actual image path.
+		Image: test.Runtime,
+	}
+	sos := []ktest.ServiceOption{
+		ktest.WithResourceRequirements(corev1.ResourceRequirements{
+			// We set a small resource alloc so that we can pack more pods into the cluster
+			Requests: corev1.ResourceList{
+				corev1.ResourceCPU:    resource.MustParse("10m"),
+				corev1.ResourceMemory: resource.MustParse("50Mi"),
+			},
+			Limits: corev1.ResourceList{
+				corev1.ResourceCPU:    resource.MustParse("30m"),
+				corev1.ResourceMemory: resource.MustParse("50Mi"),
+			},
+		}),
+		ktest.WithServiceLabel(netapi.VisibilityLabelKey, serving.VisibilityClusterLocal),
+	}
+	return v1test.Service(rn, sos...)
+}
+
+func handleEvent(reporter *performance.DataPointReporter, metricResults *vegeta.Metrics, svc kmeta.Accessor,
+	status duckv1.Status, seen sets.String, metric string) {
+	if seen.Has(svc.GetName()) {
+		return
+	}
+
+	cc := status.GetCondition(apis.ConditionReady)
+	if cc == nil || cc.Status == corev1.ConditionUnknown {
+		return
+	}
+
+	seen.Insert(svc.GetName())
+	created := svc.GetCreationTimestamp().Time
+	ready := cc.LastTransitionTime.Inner.Time
+	elapsed := ready.Sub(created)
+
+	if cc.Status == corev1.ConditionTrue {
+		(*reporter).AddDataPoint(benchmarkName, map[string]interface{}{metric: elapsed.Seconds()})
+		result := vegeta.Result{
+			Latency: elapsed,
+		}
+		// We need to count ready Services separately, for the SLA
+		if metric == "Service" {
+			result.Code = 200
+			log.Printf("Service %s ready in %vs", svc.GetName(), elapsed.Seconds())
+		}
+		metricResults.Add(&result)
+	} else if cc.Status == corev1.ConditionFalse {
+		log.Printf("Not Ready: %s; %s: %s", svc.GetName(), cc.Reason, cc.Message)
+	}
+}
+
+func checkSLA(results *vegeta.Metrics, expectedReadyServices float64) error {
+	// SLA 1: The number of services deployed to "Ready=True" should be reached.
+	// Example: Configured to run for 35m with a frequency of 5s, the theoretical limit is 420
+	// if deployments take 0s. Factoring in deployment latency, we will miss a
+	// handful of the trailing deployments, so we relax this a bit to 97% of that.
+	relaxedExpectedReadyServices := math.Floor(expectedReadyServices * 0.97)
+
+	// Success is a percentage of all requests, so we need to multiply this by the total requests
+	readyServices := results.Success * float64(results.Requests)
+	if readyServices >= relaxedExpectedReadyServices && readyServices <= expectedReadyServices {
+		log.Printf("SLA 1 passed. Amount of ready services is within the expected range. Is: %f, expected: %f-%f",
+			readyServices, relaxedExpectedReadyServices, expectedReadyServices)
+	} else {
+		return fmt.Errorf("SLA 1 failed. Amount of ready services is out of the expected range. Is: %f, Range: %f-%f",
+			readyServices, relaxedExpectedReadyServices, expectedReadyServices)
+	}
+
+	// SLA 2: The p95 latency deploying a new service takes up to max 25 seconds.
+	if results.Latencies.P95 >= 0*time.Second && results.Latencies.P95 <= 25*time.Second {
+		log.Println("SLA 2 passed. P95 latency is in 0-25s time range")
+	} else {
+		return fmt.Errorf("SLA 2 failed. P95 latency is not in 0-25s time range: %s", results.Latencies.P95)
+	}
+
+	return nil
+}
diff --git a/test/performance/benchmarks/reconciliation-delay/reconciliation-delay.yaml b/test/performance/benchmarks/reconciliation-delay/reconciliation-delay.yaml
new file mode 100644
index 000000000..a78680542
--- /dev/null
+++ b/test/performance/benchmarks/reconciliation-delay/reconciliation-delay.yaml
@@ -0,0 +1,126 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: reconciliation-delay
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: service-creator
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: reconciliation-delay
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: reconciliation-delay
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: reconciliation-delay
+      containers:
+      - name: probe
+        image: ko://knative.dev/serving/test/performance/benchmarks/reconciliation-delay
+        args:
+        - "-duration=15m"
+        - "-frequency=5s"
+        - $IMAGE_TEMPLATE_REPLACE
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+          - name: POD_NAME
+            valueFrom:
+              fieldRef:
+                fieldPath: metadata.name
+          - name: POD_UID
+            valueFrom:
+              fieldRef:
+                fieldPath: metadata.uid
+        resources:
+          requests:
+            cpu: 100m
+            memory: 500Mi
+          limits:
+            cpu: 1000m
+            memory: 1Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/rollout-probe/continuous/kodata/dev.config b/test/performance/benchmarks/rollout-probe/continuous/kodata/dev.config
deleted file mode 120000
index ee953f22c..000000000
--- a/test/performance/benchmarks/rollout-probe/continuous/kodata/dev.config
+++ /dev/null
@@ -1 +0,0 @@
-../../dev.config
\ No newline at end of file
diff --git a/test/performance/benchmarks/rollout-probe/continuous/main.go b/test/performance/benchmarks/rollout-probe/continuous/main.go
deleted file mode 100644
index 46228213f..000000000
--- a/test/performance/benchmarks/rollout-probe/continuous/main.go
+++ /dev/null
@@ -1,250 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"context"
-	"flag"
-	"log"
-	"strings"
-	"time"
-
-	vegeta "github.com/tsenart/vegeta/v12/lib"
-
-	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/apimachinery/pkg/labels"
-
-	servingclient "knative.dev/serving/pkg/client/injection/client"
-
-	"knative.dev/pkg/signals"
-	"knative.dev/pkg/test/mako"
-	"knative.dev/serving/pkg/apis/autoscaling"
-	"knative.dev/serving/pkg/apis/serving"
-	"knative.dev/serving/test/performance"
-	"knative.dev/serving/test/performance/metrics"
-)
-
-var (
-	target   = flag.String("target", "", "The target to attack.")
-	duration = flag.Duration("duration", 5*time.Minute, "The duration of the probe")
-)
-
-const (
-	namespace     = "default"
-	benchmarkName = "Development - Serving rollout probe"
-)
-
-func main() {
-	flag.Parse()
-
-	// We want this for properly handling Kubernetes container lifecycle events.
-	ctx := signals.NewContext()
-
-	// We cron quite often, so make sure that we don't severely overrun to
-	// limit how noisy a neighbor we can be.
-	ctx, cancel := context.WithTimeout(ctx, *duration+time.Minute)
-	defer cancel()
-
-	// Use the benchmark key created
-	mc, err := mako.Setup(ctx)
-	if err != nil {
-		log.Fatal("Failed to setup mako: ", err)
-	}
-	q, qclose, ctx := mc.Quickstore, mc.ShutDownFunc, mc.Context
-	// Use a fresh context here so that our RPC to terminate the sidecar
-	// isn't subject to our timeout (or we won't shut it down when we time out)
-	defer qclose(context.Background())
-
-	// Wrap fatalf in a helper or our sidecar will live forever.
-	fatalf := func(f string, args ...interface{}) {
-		qclose(context.Background())
-		log.Fatalf(f, args...)
-	}
-
-	// Validate flags after setting up "fatalf" or our sidecar will run forever.
-	if *target == "" {
-		fatalf("Missing flag: -target")
-	}
-
-	// Based on the "target" flag, load up our target benchmark.
-	// We only run one variation per run to avoid the runs being noisy neighbors,
-	// which in early iterations of the benchmark resulted in latency bleeding
-	// across the different workload types.
-	t, ok := targets[*target]
-	if !ok {
-		fatalf("Unrecognized target: %s", *target)
-	}
-
-	// Make sure the target is ready before sending the large amount of requests.
-	if err := performance.ProbeTargetTillReady(t.target.URL, *duration); err != nil {
-		fatalf("Failed to get target ready for attacking: %v", err)
-	}
-
-	// Set up the threshold analyzers for the selected benchmark.  This will
-	// cause Mako/Quickstore to analyze the results we are storing and flag
-	// things that are outside of expected bounds.
-	q.Input.ThresholdInputs = append(q.Input.ThresholdInputs, t.analyzers...)
-
-	// Send 1k QPS for the given duration with a 30s request timeout.
-	rate := vegeta.Rate{Freq: 3600, Per: time.Second}
-	targeter := vegeta.NewStaticTargeter(t.target)
-	attacker := vegeta.NewAttacker(vegeta.Timeout(30 * time.Second))
-
-	// Create a new aggregateResult to accumulate the results.
-	ar := metrics.NewAggregateResult(int(duration.Seconds()))
-
-	selector := labels.SelectorFromSet(labels.Set{
-		serving.ServiceLabelKey: *target,
-	})
-	log.Print("Selector: ", selector)
-
-	// Setup background metric processes
-	deploymentStatus := metrics.FetchDeploymentsStatus(ctx, namespace, selector, time.Second)
-	routeStatus := metrics.FetchRouteStatus(ctx, namespace, *target, time.Second)
-
-	// Start the attack!
-	results := attacker.Attack(targeter, rate, *duration, "rollout-test")
-	firstRev, secondRev := "", ""
-
-	// After a minute, update the Ksvc.
-	updateSvc := time.After(30 * time.Second)
-
-	// Since we might qfatal in the end, this would not execute the deferred calls
-	// thus failing the restore. So bind and execute explicitly.
-	var restoreFn func()
-LOOP:
-	for {
-		select {
-		case <-ctx.Done():
-			// If we timeout or the pod gets shutdown via SIGTERM then start to
-			// clean thing up.
-			break LOOP
-
-		case <-updateSvc:
-			log.Println("Updating the service:", *target)
-			sc := servingclient.Get(ctx)
-			svc, err := sc.ServingV1().Services(namespace).Get(context.Background(), *target, metav1.GetOptions{})
-			if err != nil {
-				log.Fatalf("Error getting ksvc %s: %v", *target, err)
-			}
-			svc = svc.DeepCopy()
-			// Make sure we start with a single instance.
-
-			// At the end of the benchmark, restore to the previous value.
-			if prev := svc.Spec.Template.Annotations[autoscaling.MinScaleAnnotationKey]; prev != "" {
-				restoreFn = func() {
-					restore, err := sc.ServingV1().Services(namespace).Get(context.Background(), *target, metav1.GetOptions{})
-					if err != nil {
-						log.Println("Error getting service", err)
-						return
-					}
-					restore = restore.DeepCopy()
-					restore.Spec.Template.Annotations[autoscaling.MinScaleAnnotationKey] = prev
-					_, err = sc.ServingV1().Services(namespace).Update(
-						context.Background(), restore, metav1.UpdateOptions{})
-					log.Printf("Restoring the service to initial minScale = %s, err: %#v", prev, err)
-					// Also remove the oldest revision, to keep the list of revisions short.
-					sc.ServingV1().Revisions(namespace).Delete(ctx, restore.Status.LatestReadyRevisionName,
-						metav1.DeleteOptions{})
-				}
-			}
-			svc.Spec.Template.Annotations[autoscaling.MinScaleAnnotationKey] = "1"
-			_, err = sc.ServingV1().Services(namespace).Update(context.Background(), svc, metav1.UpdateOptions{})
-			if err != nil {
-				fatalf("Error updating ksvc %s: %v", *target, err)
-			}
-			log.Println("Successfully updated the service.")
-		case res, ok := <-results:
-			if !ok {
-				// Once we have read all of the request results, break out of
-				// our loop.
-				break LOOP
-			}
-			// Handle the result for this request.
-			metrics.HandleResult(q, benchmarkName, *res, t.stat, ar)
-		case ds := <-deploymentStatus:
-			// Ignore deployment updates until we get current one.
-			if firstRev == "" {
-				break LOOP
-			}
-			// Deployment name contains revision name.
-			// If it is the first one -- report it.
-			if strings.Contains(ds.DeploymentName, firstRev) {
-				// Add a sample point for the deployment status.
-				q.AddSamplePoint(mako.XTime(ds.Time), map[string]float64{
-					"dp": float64(ds.DesiredReplicas),
-					"ap": float64(ds.ReadyReplicas),
-				})
-				performance.AddInfluxPoint(benchmarkName,
-					map[string]interface{}{"dp": float64(ds.DesiredReplicas), "ap": float64(ds.ReadyReplicas)})
-			} else if secondRev != "" && strings.Contains(ds.DeploymentName, secondRev) {
-				// Otherwise eport the pods for the new deployment.
-				q.AddSamplePoint(mako.XTime(ds.Time), map[string]float64{
-					"dp2": float64(ds.DesiredReplicas),
-					"ap2": float64(ds.ReadyReplicas),
-				})
-				performance.AddInfluxPoint(benchmarkName,
-					map[string]interface{}{"dp2": float64(ds.DesiredReplicas), "ap2": float64(ds.ReadyReplicas)})
-				// Ignore all other revisions' deployments if there are, since
-				// they are from previous test run iterations and we don't care about
-				// their reported scale values (should be 0 & 100 depending on which
-				// one it is).
-			}
-		case rs := <-routeStatus:
-			if firstRev == "" {
-				firstRev = rs.Traffic[0].RevisionName
-				log.Println("Inferred first revision =", firstRev)
-			}
-			v := make(map[string]float64, 2)
-			if len(rs.Traffic) == 1 {
-				// If the name matches the first revision then it's before
-				// we started the rollout. If not, then the rollout is
-				// 100% complete.
-				if rs.Traffic[0].RevisionName == firstRev {
-					v["t1"] = float64(*rs.Traffic[0].Percent)
-				} else {
-					v["t2"] = float64(*rs.Traffic[0].Percent)
-				}
-			} else {
-				v["t1"] = float64(*rs.Traffic[0].Percent)
-				v["t2"] = float64(*rs.Traffic[1].Percent)
-				if secondRev == "" {
-					secondRev = rs.Traffic[1].RevisionName
-					log.Println("Inferred second revision =", secondRev)
-				}
-			}
-			q.AddSamplePoint(mako.XTime(rs.Time), v)
-		}
-	}
-	if restoreFn != nil {
-		restoreFn()
-	}
-
-	// Walk over our accumulated per-second error rates and report them as
-	// sample points.  The key is seconds since the Unix epoch, and the value
-	// is the number of errors observed in that second.
-	for ts, count := range ar.ErrorRates {
-		q.AddSamplePoint(mako.XTime(time.Unix(ts, 0)), map[string]float64{
-			t.estat: float64(count),
-		})
-	}
-
-	// Commit data to Mako and handle the result.
-	if err := mc.StoreAndHandleResult(); err != nil {
-		fatalf("Failed to store and handle benchmarking result: %v", err)
-	}
-}
diff --git a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-activator-direct.yaml b/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-activator-direct.yaml
deleted file mode 100644
index b870a1c93..000000000
--- a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-activator-direct.yaml
+++ /dev/null
@@ -1,75 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: prober
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: prober
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: prober
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: rollout-probe-activator-with-cc
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      containers:
-      - name: rollout-probe
-        image: ko://knative.dev/serving/test/performance/benchmarks/rollout-probe/continuous
-        args: ["-target=activator-with-cc", "--duration=3m"]
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-activator-lin-direct.yaml b/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-activator-lin-direct.yaml
deleted file mode 100644
index 88f95837a..000000000
--- a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-activator-lin-direct.yaml
+++ /dev/null
@@ -1,75 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: prober
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: prober
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: prober
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: rollout-probe-activator-with-cc-lin
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 0
-  template:
-    spec:
-      serviceAccountName: prober
-      containers:
-      - name: rollout-probe
-        image: ko://knative.dev/serving/test/performance/benchmarks/rollout-probe/continuous
-        args: ["-target=activator-with-cc-lin", "--duration=3m"]
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-queue-direct.yaml b/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-queue-direct.yaml
deleted file mode 100644
index a1492e7c0..000000000
--- a/test/performance/benchmarks/rollout-probe/continuous/rollout-probe-queue-direct.yaml
+++ /dev/null
@@ -1,75 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: prober
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: prober
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: prober
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: rollout-probe-queue-with-cc
-  namespace: default
-spec:
-  parallelism: 1
-  backoffLimit: 1
-  template:
-    spec:
-      serviceAccountName: prober
-      containers:
-      - name: rollout-probe
-        image: ko://knative.dev/serving/test/performance/benchmarks/rollout-probe/continuous
-        args: ["-target=queue-proxy-with-cc", "--duration=3m"]
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-      - name: mako-stub
-        image: ko://knative.dev/pkg/test/mako/stub-sidecar
-        args:
-        - "-p=10001"
-        ports:
-        - name: quickstore
-          containerPort: 9813
-        volumeMounts:
-        - name: config-mako
-          mountPath: /etc/config-mako
-        terminationMessagePolicy: FallbackToLogsOnError
-        resources:
-          requests:
-            memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
diff --git a/test/performance/benchmarks/rollout-probe/continuous/sla.go b/test/performance/benchmarks/rollout-probe/continuous/sla.go
deleted file mode 100644
index 3b5980e17..000000000
--- a/test/performance/benchmarks/rollout-probe/continuous/sla.go
+++ /dev/null
@@ -1,122 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"net/http"
-	"time"
-
-	tpb "github.com/google/mako/clients/proto/analyzers/threshold_analyzer_go_proto"
-	mpb "github.com/google/mako/spec/proto/mako_go_proto"
-	vegeta "github.com/tsenart/vegeta/v12/lib"
-	"knative.dev/pkg/ptr"
-	"knative.dev/pkg/test/mako"
-)
-
-// This function constructs an analyzer that validates the p95 aggregate value of the given metric.
-func new95PercentileLatency(name, valueKey string, min, max time.Duration) *tpb.ThresholdAnalyzerInput {
-	return &tpb.ThresholdAnalyzerInput{
-		Name: ptr.String(name),
-		Configs: []*tpb.ThresholdConfig{{
-			Min: bound(min),
-			Max: bound(max),
-			DataFilter: &mpb.DataFilter{
-				DataType:            mpb.DataFilter_METRIC_AGGREGATE_PERCENTILE.Enum(),
-				PercentileMilliRank: ptr.Int32(95000),
-				ValueKey:            ptr.String(valueKey),
-			},
-		}},
-		CrossRunConfig: mako.NewCrossRunConfig(10),
-	}
-}
-
-// This analyzer validates that the p95 latency hitting a Knative Service
-// going through JUST the queue-proxy falls in the +10ms range.
-func newQueue95PercentileLatency(valueKey string) *tpb.ThresholdAnalyzerInput {
-	return new95PercentileLatency("Queue p95 latency", valueKey, 100*time.Millisecond, 110*time.Millisecond)
-}
-
-// This analyzer validates that the p95 latency hitting a Knative Service
-// going through BOTH the activator and queue-proxy falls in the +10ms range.
-func newActivator95PercentileLatency(valueKey string) *tpb.ThresholdAnalyzerInput {
-	return new95PercentileLatency("Activator p95 latency", valueKey, 100*time.Millisecond, 110*time.Millisecond)
-}
-
-var (
-	// Map the above to our benchmark targets.
-	targets = map[string]struct {
-		target    vegeta.Target
-		stat      string
-		estat     string
-		analyzers []*tpb.ThresholdAnalyzerInput
-	}{
-		"queue-proxy": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://queue-proxy.default.svc.cluster.local?sleep=100",
-			},
-			stat:      "q",
-			estat:     "qe",
-			analyzers: []*tpb.ThresholdAnalyzerInput{newQueue95PercentileLatency("q")},
-		},
-		"queue-proxy-with-cc": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://queue-proxy-with-cc.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "qc",
-			estat: "qce",
-			// We use the same threshold analyzer, since we want Breaker to exert minimal latency impact.
-			analyzers: []*tpb.ThresholdAnalyzerInput{newQueue95PercentileLatency("qc")},
-		},
-		"activator": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator.default.svc.cluster.local?sleep=100",
-			},
-			stat:      "a",
-			estat:     "ae",
-			analyzers: []*tpb.ThresholdAnalyzerInput{newActivator95PercentileLatency("a")},
-		},
-		"activator-with-cc": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator-with-cc.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "ac",
-			estat: "ace",
-			// We use the same threshold analyzer, since we want Throttler/Breaker to exert minimal latency impact.
-			analyzers: []*tpb.ThresholdAnalyzerInput{newActivator95PercentileLatency("ac")},
-		},
-		"activator-with-cc-lin": {
-			target: vegeta.Target{
-				Method: http.MethodGet,
-				URL:    "http://activator-with-cc-lin.default.svc.cluster.local?sleep=100",
-			},
-			stat:  "ac",
-			estat: "ace",
-			// We use the same threshold analyzer, since we want Throttler/Breaker to exert minimal latency impact.
-			analyzers: []*tpb.ThresholdAnalyzerInput{newActivator95PercentileLatency("ac")},
-		},
-	}
-)
-
-// bound is a helper for making the inline SLOs more readable by expressing
-// them as durations.
-func bound(d time.Duration) *float64 {
-	return ptr.Float64(d.Seconds())
-}
diff --git a/test/performance/benchmarks/rollout-probe/dev.config b/test/performance/benchmarks/rollout-probe/dev.config
deleted file mode 100644
index d92e25fbc..000000000
--- a/test/performance/benchmarks/rollout-probe/dev.config
+++ /dev/null
@@ -1,92 +0,0 @@
-# Creating this benchmark:
-# mako create_benchmark \
-#   test/performance/benchmarks/rollout-probe/continuous/dev.config
-# Updating this benchmark
-# mako update_benchmark \
-#   test/performance/benchmarks/rollout-probe/dev.config
-project_name: "Knative"
-benchmark_name: "Development - Serving rollout probe"
-description: "Measure rollout flakiness under load"
-benchmark_key: '5205911243063296'
-
-# Human owners for manual benchmark adjustments.
-
-# Anyone can add their IAM robot here to publish to this benchmark.
-owner_list: "mako-job@knative-performance.iam.gserviceaccount.com"
-
-# Define the name and type for x-axis of run charts
-input_value_info: {
-  value_key: "t"
-  label: "time"
-  type: TIMESTAMP
-}
-
-# Note: value_key is stored repeatedly and should be very short (ideally one or two characters).
-metric_info_list: {
-  value_key: "q"
-  label: "queue-proxy"
-}
-metric_info_list: {
-  value_key: "al"
-  label: "activator-linear"
-}
-metric_info_list: {
-  value_key: "a"
-  label: "activator"
-}
-metric_info_list: {
-  value_key: "qc"
-  label: "queue-proxy-with-cc"
-}
-metric_info_list: {
-  value_key: "ac"
-  label: "activator-with-cc"
-}
-
-# error metrics.
-metric_info_list: {
-  value_key: "qe"
-  label: "queue-errors"
-}
-metric_info_list: {
-  value_key: "qce"
-  label: "queue-cc-errors"
-}
-metric_info_list: {
-  value_key: "el"
-  label: "activator-linear-errors"
-}
-metric_info_list: {
-  value_key: "ae"
-  label: "activator-errors"
-}
-metric_info_list: {
-  value_key: "ace"
-  label: "activator-cc-errors"
-}
-
-# additional metrics
-metric_info_list: {
-  value_key: "dp"
-  label: "desired-pods"
-}
-metric_info_list: {
-  value_key: "ap"
-  label: "available-pods"
-}
-metric_info_list: {
-  value_key: "dp2"
-  label: "desired-pods-new"
-}
-metric_info_list: {
-  value_key: "ap2"
-  label: "available-pods-new"
-}
-metric_info_list: {
-  value_key: "t1"
-  label: "tarffic-old"
-}
-metric_info_list: {
-  value_key: "t2"
-  label: "tarffic-new"
-}
diff --git a/test/performance/benchmarks/rollout-probe/main.go b/test/performance/benchmarks/rollout-probe/main.go
new file mode 100644
index 000000000..da6dbfba3
--- /dev/null
+++ b/test/performance/benchmarks/rollout-probe/main.go
@@ -0,0 +1,233 @@
+/*
+Copyright 2022 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"log"
+	"net/http"
+	"os"
+	"strings"
+	"time"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	"knative.dev/pkg/injection"
+	"knative.dev/serving/test/performance/performance"
+
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/labels"
+
+	servingclient "knative.dev/serving/pkg/client/injection/client"
+
+	"knative.dev/pkg/signals"
+	"knative.dev/serving/pkg/apis/autoscaling"
+	"knative.dev/serving/pkg/apis/serving"
+)
+
+var (
+	target   = flag.String("target", "", "The target to attack.")
+	duration = flag.Duration("duration", 5*time.Minute, "The duration of the probe")
+)
+
+var (
+	// Map the above to our benchmark targets.
+	targets = map[string]struct{ target vegeta.Target }{
+		"queue-proxy-with-cc": {
+			target: vegeta.Target{
+				Method: http.MethodGet,
+				URL:    "http://queue-proxy-with-cc.default.svc.cluster.local?sleep=100",
+			},
+		},
+		"activator-with-cc": {
+			target: vegeta.Target{
+				Method: http.MethodGet,
+				URL:    "http://activator-with-cc.default.svc.cluster.local?sleep=100",
+			},
+		},
+		"activator-with-cc-lin": {
+			target: vegeta.Target{
+				Method: http.MethodGet,
+				URL:    "http://activator-with-cc-lin.default.svc.cluster.local?sleep=100",
+			},
+		},
+	}
+)
+
+const (
+	namespace     = "default"
+	benchmarkName = "Knative Serving rollout probe"
+)
+
+func main() {
+	ctx := signals.NewContext()
+	cfg := injection.ParseAndGetRESTConfigOrDie()
+	ctx, startInformers := injection.EnableInjectionOrDie(ctx, cfg)
+	startInformers()
+
+	if *target == "" {
+		log.Fatalf("-target is a required flag.")
+	}
+
+	// We cron quite often, so make sure that we don't severely overrun to
+	// limit how noisy a neighbor we can be.
+	ctx, cancel := context.WithTimeout(ctx, *duration+time.Minute)
+	defer cancel()
+
+	// Based on the "target" flag, load up our target benchmark.
+	// We only run one variation per run to avoid the runs being noisy neighbors,
+	// which in early iterations of the benchmark resulted in latency bleeding
+	// across the different workload types.
+	t, ok := targets[*target]
+	if !ok {
+		log.Fatalf("Unrecognized target: %s", *target)
+	}
+
+	// Make sure the target is ready before sending the large amount of requests.
+	if err := performance.ProbeTargetTillReady(t.target.URL, *duration); err != nil {
+		log.Fatalf("Failed to get target ready for attacking: %v", err)
+	}
+
+	// Send 1k QPS for the given duration with a 30s request timeout.
+	rate := vegeta.Rate{Freq: 3600, Per: time.Second}
+	targeter := vegeta.NewStaticTargeter(t.target)
+	attacker := vegeta.NewAttacker(vegeta.Timeout(30 * time.Second))
+
+	selector := labels.SelectorFromSet(labels.Set{
+		serving.ServiceLabelKey: *target,
+	})
+	log.Print("Running rollout probe test with selector: ", selector)
+
+	reporter, err := performance.NewDataPointReporterFactory(map[string]string{"target": *target}, benchmarkName)
+	if err != nil {
+		log.Fatalf("failed to create data point reporter: %v", err.Error())
+	}
+	defer reporter.FlushAndShutdown()
+
+	// Setup background metric processes
+	deploymentStatus := performance.FetchDeploymentsStatus(ctx, namespace, selector, time.Second)
+	routeStatus := performance.FetchRouteStatus(ctx, namespace, *target, time.Second)
+
+	// Start the attack!
+	results := attacker.Attack(targeter, rate, *duration, "rollout-test")
+	firstRev, secondRev := "", ""
+
+	// After a minute, update the Ksvc.
+	updateSvc := time.After(30 * time.Second)
+
+	metricResults := &vegeta.Metrics{}
+
+LOOP:
+	for {
+		select {
+		case <-ctx.Done():
+			// If we time out or the pod gets shutdown via SIGTERM then start to
+			// clean thing up.
+			break LOOP
+
+		case <-updateSvc:
+			log.Println("Updating the service:", *target)
+			sc := servingclient.Get(ctx)
+			svc, err := sc.ServingV1().Services(namespace).Get(context.Background(), *target, metav1.GetOptions{})
+			if err != nil {
+				log.Fatalf("Error getting ksvc %s: %v", *target, err)
+			}
+			svc = svc.DeepCopy()
+
+			// Update something irrelevant to trigger a rollout
+			svc.Spec.Template.Annotations[autoscaling.MinScaleAnnotationKey] = "1"
+			_, err = sc.ServingV1().Services(namespace).Update(context.Background(), svc, metav1.UpdateOptions{})
+			if err != nil {
+				log.Fatalf("Error updating ksvc %s: %v", *target, err)
+			}
+			log.Println("Successfully updated the service.")
+
+		case res, ok := <-results:
+			if ok {
+				metricResults.Add(res)
+			} else {
+				// If there are no more results, then we're done!
+				break LOOP
+			}
+
+		case ds := <-deploymentStatus:
+			// Ignore deployment updates until we get current one.
+			if firstRev == "" {
+				continue
+			}
+			// Deployment name contains revision name.
+			// If it is the first one -- report it.
+			if strings.Contains(ds.DeploymentName, firstRev) {
+				// Add a sample point for the deployment status.
+				reporter.AddDataPoint(benchmarkName,
+					map[string]interface{}{"desired-pods": float64(ds.DesiredReplicas), "available-pods": float64(ds.ReadyReplicas)})
+			} else if secondRev != "" && strings.Contains(ds.DeploymentName, secondRev) {
+				// Otherwise report the pods for the new deployment.
+				reporter.AddDataPoint(benchmarkName,
+					map[string]interface{}{"desired-pods-new": float64(ds.DesiredReplicas), "available-pods-new": float64(ds.ReadyReplicas)})
+				// Ignore all other revisions' deployments if there are, since
+				// they are from previous test run iterations, and we don't care about
+				// their reported scale values (should be 0 & 100 depending on which
+				// one it is).
+			}
+
+		case rs := <-routeStatus:
+			if firstRev == "" {
+				firstRev = rs.Traffic[0].RevisionName
+				log.Println("Inferred first revision =", firstRev)
+			}
+			if len(rs.Traffic) != 1 {
+				// If the name matches the first revision then it's before
+				// we started the rollout. If not, then the rollout is
+				// 100% complete.
+				if secondRev == "" {
+					secondRev = rs.Traffic[1].RevisionName
+					log.Println("Inferred second revision =", secondRev)
+				}
+			}
+		}
+	}
+
+	// Compute latency percentiles
+	metricResults.Close()
+
+	// Report the results
+	reporter.AddDataPointsForMetrics(metricResults, benchmarkName)
+	_ = vegeta.NewTextReporter(metricResults).Report(os.Stdout)
+
+	if err := checkSLA(metricResults); err != nil {
+		// make sure to still write the stats
+		reporter.FlushAndShutdown()
+		log.Fatalf(err.Error())
+	}
+
+	log.Println("Load test finished")
+}
+
+func checkSLA(results *vegeta.Metrics) error {
+	// SLA 1: The p95 latency hitting a Knative Service
+	// going through either JUST the queue-proxy or BOTH the activator and queue-proxy
+	// falls in the +10ms range. Given that we sleep 100ms, the SLA is between 100-110ms.
+	if results.Latencies.P95 >= 100*time.Millisecond && results.Latencies.P95 <= 110*time.Millisecond {
+		log.Println("SLA 1 passed. P95 latency is in 100-110ms time range")
+	} else {
+		return fmt.Errorf("SLA 1 failed. P95 latency is not in 100-110ms time range: %s", results.Latencies.P95)
+	}
+
+	return nil
+}
diff --git a/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct-lin.yaml b/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct-lin.yaml
new file mode 100644
index 000000000..f4df6d2b9
--- /dev/null
+++ b/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct-lin.yaml
@@ -0,0 +1,115 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: prober
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: prober
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: prober
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: rollout-probe-activator-direct-lin
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: prober
+      containers:
+      - name: rollout-probe
+        image: ko://knative.dev/serving/test/performance/benchmarks/rollout-probe
+        args: ["-target=activator-with-cc-lin", "--duration=3m"]
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 3Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct.yaml b/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct.yaml
new file mode 100644
index 000000000..5d31e242c
--- /dev/null
+++ b/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct.yaml
@@ -0,0 +1,115 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: prober
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: prober
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: prober
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: rollout-probe-activator-direct
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 0
+  template:
+    spec:
+      serviceAccountName: prober
+      containers:
+      - name: rollout-probe
+        image: ko://knative.dev/serving/test/performance/benchmarks/rollout-probe
+        args: ["-target=activator-with-cc", "--duration=3m"]
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 3Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/rollout-probe/rollout-probe-queue-proxy-direct.yaml b/test/performance/benchmarks/rollout-probe/rollout-probe-queue-proxy-direct.yaml
new file mode 100644
index 000000000..60ef86ae1
--- /dev/null
+++ b/test/performance/benchmarks/rollout-probe/rollout-probe-queue-proxy-direct.yaml
@@ -0,0 +1,115 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: prober
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: prober
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: prober
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: rollout-probe-queue-direct
+  namespace: default
+spec:
+  parallelism: 1
+  backoffLimit: 1
+  template:
+    spec:
+      serviceAccountName: prober
+      containers:
+      - name: rollout-probe
+        image: ko://knative.dev/serving/test/performance/benchmarks/rollout-probe
+        args: ["-target=queue-proxy-with-cc", "--duration=3m"]
+        env:
+          - name: KO_DOCKER_REPO
+            value: $KO_DOCKER_REPO
+          - name: SYSTEM_NAMESPACE
+            value: $SYSTEM_NAMESPACE
+          - name: USE_OPEN_SEARCH
+            value: $USE_OPEN_SEARCH
+          - name: USE_ES
+            value: $USE_ES
+          - name: INFLUX_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxurl
+                optional: true
+          - name: INFLUX_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: influxtoken
+                optional: true
+          - name: ES_URL
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esurl
+                optional: true
+          - name: ES_USERNAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: esusername
+                optional: true
+          - name: ES_PASSWORD
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: espassword
+                optional: true
+          - name: JOB_NAME
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: jobname
+          - name: BUILD_ID
+            valueFrom:
+              secretKeyRef:
+                name: performance-test-config
+                key: buildid
+        resources:
+          requests:
+            cpu: 1000m
+            memory: 3Gi
+          limits:
+            cpu: 1000m
+            memory: 3Gi
+        securityContext:
+          seccompProfile:
+            type: RuntimeDefault
+          allowPrivilegeEscalation: false
+          readOnlyRootFilesystem: true
+          runAsNonRoot: true
+          capabilities:
+            drop:
+              - ALL
+      restartPolicy: Never
diff --git a/test/performance/benchmarks/deployment-probe/continuous/kodata/basic-template.yaml b/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct-lin.yaml
similarity index 62%
rename from test/performance/benchmarks/deployment-probe/continuous/kodata/basic-template.yaml
rename to test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct-lin.yaml
index 3b792f063..ec637edc4 100644
--- a/test/performance/benchmarks/deployment-probe/continuous/kodata/basic-template.yaml
+++ b/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct-lin.yaml
@@ -1,4 +1,4 @@
-# Copyright 2019 The Knative Authors
+# Copyright 2022 The Knative Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -15,19 +15,25 @@
 apiVersion: serving.knative.dev/v1
 kind: Service
 metadata:
-  generateName: basic-
+  name: activator-with-cc-lin
   namespace: default
 spec:
   template:
+    metadata:
+      annotations:
+        autoscaling.knative.dev/minScale: "100"
+        autoscaling.knative.dev/maxScale: "150"
+        # Always hook the activator in.
+        autoscaling.knative.dev/targetBurstCapacity: "-1"
     spec:
-      enableServiceLinks: false
       containers:
-      - image: gcr.io/knative-samples/autoscale-go:0.1
-        # Limit resources so that we can pack more on-cluster.
+      - image: ko://knative.dev/serving/test/test_images/autoscale
         resources:
           requests:
-            cpu: 10m
-            memory: 50Mi
+            cpu: 20m
+            memory: 20Mi
           limits:
-            cpu: 30m
+            cpu: 50m
             memory: 50Mi
+      containerConcurrency: 5
+---
diff --git a/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct.yaml b/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct.yaml
new file mode 100644
index 000000000..5c4677fea
--- /dev/null
+++ b/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct.yaml
@@ -0,0 +1,40 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: serving.knative.dev/v1
+kind: Service
+metadata:
+  name: activator-with-cc
+  namespace: default
+spec:
+  template:
+    metadata:
+      annotations:
+        autoscaling.knative.dev/minScale: "100"
+        autoscaling.knative.dev/maxScale: "150"
+        # Always hook the activator in.
+        autoscaling.knative.dev/targetBurstCapacity: "-1"
+        autoscaling.knative.dev/metricAggregationAlgorithm: "weightedExponential"
+    spec:
+      containers:
+      - image: ko://knative.dev/serving/test/test_images/autoscale
+        resources:
+          requests:
+            cpu: 20m
+            memory: 20Mi
+          limits:
+            cpu: 50m
+            memory: 50Mi
+      containerConcurrency: 5
+---
diff --git a/test/performance/benchmarks/rollout-probe/rollout-probe-setup-queue-proxy-direct.yaml b/test/performance/benchmarks/rollout-probe/rollout-probe-setup-queue-proxy-direct.yaml
new file mode 100644
index 000000000..e87e041ef
--- /dev/null
+++ b/test/performance/benchmarks/rollout-probe/rollout-probe-setup-queue-proxy-direct.yaml
@@ -0,0 +1,39 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: serving.knative.dev/v1
+kind: Service
+metadata:
+  name: queue-proxy-with-cc
+  namespace: default
+spec:
+  template:
+    metadata:
+      annotations:
+        autoscaling.knative.dev/minScale: "100"
+        autoscaling.knative.dev/maxScale: "150"
+        # Only hook the activator in when scaled to zero.
+        autoscaling.knative.dev/targetBurstCapacity: "0"
+    spec:
+      containers:
+      - image: ko://knative.dev/serving/test/test_images/autoscale
+        resources:
+          requests:
+            cpu: 20m
+            memory: 20Mi
+          limits:
+            cpu: 50m
+            memory: 50Mi
+      containerConcurrency: 5
+---
diff --git a/test/performance/benchmarks/scale-from-zero/continuous/kodata/dev.config b/test/performance/benchmarks/scale-from-zero/continuous/kodata/dev.config
deleted file mode 120000
index ee953f22c..000000000
--- a/test/performance/benchmarks/scale-from-zero/continuous/kodata/dev.config
+++ /dev/null
@@ -1 +0,0 @@
-../../dev.config
\ No newline at end of file
diff --git a/test/performance/benchmarks/scale-from-zero/continuous/scale-from-zero-direct.yaml b/test/performance/benchmarks/scale-from-zero/continuous/scale-from-zero-direct.yaml
deleted file mode 100644
index 530120169..000000000
--- a/test/performance/benchmarks/scale-from-zero/continuous/scale-from-zero-direct.yaml
+++ /dev/null
@@ -1,160 +0,0 @@
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ServiceAccount
-metadata:
-  name: scale-from-zero
-  namespace: default
----
-apiVersion: rbac.authorization.k8s.io/v1
-kind: ClusterRoleBinding
-metadata:
-  name: scale-from-zero
-  namespace: default
-subjects:
-  - kind: ServiceAccount
-    name: scale-from-zero
-    namespace: default
-roleRef:
-  kind: ClusterRole
-  name: cluster-admin
-  apiGroup: rbac.authorization.k8s.io
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: scale-from-zero-1
-  namespace: default
-spec:
-  parallelism: 1
-  template:
-    spec:
-      serviceAccountName: scale-from-zero
-      containers:
-        - name: scale-from-zero
-          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero/continuous
-          args:
-            - "-parallel=1"
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-            - name: config-mako
-              mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: scale-from-zero-5
-  namespace: default
-spec:
-  parallelism: 1
-  template:
-    spec:
-      serviceAccountName: scale-from-zero
-      containers:
-        - name: scale-from-zero
-          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero/continuous
-          args:
-            - "-parallel=5"
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-            - name: config-mako
-              mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: scale-from-zero-25
-  namespace: default
-spec:
-  parallelism: 1
-  template:
-    spec:
-      serviceAccountName: scale-from-zero
-      containers:
-        - name: scale-from-zero
-          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero/continuous
-          args:
-            - "-parallel=25"
-          resources:
-            requests:
-              cpu: 1000m
-              memory: 3Gi
-          volumeMounts:
-            - name: config-mako
-              mountPath: /etc/config-mako
-        - name: mako-stub
-          image: ko://knative.dev/pkg/test/mako/stub-sidecar
-          args:
-          - "-p=10001"
-          ports:
-          - name: quickstore
-            containerPort: 9813
-          volumeMounts:
-          - name: config-mako
-            mountPath: /etc/config-mako
-          terminationMessagePolicy: FallbackToLogsOnError
-          resources:
-            requests:
-              memory: 4Gi
-      volumes:
-      - name: config-mako
-        configMap:
-          name: config-mako
-      restartPolicy: Never
-
diff --git a/test/performance/benchmarks/scale-from-zero/continuous/sla.go b/test/performance/benchmarks/scale-from-zero/continuous/sla.go
deleted file mode 100644
index 91e6a3732..000000000
--- a/test/performance/benchmarks/scale-from-zero/continuous/sla.go
+++ /dev/null
@@ -1,19 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-// TODO: Determine the SLAs.
diff --git a/test/performance/benchmarks/scale-from-zero/dev.config b/test/performance/benchmarks/scale-from-zero/dev.config
deleted file mode 100644
index b367d1738..000000000
--- a/test/performance/benchmarks/scale-from-zero/dev.config
+++ /dev/null
@@ -1,62 +0,0 @@
-# Creating this benchmark:
-# mako create_benchmark \
-#   test/performance/benchmarks/scale-from-zero/dev.config
-# Updating this benchmark:
-# mako update_benchmark \
-#   test/performance/benchmarks/scale-from-zero/dev.config
-project_name: "Knative"
-benchmark_name: "Development - Serving scale from zero"
-description: "Scale from zero test against ksvcs in parallel."
-benchmark_key: '5024954898710528'
-
-# Human owners for manual benchmark adjustments.
-
-# Anyone can add their IAM robot here to publish to this benchmark.
-owner_list: "mako-job@knative-performance.iam.gserviceaccount.com"
-
-# Define the name and type for x-axis of run charts
-input_value_info: {
-  value_key: "t"
-  label: "time"
-  type: TIMESTAMP
-}
-
-# Note: value_key is stored repeatedly and should be very short (ideally one or two characters).
-metric_info_list: {
-  value_key: "l1"
-  label: "latency1"
-}
-metric_info_list: {
-  value_key: "dl1"
-  label: "deployment-latency1"
-}
-metric_info_list: {
-  value_key: "e1"
-  label: "error1"
-}
-
-metric_info_list: {
-  value_key: "l5"
-  label: "latency5"
-}
-metric_info_list: {
-  value_key: "dl5"
-  label: "deployment-latency5"
-}
-metric_info_list: {
-  value_key: "e5"
-  label: "error5"
-}
-
-metric_info_list: {
-  value_key: "l25"
-  label: "latency25"
-}
-metric_info_list: {
-  value_key: "dl25"
-  label: "deployment-latency25"
-}
-metric_info_list: {
-  value_key: "e25"
-  label: "error25"
-}
diff --git a/test/performance/benchmarks/scale-from-zero/continuous/main.go b/test/performance/benchmarks/scale-from-zero/main.go
similarity index 51%
rename from test/performance/benchmarks/scale-from-zero/continuous/main.go
rename to test/performance/benchmarks/scale-from-zero/main.go
index db68c0ef0..5e58841f0 100644
--- a/test/performance/benchmarks/scale-from-zero/continuous/main.go
+++ b/test/performance/benchmarks/scale-from-zero/main.go
@@ -22,21 +22,24 @@ import (
 	"flag"
 	"fmt"
 	"log"
+	"os"
 	"strconv"
 	"sync"
 	"testing"
 	"time"
 
+	vegeta "github.com/tsenart/vegeta/v12/lib"
 	v1 "k8s.io/api/apps/v1"
+	netapi "knative.dev/networking/pkg/apis/networking"
+	"knative.dev/pkg/environment"
+	"knative.dev/pkg/injection"
+	"knative.dev/pkg/signals"
+	"knative.dev/serving/test/performance/performance"
 
-	"github.com/google/mako/go/quickstore"
 	"k8s.io/apimachinery/pkg/labels"
 	"k8s.io/apimachinery/pkg/watch"
-	"knative.dev/pkg/environment"
-	"knative.dev/pkg/test/mako"
 	"knative.dev/pkg/test/spoof"
 	"knative.dev/serving/pkg/apis/serving"
-	"knative.dev/serving/test/performance"
 
 	"golang.org/x/sync/errgroup"
 
@@ -51,19 +54,127 @@ import (
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 )
 
-var (
-	parallelCount = flag.Int("parallel", 0, "The count of ksvcs we want to run scale-from-zero in parallel")
-)
-
 const (
-	benchmarkName            = "Development - Serving scale from zero"
-	testNamespace            = "default"
+	benchmarkName            = "Knative Serving scale from zero"
+	namespace                = "default"
 	serviceName              = "perftest-scalefromzero"
 	helloWorldExpectedOutput = "Hello World!"
 	helloWorldImage          = "helloworld"
-	waitToServe              = 2 * time.Minute
+	waitToServe              = 5 * time.Minute
 )
 
+var (
+	parallelCount = flag.Int("parallel", 0, "The count of ksvcs we want to run scale-from-zero in parallel")
+
+	// Map the above to our benchmark targets.
+	slas = map[int]struct {
+		p95min     time.Duration
+		p95max     time.Duration
+		latencyMax time.Duration
+	}{
+		1: {
+			// Scaling one service from zero should be around 10ms
+			p95min:     0,
+			p95max:     15 * time.Millisecond,
+			latencyMax: 15 * time.Millisecond,
+		},
+		5: {
+			// Scaling five service from zero should be around 15ms
+			p95min:     0,
+			p95max:     15 * time.Millisecond,
+			latencyMax: 20 * time.Millisecond,
+		},
+		25: {
+			// Scaling 25 services in parallel will hit some API limits, which cause a step
+			// in the deployment updated time which adds to the total time until a service is ready
+			p95min:     0,
+			p95max:     2 * time.Second,
+			latencyMax: 5 * time.Second,
+		},
+		100: {
+			// Scaling 100 services in parallel will hit some API limits, which cause a step
+			// in the deployment updated time which adds to the total time until a service is ready
+			p95min:     0,
+			p95max:     15 * time.Second,
+			latencyMax: 20 * time.Second,
+		},
+	}
+)
+
+func main() {
+	log.Println("Starting scale from zero test")
+
+	ctx := signals.NewContext()
+	ctx, cancel := context.WithTimeout(ctx, 10*time.Minute)
+	defer cancel()
+
+	// To make testing.T work properly
+	testing.Init()
+
+	env := environment.ClientConfig{}
+
+	// The local domain is directly resolvable by the test
+	flag.Set("resolvabledomain", "true")
+	flag.Parse()
+
+	cfg, err := env.GetRESTConfig()
+	if err != nil {
+		log.Fatalf("failed to get kubeconfig %s", err)
+	}
+
+	ctx, _ = injection.EnableInjectionOrDie(ctx, cfg)
+
+	clients, err := test.NewClients(cfg, namespace)
+	if err != nil {
+		log.Fatal("Failed to setup clients: ", err)
+	}
+
+	reporter, err := performance.NewDataPointReporterFactory(map[string]string{"parallel": strconv.Itoa(*parallelCount)}, benchmarkName)
+	if err != nil {
+		log.Fatalf("Failed to create data point reporter: %v", err)
+	}
+	defer reporter.FlushAndShutdown()
+
+	// We use vegeta.Metrics here as a metrics collector because it already contains logic to calculate percentiles
+	vegetaReporter := performance.NewVegetaReporter()
+
+	// Create the services once.
+	services, cleanup, err := createServices(clients, *parallelCount)
+	if err != nil {
+		log.Fatalf("Failed to create services: %v", err)
+	}
+	defer cleanup()
+
+	// Wrap fatalf in a helper to clean up created resources
+	fatalf := func(f string, args ...interface{}) {
+		cleanup()
+		vegetaReporter.StopAndCollectMetrics()
+		log.Fatalf(f, args...)
+	}
+
+	// Wait all services scaling to zero.
+	if err := waitForScaleToZero(ctx, services); err != nil {
+		fatalf("Failed to wait for all services to scale to zero: %v", err)
+	}
+
+	parallelScaleFromZero(ctx, clients, services, &reporter, vegetaReporter)
+
+	metricResults := vegetaReporter.StopAndCollectMetrics()
+
+	// Report the results
+	reporter.AddDataPointsForMetrics(metricResults, benchmarkName)
+	_ = vegeta.NewTextReporter(metricResults).Report(os.Stdout)
+
+	sla := slas[*parallelCount]
+	if err := checkSLA(metricResults, sla.p95min, sla.p95max, sla.latencyMax); err != nil {
+		// make sure to still write the stats
+		reporter.FlushAndShutdown()
+		log.Fatalf(err.Error())
+	}
+
+	log.Println("Scale from zero test completed")
+}
+
 func createServices(clients *test.Clients, count int) ([]*v1test.ResourceObjects, func(), error) {
 	testNames := make([]*test.ResourceNames, count)
 
@@ -99,6 +210,7 @@ func createServices(clients *test.Clients, count int) ([]*v1test.ResourceObjects
 		ktest.WithConfigAnnotations(map[string]string{
 			autoscaling.WindowAnnotationKey: "7s",
 		}),
+		ktest.WithServiceLabel(netapi.VisibilityLabelKey, serving.VisibilityClusterLocal),
 	}
 	g := errgroup.Group{}
 	for i := 0; i < count; i++ {
@@ -129,7 +241,7 @@ func waitForScaleToZero(ctx context.Context, objs []*v1test.ResourceObjects) err
 				serving.ServiceLabelKey: ro.Service.Name,
 			})
 
-			if err := performance.WaitForScaleToZero(ctx, testNamespace, selector, 2*time.Minute); err != nil {
+			if err := performance.WaitForScaleToZero(ctx, namespace, selector, 2*time.Minute); err != nil {
 				m := fmt.Sprintf("%02d: failed waiting for deployment to scale to zero: %v", idx, err)
 				log.Println(m)
 				return errors.New(m)
@@ -140,37 +252,28 @@ func waitForScaleToZero(ctx context.Context, objs []*v1test.ResourceObjects) err
 	return g.Wait()
 }
 
-func parallelScaleFromZero(ctx context.Context, clients *test.Clients, objs []*v1test.ResourceObjects, q *quickstore.Quickstore) {
+func parallelScaleFromZero(ctx context.Context, clients *test.Clients, objs []*v1test.ResourceObjects, reporter *performance.DataPointReporter, vegetaReporter *performance.VegetaReporter) {
 	count := len(objs)
-	// Get the key for saving latency and error metrics in the benchmark.
-	lk := "l" + strconv.Itoa(count)
-	dlk := "dl" + strconv.Itoa(count)
-	ek := "e" + strconv.Itoa(count)
 	var wg sync.WaitGroup
 	wg.Add(count)
 	for i := 0; i < count; i++ {
 		ndx := i
 		go func() {
 			defer wg.Done()
-			sdur, ddur, err := runScaleFromZero(ctx, clients, ndx, objs[ndx])
+			serviceReadyDuration, deploymentUpdatedDuration, err := runScaleFromZero(ctx, clients, ndx, objs[ndx])
 			if err == nil {
-				q.AddSamplePoint(mako.XTime(time.Now()), map[string]float64{
-					lk: sdur.Seconds(),
+				vegetaReporter.AddResult(&vegeta.Result{Latency: serviceReadyDuration})
+				(*reporter).AddDataPoint(benchmarkName, map[string]interface{}{
+					"service-ready-latency": serviceReadyDuration.Milliseconds(),
 				})
-				q.AddSamplePoint(mako.XTime(time.Now()), map[string]float64{
-					dlk: ddur.Seconds(),
+				(*reporter).AddDataPoint(benchmarkName, map[string]interface{}{
+					"deployment-updated-latency": deploymentUpdatedDuration.Milliseconds(),
 				})
-				performance.AddInfluxPoint(benchmarkName, map[string]interface{}{"lk": sdur.Seconds()})
-				performance.AddInfluxPoint(benchmarkName, map[string]interface{}{"dlk": ddur.Seconds()})
 			} else {
 				// Add 1 to the error metric whenever there is an error.
-				q.AddSamplePoint(mako.XTime(time.Now()), map[string]float64{
-					ek: 1,
+				(*reporter).AddDataPoint(benchmarkName, map[string]interface{}{
+					"errors": float64(1),
 				})
-				performance.AddInfluxPoint(benchmarkName, map[string]interface{}{"ek": float64(1)})
-				// By reporting errors like this, the error strings show up on
-				// the details page for each Mako run.
-				q.AddError(mako.XTime(time.Now()), err.Error())
 			}
 		}()
 	}
@@ -183,18 +286,18 @@ func runScaleFromZero(ctx context.Context, clients *test.Clients, idx int, ro *v
 		serving.ServiceLabelKey: ro.Service.Name,
 	})
 
-	watcher, err := clients.KubeClient.AppsV1().Deployments(testNamespace).Watch(
+	watcher, err := clients.KubeClient.AppsV1().Deployments(namespace).Watch(
 		ctx, metav1.ListOptions{LabelSelector: selector.String()})
 	if err != nil {
-		m := fmt.Sprintf("%02d: unable to watch the deployment for the service: %v", idx, err)
-		log.Println(m)
-		return 0, 0, errors.New(m)
+		msg := fmt.Sprintf("%02d: unable to watch the deployment for the service: %v", idx, err)
+		log.Println(msg)
+		return 0, 0, errors.New(msg)
 	}
 	defer watcher.Stop()
 
-	ddch := watcher.ResultChan()
-	sdch := make(chan struct{})
-	errch := make(chan error)
+	deploymentChangeChan := watcher.ResultChan()
+	serviceReadyChan := make(chan struct{})
+	errorChan := make(chan error)
 
 	go func() {
 		log.Printf("%02d: waiting for endpoint to serve request", idx)
@@ -211,11 +314,11 @@ func runScaleFromZero(ctx context.Context, clients *test.Clients, idx int, ro *v
 		if err != nil {
 			m := fmt.Sprintf("%02d: the endpoint for Route %q at %q didn't serve the expected text %q: %v", idx, ro.Route.Name, url, helloWorldExpectedOutput, err)
 			log.Println(m)
-			errch <- errors.New(m)
+			errorChan <- errors.New(m)
 			return
 		}
 
-		sdch <- struct{}{}
+		serviceReadyChan <- struct{}{}
 	}()
 
 	start := time.Now()
@@ -223,67 +326,39 @@ func runScaleFromZero(ctx context.Context, clients *test.Clients, idx int, ro *v
 	var dd time.Duration
 	for {
 		select {
-		case event := <-ddch:
+		case event := <-deploymentChangeChan:
 			if event.Type == watch.Modified {
 				dm := event.Object.(*v1.Deployment)
 				if *dm.Spec.Replicas != 0 && dd == 0 {
 					dd = time.Since(start)
 				}
 			}
-		case <-sdch:
-			return time.Since(start), dd, nil
-		case err := <-errch:
+		case <-serviceReadyChan:
+			since := time.Since(start)
+			log.Printf("Service is ready after: name: %s, deployment-updated: %vms, service-ready-since-deployment: %vms, service-ready-total: %vms",
+				ro.Service.Name, dd.Milliseconds(), (since - dd).Milliseconds(), since.Milliseconds())
+			return since, dd, nil
+		case err := <-errorChan:
+			log.Println("Service scaling failed: ", ro.Service.Name, err.Error())
 			return 0, 0, err
 		}
 	}
 }
 
-func testScaleFromZero(clients *test.Clients, count int) {
-	parallelTag := fmt.Sprintf("parallel=%d", count)
-	mc, err := mako.Setup(context.Background(), parallelTag)
-	if err != nil {
-		log.Fatal("failed to setup mako: ", err)
+func checkSLA(results *vegeta.Metrics, p95min time.Duration, p95max time.Duration, latencyMax time.Duration) error {
+	// SLA 1: The p95 latency hitting the target has to be between the range defined
+	if results.Latencies.P95 >= p95min && results.Latencies.P95 <= p95max {
+		log.Printf("SLA 1 passed. P95 latency is in %d-%dms time range", p95min, p95max)
+	} else {
+		return fmt.Errorf("SLA 1 failed. P95 latency is not in %d-%dms time range: %s", p95min, p95max, results.Latencies.P95)
 	}
-	q, qclose, ctx := mc.Quickstore, mc.ShutDownFunc, mc.Context
-	defer qclose(ctx)
 
-	// Create the services once.
-	objs, cleanup, err := createServices(clients, count)
-	// Wrap fatalf in a helper or our sidecar will live forever, also wrap cleanup.
-	fatalf := func(f string, args ...interface{}) {
-		cleanup()
-		qclose(ctx)
-		log.Fatalf(f, args...)
-	}
-	if err != nil {
-		fatalf("Failed to create services: %v", err)
-	}
-	defer cleanup()
-
-	// Wait all services scaling to zero.
-	if err := waitForScaleToZero(ctx, objs); err != nil {
-		fatalf("Failed to wait for all services to scale to zero: %v", err)
-	}
-
-	parallelScaleFromZero(ctx, clients, objs, q)
-	if err := mc.StoreAndHandleResult(); err != nil {
-		fatalf("Failed to store and handle benchmarking result: %v", err)
-	}
-}
-
-func main() {
-	env := environment.ClientConfig{}
-	flag.Parse()
-
-	cfg, err := env.GetRESTConfig()
-	if err != nil {
-		log.Fatalf("failed to get kubeconfig %s", err)
-	}
-
-	clients, err := test.NewClients(cfg, testNamespace)
-	if err != nil {
-		log.Fatal("Failed to setup clients: ", err)
+	// SLA 2: The max latency hitting the target has to be between the range defined
+	if results.Latencies.Max <= latencyMax {
+		log.Printf("SLA 2 passed. Max latency is below or equal to %dms", latencyMax)
+	} else {
+		return fmt.Errorf("SLA 2 failed. Max latency is higher than %dms: %s", latencyMax, results.Latencies.Max)
 	}
 
-	testScaleFromZero(clients, *parallelCount)
+	return nil
 }
diff --git a/test/performance/benchmarks/scale-from-zero/scale-from-zero-1.yaml b/test/performance/benchmarks/scale-from-zero/scale-from-zero-1.yaml
new file mode 100644
index 000000000..887677e05
--- /dev/null
+++ b/test/performance/benchmarks/scale-from-zero/scale-from-zero-1.yaml
@@ -0,0 +1,117 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: scale-from-zero
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: scale-from-zero
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: scale-from-zero
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: scale-from-zero-1
+  namespace: default
+spec:
+  parallelism: 1
+  template:
+    spec:
+      serviceAccountName: scale-from-zero
+      containers:
+        - name: scale-from-zero
+          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero
+          args:
+          - "-parallel=1"
+          - $IMAGE_TEMPLATE_REPLACE
+          env:
+            - name: KO_DOCKER_REPO
+              value: $KO_DOCKER_REPO
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1000m
+              memory: 3Gi
+            limits:
+              cpu: 1000m
+              memory: 3Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
+      restartPolicy: Never
+---
diff --git a/test/performance/benchmarks/scale-from-zero/scale-from-zero-100.yaml b/test/performance/benchmarks/scale-from-zero/scale-from-zero-100.yaml
new file mode 100644
index 000000000..22b7d1ef2
--- /dev/null
+++ b/test/performance/benchmarks/scale-from-zero/scale-from-zero-100.yaml
@@ -0,0 +1,117 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: scale-from-zero
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: scale-from-zero
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: scale-from-zero
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: scale-from-zero-100
+  namespace: default
+spec:
+  parallelism: 1
+  template:
+    spec:
+      serviceAccountName: scale-from-zero
+      containers:
+        - name: scale-from-zero
+          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero
+          args:
+            - "-parallel=100"
+            - $IMAGE_TEMPLATE_REPLACE
+          env:
+            - name: KO_DOCKER_REPO
+              value: $KO_DOCKER_REPO
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1500m
+              memory: 6Gi
+            limits:
+              cpu: 1500m
+              memory: 6Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
+      restartPolicy: Never
+---
diff --git a/test/performance/benchmarks/scale-from-zero/scale-from-zero-25.yaml b/test/performance/benchmarks/scale-from-zero/scale-from-zero-25.yaml
new file mode 100644
index 000000000..4dceeb7f7
--- /dev/null
+++ b/test/performance/benchmarks/scale-from-zero/scale-from-zero-25.yaml
@@ -0,0 +1,117 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: scale-from-zero
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: scale-from-zero
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: scale-from-zero
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: scale-from-zero-25
+  namespace: default
+spec:
+  parallelism: 1
+  template:
+    spec:
+      serviceAccountName: scale-from-zero
+      containers:
+        - name: scale-from-zero
+          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero
+          args:
+            - "-parallel=25"
+            - $IMAGE_TEMPLATE_REPLACE
+          env:
+            - name: KO_DOCKER_REPO
+              value: $KO_DOCKER_REPO
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1000m
+              memory: 4Gi
+            limits:
+              cpu: 1000m
+              memory: 4Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
+      restartPolicy: Never
+---
diff --git a/test/performance/benchmarks/scale-from-zero/scale-from-zero-5.yaml b/test/performance/benchmarks/scale-from-zero/scale-from-zero-5.yaml
new file mode 100644
index 000000000..13457d914
--- /dev/null
+++ b/test/performance/benchmarks/scale-from-zero/scale-from-zero-5.yaml
@@ -0,0 +1,117 @@
+# Copyright 2022 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+apiVersion: v1
+kind: ServiceAccount
+metadata:
+  name: scale-from-zero
+  namespace: default
+---
+apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
+metadata:
+  name: scale-from-zero
+  namespace: default
+subjects:
+  - kind: ServiceAccount
+    name: scale-from-zero
+    namespace: default
+roleRef:
+  kind: ClusterRole
+  name: cluster-admin
+  apiGroup: rbac.authorization.k8s.io
+---
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: scale-from-zero-5
+  namespace: default
+spec:
+  parallelism: 1
+  template:
+    spec:
+      serviceAccountName: scale-from-zero
+      containers:
+        - name: scale-from-zero
+          image: ko://knative.dev/serving/test/performance/benchmarks/scale-from-zero
+          args:
+            - "-parallel=5"
+            - $IMAGE_TEMPLATE_REPLACE
+          env:
+            - name: KO_DOCKER_REPO
+              value: $KO_DOCKER_REPO
+            - name: SYSTEM_NAMESPACE
+              value: $SYSTEM_NAMESPACE
+            - name: USE_OPEN_SEARCH
+              value: $USE_OPEN_SEARCH
+            - name: USE_ES
+              value: $USE_ES
+            - name: INFLUX_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxurl
+                  optional: true
+            - name: INFLUX_TOKEN
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: influxtoken
+                  optional: true
+            - name: ES_URL
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esurl
+                  optional: true
+            - name: ES_USERNAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: esusername
+                  optional: true
+            - name: ES_PASSWORD
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: espassword
+                  optional: true
+            - name: JOB_NAME
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: jobname
+            - name: BUILD_ID
+              valueFrom:
+                secretKeyRef:
+                  name: performance-test-config
+                  key: buildid
+          resources:
+            requests:
+              cpu: 1000m
+              memory: 3Gi
+            limits:
+              cpu: 1000m
+              memory: 3Gi
+          securityContext:
+            seccompProfile:
+              type: RuntimeDefault
+            allowPrivilegeEscalation: false
+            readOnlyRootFilesystem: true
+            runAsNonRoot: true
+            capabilities:
+              drop:
+                - ALL
+      restartPolicy: Never
+---
diff --git a/test/performance/config/README.md b/test/performance/config/README.md
deleted file mode 100644
index 8fb844b7a..000000000
--- a/test/performance/config/README.md
+++ /dev/null
@@ -1,65 +0,0 @@
-### Vegeta-based load generator
-
-This directory contains a simple `vegeta`-based load generator, which can be run
-with:
-
-```shell
-ko apply -f test/performance/config
-```
-
-By default, it is configured to load test the
-[autoscale-go](https://github.com/knative/docs/tree/main/docs/serving/autoscaling/autoscale-go)
-sample, which must already be deployed. You can change the target by altering
-the `ConfigMap` to point to a different endpoint.
-
-### Examining output
-
-Once the load generation pods terminate, their outputs can be examined with:
-
-```shell
-for x in $(kubectl get pods -l app=load-test -oname); do
-  kubectl logs $x | python -mjson.tool
-done
-```
-
-This will produce a series of pretty-printed JSON blocks like:
-
-```json
-{
-  "bytes_in": {
-    "mean": 38.15242083333333,
-    "total": 9156581
-  },
-  "bytes_out": {
-    "mean": 0,
-    "total": 0
-  },
-  "duration": 240001544213,
-  "earliest": "2019-06-29T22:49:57.272758595Z",
-  "end": "2019-06-29T22:53:57.399043387Z",
-  "errors": [
-    "503 Service Unavailable",
-    "502 Bad Gateway",
-    "Get http://autoscale-go.default.svc.cluster.local?sleep=100: net/http: request canceled (Client.Timeout exceeded while awaiting headers)"
-  ],
-  "latencies": {
-    "50th": 102296894,
-    "95th": 29927947157,
-    "99th": 30000272067,
-    "max": 30186427377,
-    "mean": 2483484840,
-    "total": 596036361667202
-  },
-  "latest": "2019-06-29T22:53:57.274302808Z",
-  "rate": 999.9935658205657,
-  "requests": 240000,
-  "status_codes": {
-    "0": 12302,
-    "200": 185803,
-    "502": 7,
-    "503": 41888
-  },
-  "success": 0.7741791666666666,
-  "wait": 124740579
-}
-```
diff --git a/test/performance/config/config-mako.yaml b/test/performance/config/config-mako.yaml
deleted file mode 100644
index 91b25069c..000000000
--- a/test/performance/config/config-mako.yaml
+++ /dev/null
@@ -1,47 +0,0 @@
-# Copyright 2019 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ConfigMap
-metadata:
-  name: config-mako
-
-data:
-  _example: |
-    ################################
-    #                              #
-    #    EXAMPLE CONFIGURATION     #
-    #                              #
-    ################################
-
-    # This block is not actually functional configuration,
-    # but serves to illustrate the available configuration
-    # options and document them in a way that is accessible
-    # to users that `kubectl edit` this config map.
-    #
-    # These sample configuration options may be copied out of
-    # this example block and unindented to be in the data block
-    # to actually change the configuration.
-
-    # The Mako environment in which we are running.
-    # Only our performance automation should run in "prod", but
-    # there should be a "dev" environment with a fairly broad
-    # write ACL.  Users can also develop against custom configurations
-    # by adding `foo.config` under their benchmark's kodata directory.
-    environment: dev
-
-    # Additional tags to tag the runs. These tags are added
-    # to the list that the binary itself publishes (Kubernetes version, etc).
-    # It is a comma separated list of tags.
-    additionalTags: "key=value,absolute"
diff --git a/test/performance/config/job.yaml b/test/performance/config/job.yaml
deleted file mode 100644
index b8f9b94dd..000000000
--- a/test/performance/config/job.yaml
+++ /dev/null
@@ -1,52 +0,0 @@
-# Copyright 2019 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-apiVersion: v1
-kind: ConfigMap
-metadata:
-  name: vegeta-payload
-data:
-  payload: |
-    GET http://autoscale-go.default.svc.cluster.local?sleep=100
----
-apiVersion: batch/v1
-kind: Job
-metadata:
-  name: load-test
-  namespace: default
-spec:
-  parallelism: 1
-  template:
-    metadata:
-      labels:
-        app: load-test
-    spec:
-      containers:
-      - name: vegeta
-        image: ko://github.com/tsenart/vegeta/v12
-        command: ["/bin/bash", "-c"]
-        args:
-        - "/ko-app/vegeta -cpus=1 attack -duration=4m -rate=1000/1s -targets=/var/vegeta/payload | /ko-app/vegeta report -type=json"
-        resources:
-          requests:
-            cpu: 1000m
-            memory: 3Gi
-        volumeMounts:
-        - name: vegeta-payload
-          mountPath: /var/vegeta
-      volumes:
-      - name: vegeta-payload
-        configMap:
-          name: vegeta-payload
-      restartPolicy: Never
diff --git a/test/performance/influx_client.go b/test/performance/influx_client.go
deleted file mode 100644
index 06e532fa6..000000000
--- a/test/performance/influx_client.go
+++ /dev/null
@@ -1,89 +0,0 @@
-/*
-Copyright 2022 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package performance
-
-import (
-	"crypto/tls"
-	"fmt"
-	"os"
-	"time"
-
-	influxdb2 "github.com/influxdata/influxdb-client-go/v2"
-)
-
-const (
-	influxToken             = "INFLUX_TOKEN"
-	influxURL               = "INFLUX_URL"
-	prowTag                 = "PROW_TAG"
-	org                     = "Knativetest"
-	bucket                  = "knative-serving"
-	influxURLSecretVolume   = "influx-url-secret-volume"
-	influxTokenSecretVolume = "influx-token-secret-volume"
-	influxURLSecretKey      = "influxdb-url"
-	influxTokenSecretKey    = "influxdb-token"
-)
-
-func AddInfluxPoint(measurement string, fields map[string]interface{}) error {
-
-	url, err := getSecretValue(influxURLSecretVolume, influxURLSecretKey, influxURL)
-	if err != nil {
-		return err
-	}
-
-	token, err := getSecretValue(influxTokenSecretVolume, influxTokenSecretKey, influxToken)
-	if err != nil {
-		return err
-	}
-
-	tags := map[string]string{}
-	build, found := os.LookupEnv(prowTag)
-	if found {
-		tags[prowTag] = build
-	}
-
-	client := influxdb2.NewClientWithOptions(url, token,
-		influxdb2.DefaultOptions().
-			SetUseGZip(true).
-			SetBatchSize(20).
-			//nolint:gosec // We explicitly don't need to check certs here since this is test code.
-			SetTLSConfig(&tls.Config{InsecureSkipVerify: true}))
-	defer client.Close()
-
-	writeAPI := client.WriteAPI(org, bucket)
-	p := influxdb2.NewPoint(measurement,
-		tags,
-		fields,
-		time.Now())
-	// Write point asynchronously
-	writeAPI.WritePoint(p)
-	// Force all unwritten data to be sent
-	writeAPI.Flush()
-
-	return nil
-}
-
-func getSecretValue(secretVolume, secretKey, envVarName string) (string, error) {
-	value, err := os.ReadFile(fmt.Sprintf("/etc/%s/%s", secretVolume, secretKey))
-	if err != nil {
-		valueFromEnv, ok := os.LookupEnv(envVarName)
-		if !ok {
-			return "", fmt.Errorf("failed to get INFLUX %s", secretKey)
-		}
-		return valueFromEnv, nil
-	}
-	return string(value), nil
-}
diff --git a/test/performance/metrics/request.go b/test/performance/metrics/request.go
deleted file mode 100644
index 8a74ebaab..000000000
--- a/test/performance/metrics/request.go
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
-Copyright 2019 The Knative Authors
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package metrics
-
-import (
-	"log"
-
-	"github.com/google/mako/go/quickstore"
-	vegeta "github.com/tsenart/vegeta/v12/lib"
-
-	"knative.dev/pkg/test/mako"
-	"knative.dev/serving/test/performance"
-)
-
-// AggregateResult is the aggregated result of requests for better visualization.
-type AggregateResult struct {
-	// ErrorRates is a map that saves the number of errors for each timestamp (in secs)
-	ErrorRates map[int64]int64
-	// RequestRates is a map that saves the number of requests for each timestamp (in secs)
-	RequestRates map[int64]int64
-}
-
-// NewAggregateResult returns the pointer of a new AggregateResult object.
-func NewAggregateResult(initialSize int) *AggregateResult {
-	return &AggregateResult{
-		ErrorRates:   make(map[int64]int64, initialSize),
-		RequestRates: make(map[int64]int64, initialSize),
-	}
-}
-
-// HandleResult will handle the attack result by:
-// 1. Adding its latency as a sample point if no error, or adding it as an error if there is
-// 2. Updating the aggregate results
-func HandleResult(q *quickstore.Quickstore, benchmarkName string, res vegeta.Result, latencyKey string, ar *AggregateResult) {
-	// Handle the result by reporting an error or a latency sample point.
-	var isAnError int64
-	if res.Error != "" {
-		// By reporting errors like this the error strings show up on
-		// the details page for each Mako run.
-		q.AddError(mako.XTime(res.Timestamp), res.Error)
-		isAnError = 1
-		log.Printf("Error from the app: %#v", res.Error)
-	} else {
-		// Add a sample points for the target benchmark's latency stat
-		// with the latency of the request this result is for.
-		q.AddSamplePoint(mako.XTime(res.Timestamp), map[string]float64{
-			latencyKey: res.Latency.Seconds(),
-		})
-		performance.AddInfluxPoint(benchmarkName, map[string]interface{}{latencyKey: res.Latency.Seconds()})
-		isAnError = 0
-	}
-
-	// Update our error and request rates.
-	// We handle errors this way to force zero values into every time for
-	// which we have data, even if there is no error.
-	ar.ErrorRates[res.Timestamp.Unix()] += isAnError
-	ar.RequestRates[res.Timestamp.Unix()]++
-}
diff --git a/test/performance/performance-tests-mako.sh b/test/performance/performance-tests-mako.sh
old mode 100755
new mode 100644
diff --git a/test/performance/performance-tests.sh b/test/performance/performance-tests.sh
index 1cfb77c97..580d28e83 100755
--- a/test/performance/performance-tests.sh
+++ b/test/performance/performance-tests.sh
@@ -14,8 +14,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-# This script runs the end-to-end tests against Knative Serving built from source.
-# It is started by prow for each PR. For convenience, it can also be executed manually.
+# This script runs the performance tests against Knative
+# Serving built from source. It can be optionally started for each PR.
+# For convenience, it can also be executed manually.
 
 # If you already have a Kubernetes cluster setup and kubectl pointing
 # to it, call this script with the --run-tests arguments and it will use
@@ -25,41 +26,220 @@
 # project $PROJECT_ID, start knative in it, run the tests and delete the
 # cluster.
 
-source $(dirname $0)/../e2e-common.sh
+source $(dirname "$0")/../e2e-common.sh
 
-# Skip installing istio as an add-on.
-# Temporarily increasing the cluster size for serving tests to rule out
-# resource/eviction as causes of flakiness.
-initialize --skip-istio-addon --min-nodes=4 --max-nodes=4 --perf --cluster-version=1.25 "$@"
+set -o errexit
+set -o nounset
+set -o pipefail
 
-header "Running tests"
+declare JOB_NAME
+declare BUILD_ID
+declare ARTIFACTS
 
-function run_kperf() {
-  go_run knative.dev/kperf/cmd/kperf@latest "$@"
+ns="default"
+
+initialize --num-nodes=10 --cluster-version=1.25 "$@"
+
+
+function run_job() {
+  local name=$1
+  local file=$2
+
+  # cleanup from old runs
+  kubectl delete job "$name" -n "$ns" --ignore-not-found=true
+
+  # start the load test and get the logs
+  envsubst < "$file" | ko apply --sbom=none -Bf -
+
+  # sleep a bit to make sure the job is created
+  sleep 5
+
+  # Follow logs to wait for job termination
+  kubectl wait --for=condition=ready -n "$ns" pod --selector=job-name="$name" --timeout=-1s
+  kubectl logs -n "$ns" -f "job.batch/$name"
+
+  # Dump logs to a file to upload it as CI job artifact
+  kubectl logs -n "$ns" "job.batch/$name" >"$ARTIFACTS/$name.log"
+
+  # clean up
+  kubectl delete "job/$name" -n "$ns" --ignore-not-found=true
+  kubectl wait --for=delete "job/$name" --timeout=60s -n "$ns"
 }
 
-mkdir -p "${ARTIFACTS}/kperf"
 
-header "Running performance tests"
-export TIMEOUT=30m
+if ((IS_PROW)); then
+  export INFLUX_URL=$(cat /etc/influx-url-secret-volume/influxdb-url)
+  export INFLUX_TOKEN=$(cat /etc/influx-token-secret-volume/influxdb-token)
+else
+ export JOB_NAME="local"
+ export BUILD_ID="local"
+fi
+
+if [[ -z "${INFLUX_URL}" ]]; then
+  echo "env variable 'INFLUX_URL' not specified!"
+  exit 1
+fi
+if [[ -z "${INFLUX_TOKEN}" ]]; then
+  echo "env variable 'INFLUX_TOKEN' not specified!"
+  exit 1
+fi
+
+echo "Running load test with BUILD_ID: ${BUILD_ID}, JOB_NAME: ${JOB_NAME}, reporting results to: ${INFLUX_URL}"
+
+###############################################################################################
+header "Preparing cluster config"
+
+kubectl delete secret performance-test-config -n "$ns" --ignore-not-found=true
+kubectl create secret generic performance-test-config -n "$ns" \
+  --from-literal=influxurl="${INFLUX_URL}" \
+  --from-literal=influxtoken="${INFLUX_TOKEN}" \
+  --from-literal=jobname="${JOB_NAME}" \
+  --from-literal=buildid="${BUILD_ID}"
+
+echo "Enabling init-containers for the real-traffic test"
+toggle_feature kubernetes.podspec-init-containers enabled config-features
+
+# grafana expects time in milliseconds
+start=$(($(date +%s%N)/1000000))
+
+################################################################################################
+header "Real traffic test"
+
+run_job real-traffic-test "${REPO_ROOT_DIR}/test/performance/benchmarks/real-traffic-test/real-traffic-test.yaml"
+sleep 100 # wait a bit for the cleanup to be done
+kubectl delete ksvc -n "$ns" --all --wait --now
+
+###############################################################################################
+header "Dataplane probe: Setup"
+
+ko apply --sbom=none -Bf "${REPO_ROOT_DIR}/test/performance/benchmarks/dataplane-probe/dataplane-probe-setup.yaml"
+kubectl wait --timeout=60s --for=condition=ready ksvc -n "$ns" --all
+kubectl wait --timeout=60s --for=condition=available deploy -n "$ns" deployment
+
+#############################################################################################
+header "Dataplane probe: deployment"
+
+run_job dataplane-probe-deployment "${REPO_ROOT_DIR}/test/performance/benchmarks/dataplane-probe/dataplane-probe-deployment.yaml"
+
+# additional clean up
+kubectl delete deploy deployment -n "$ns" --ignore-not-found=true
+kubectl delete svc deployment -n "$ns" --ignore-not-found=true
+kubectl wait --for=delete deploy/deployment --timeout=60s -n "$ns"
+kubectl wait --for=delete svc/deployment --timeout=60s -n "$ns"
+
+##############################################################################################
+header "Dataplane probe: activator"
+
+run_job dataplane-probe-activator "${REPO_ROOT_DIR}/test/performance/benchmarks/dataplane-probe/dataplane-probe-activator.yaml"
+
+# additional clean up
+kubectl delete ksvc activator -n "$ns" --ignore-not-found=true
+kubectl wait --for=delete ksvc/activator --timeout=60s -n "$ns"
+
+##############################################################################################
+header "Dataplane probe: queue proxy"
+
+run_job dataplane-probe-queue "${REPO_ROOT_DIR}/test/performance/benchmarks/dataplane-probe/dataplane-probe-queue.yaml"
+
+# additional clean up
+kubectl delete ksvc queue-proxy -n "$ns" --ignore-not-found=true
+kubectl wait --for=delete ksvc/queue-proxy --timeout=60s -n "$ns"
+
+##############################################################################################
+header "Reconciliation delay test"
+
+run_job reconciliation-delay "${REPO_ROOT_DIR}/test/performance/benchmarks/reconciliation-delay/reconciliation-delay.yaml"
+###############################################################################################
+header "Scale from Zero test"
+
+run_job scale-from-zero-1 "${REPO_ROOT_DIR}/test/performance/benchmarks/scale-from-zero/scale-from-zero-1.yaml"
+kubectl delete ksvc -n "$ns" --all --wait --now
+sleep 5 # wait a bit for the cleanup to be done
+
+run_job scale-from-zero-5 "${REPO_ROOT_DIR}/test/performance/benchmarks/scale-from-zero/scale-from-zero-5.yaml"
+kubectl delete ksvc -n "$ns" --all --wait --now
+sleep 25 # wait a bit for the cleanup to be done
+
+run_job scale-from-zero-25 "${REPO_ROOT_DIR}/test/performance/benchmarks/scale-from-zero/scale-from-zero-25.yaml"
+kubectl delete ksvc -n "$ns" --all --wait --now
+sleep 50 # wait a bit for the cleanup to be done
+
+run_job scale-from-zero-100 "${REPO_ROOT_DIR}/test/performance/benchmarks/scale-from-zero/scale-from-zero-100.yaml"
+kubectl delete ksvc -n "$ns" --all --wait --now
+sleep 100 # wait a bit for the cleanup to be done
+
+################################################################################################
+header "Load test: Setup"
+
+ko apply --sbom=none -Bf "${REPO_ROOT_DIR}/test/performance/benchmarks/load-test/load-test-setup.yaml"
+kubectl wait --timeout=60s --for=condition=ready ksvc -n "$ns" --all
+
+#################################################################################################
+header "Load test: zero"
+
+run_job load-test-zero "${REPO_ROOT_DIR}/test/performance/benchmarks/load-test/load-test-0-direct.yaml"
+
+# additional clean up
+kubectl delete ksvc load-test-zero -n "$ns"  --ignore-not-found=true
+kubectl wait --for=delete ksvc/load-test-zero --timeout=60s -n "$ns"
+
+##################################################################################################
+header "Load test: always direct"
+
+run_job load-test-always "${REPO_ROOT_DIR}/test/performance/benchmarks/load-test/load-test-always-direct.yaml"
+
+# additional clean up
+kubectl delete ksvc load-test-always -n "$ns"  --ignore-not-found=true
+kubectl wait --for=delete ksvc/load-test-always --timeout=60s -n "$ns"
+
+#################################################################################################
+header "Load test: 200 direct"
+
+run_job load-test-200 "${REPO_ROOT_DIR}/test/performance/benchmarks/load-test/load-test-200-direct.yaml"
+
+# additional clean up
+kubectl delete ksvc load-test-200 -n "$ns"  --ignore-not-found=true
+kubectl wait --for=delete ksvc/load-test-200 --timeout=60s -n "$ns"
+
+###############################################################################################
+header "Rollout probe: activator direct"
+
+ko apply --sbom=none -Bf "${REPO_ROOT_DIR}/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct.yaml"
+kubectl wait --timeout=800s --for=condition=ready ksvc -n "$ns" --all
+
+run_job rollout-probe-activator-direct "${REPO_ROOT_DIR}/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct.yaml"
+
+# additional clean up
+kubectl delete ksvc activator-with-cc -n "$ns" --ignore-not-found=true
+kubectl wait --for=delete ksvc/activator-with-cc --timeout=60s -n "$ns"
+
+#################################################################################################
+header "Rollout probe: activator direct lin"
+
+ko apply --sbom=none -Bf "${REPO_ROOT_DIR}/test/performance/benchmarks/rollout-probe/rollout-probe-setup-activator-direct-lin.yaml"
+kubectl wait --timeout=800s --for=condition=ready ksvc -n "$ns" --all
+
+run_job rollout-probe-activator-direct-lin "${REPO_ROOT_DIR}/test/performance/benchmarks/rollout-probe/rollout-probe-activator-direct-lin.yaml"
+
+# additional clean up
+kubectl delete ksvc activator-with-cc-lin -n "$ns" --ignore-not-found=true
+kubectl wait --for=delete ksvc/activator-with-cc-lin --timeout=60s -n "$ns"
+
+##################################################################################################
+header "Rollout probe: queue-proxy direct"
+
+ko apply --sbom=none -Bf "${REPO_ROOT_DIR}/test/performance/benchmarks/rollout-probe/rollout-probe-setup-queue-proxy-direct.yaml"
+kubectl wait --timeout=800s --for=condition=ready ksvc -n "$ns" --all
 
-# create services
-run_kperf service generate --number 100 --batch 30 --concurrency 10 --interval 15 --namespace kperf --svc-prefix ktest --wait --timeout 30s --max-scale 3 --min-scale 0 || fail_test "kperf service generate failed"
+run_job rollout-probe-queue-direct "${REPO_ROOT_DIR}/test/performance/benchmarks/rollout-probe/rollout-probe-queue-proxy-direct.yaml"
 
-# wait for scale to zero
-counter=100
-until counter=0
-do
-   sleep 1
-   counter="kubectl get pods -n kperf | awk '{print $1}' | grep domain | wc -l"
-done
+# additional clean up
+kubectl delete ksvc queue-proxy-with-cc -n "$ns" --ignore-not-found=true
+kubectl wait --for=delete ksvc/queue-proxy-with-cc --timeout=60s -n "$ns"
 
-#scale and measure
-run_kperf service scale --namespace kperf  --svc-prefix ktest --range 0,99  --verbose --output "${ARTIFACTS}/kperf" || fail_test "kperf service scale failed"
+# grafana expects time in milliseconds
+end=$(($(date +%s%N)/1000000))
 
-run_kperf service clean --namespace kperf --svc-prefix ktest || fail_test "kperf service clean failed"
+echo "You can find the results here: https://grafana.knative.dev/d/igHJ5-fdk/knative-serving-performance-tests?orgId=1&var-buildid=${BUILD_ID}&from=${start}&to=${end}"
 
-# Remove the kail log file if the test flow passes.
-# This is for preventing too many large log files to be uploaded to GCS in CI.
-rm "${ARTIFACTS}/k8s.log-$(basename "${E2E_SCRIPT}").txt"
 success
diff --git a/test/performance/performance/es.go b/test/performance/performance/es.go
new file mode 100644
index 000000000..6a7794e46
--- /dev/null
+++ b/test/performance/performance/es.go
@@ -0,0 +1,109 @@
+package performance
+
+import (
+	"log"
+	"os"
+	"strings"
+	"time"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	indexers2 "knative.dev/serving/test/performance/performance/indexers"
+)
+
+const (
+	ESServerURLSEnv  = "ES_URL"
+	UseOpenSearcnEnv = "USE_OPEN_SEARCH"
+	UseESEnv         = "USE_ES"
+)
+
+// ESReporter wraps an ES based indexer
+type ESReporter struct {
+	access *indexers2.Indexer
+	tags   map[string]string
+}
+
+func sanitizeIndex(index string) string {
+	indexRaw := strings.ToLower(index)
+	return strings.Replace(indexRaw, " ", "-", -1)
+}
+
+func splitServers(envURLS string) []string {
+	var addrs []string
+	list := strings.Split(envURLS, ",")
+	for _, u := range list {
+		addrs = append(addrs, strings.TrimSpace(u))
+	}
+	return addrs
+}
+
+func NewESReporter(tags map[string]string, indexerType indexers2.IndexerType, index string) (*ESReporter, error) {
+	var servers []string
+
+	if v, b := os.LookupEnv(ESServerURLSEnv); b {
+		servers = splitServers(v)
+	}
+	indexer, err := indexers2.NewIndexer(indexers2.IndexerConfig{
+		Type:               indexerType,
+		Index:              sanitizeIndex(index),
+		Servers:            servers,
+		InsecureSkipVerify: true,
+	})
+	if err != nil {
+		return nil, err
+	}
+
+	buildID, found := os.LookupEnv(buildIDKey)
+	if found {
+		tags[buildIDKey] = buildID
+	}
+	jobName, found := os.LookupEnv(jobNameKey)
+	if found {
+		tags[jobNameKey] = jobName
+	}
+
+	return &ESReporter{
+		access: indexer,
+		tags:   tags,
+	}, nil
+}
+
+func (esr *ESReporter) AddDataPointsForMetrics(m *vegeta.Metrics, benchmarkName string) {
+	metrics := []map[string]interface{}{
+		{
+			"requests":     float64(m.Requests),
+			"rate":         m.Rate,
+			"throughput":   m.Throughput,
+			"duration":     float64(m.Duration),
+			"latency-mean": float64(m.Latencies.Mean),
+			"latency-min":  float64(m.Latencies.Min),
+			"latency-max":  float64(m.Latencies.Max),
+			"latency-p95":  float64(m.Latencies.P95),
+			"success":      m.Success,
+			"errors":       float64(len(m.Errors)),
+			"bytes-in":     float64(m.BytesIn.Total),
+			"bytes-out":    float64(m.BytesOut.Total),
+		},
+	}
+
+	for _, m := range metrics {
+		esr.AddDataPoint(benchmarkName, m)
+	}
+}
+
+func (esr *ESReporter) AddDataPoint(measurement string, fields map[string]interface{}) {
+	p := fields
+	p["_measurement"] = measurement
+	p["tags"] = esr.tags
+	// Use the same format as in influxdb
+	p["@timestamp"] = time.Now().Format(time.RFC3339Nano)
+	docs := []interface{}{p}
+	msg, err := (*esr.access).Index(docs, indexers2.IndexingOpts{})
+	if err != nil {
+		log.Printf("Indexing failed: %s", err.Error())
+	}
+	log.Printf("%s\n", msg)
+}
+
+func (esr *ESReporter) FlushAndShutdown() {
+
+}
diff --git a/test/performance/performance/indexers/elastic.go b/test/performance/performance/indexers/elastic.go
new file mode 100644
index 000000000..c7b48726e
--- /dev/null
+++ b/test/performance/performance/indexers/elastic.go
@@ -0,0 +1,132 @@
+// Copyright 2023 The go-commons Authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package indexers
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"crypto/tls"
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"net/http"
+	"runtime"
+	"strings"
+	"sync"
+	"time"
+
+	elasticsearch "github.com/elastic/go-elasticsearch/v7"
+	"github.com/elastic/go-elasticsearch/v7/esutil"
+)
+
+const elastic = "elastic"
+
+// Elastic ElasticSearch instance
+type Elastic struct {
+	index string
+}
+
+// ESClient elasticsearch client instance
+var ESClient *elasticsearch.Client
+
+// Init function
+func init() {
+	indexerMap[elastic] = &Elastic{}
+}
+
+// Returns new indexer for elastic search
+func (esIndexer *Elastic) new(indexerConfig IndexerConfig) error {
+	var err error
+	if indexerConfig.Index == "" {
+		return fmt.Errorf("index name not specified")
+	}
+	esIndex := strings.ToLower(indexerConfig.Index)
+	cfg := elasticsearch.Config{
+		Addresses: indexerConfig.Servers,
+		Transport: &http.Transport{TLSClientConfig: &tls.Config{InsecureSkipVerify: indexerConfig.InsecureSkipVerify}},
+	}
+	ESClient, err = elasticsearch.NewClient(cfg)
+	if err != nil {
+		return fmt.Errorf("error creating the ES client: %s", err)
+	}
+	r, err := ESClient.Cluster.Health()
+	if err != nil {
+		return fmt.Errorf("ES health check failed: %s", err)
+	}
+	if r.StatusCode != 200 {
+		return fmt.Errorf("unexpected ES status code: %d", r.StatusCode)
+	}
+	esIndexer.index = esIndex
+	r, _ = ESClient.Indices.Exists([]string{esIndex})
+	if r.IsError() {
+		r, _ = ESClient.Indices.Create(esIndex)
+		if r.IsError() {
+			return fmt.Errorf("error creating index %s on ES: %s", esIndex, r.String())
+		}
+	}
+	return nil
+}
+
+// Index uses bulkIndexer to index the documents in the given index
+func (esIndexer *Elastic) Index(documents []interface{}, opts IndexingOpts) (string, error) {
+	var statString string
+	var indexerStatsLock sync.Mutex
+	indexerStats := make(map[string]int)
+	hasher := sha256.New()
+	bi, err := esutil.NewBulkIndexer(esutil.BulkIndexerConfig{
+		Client:     ESClient,
+		Index:      esIndexer.index,
+		FlushBytes: 5e+6,
+		NumWorkers: runtime.NumCPU(),
+		Timeout:    10 * time.Minute, // TODO: hardcoded
+	})
+	if err != nil {
+		return "", fmt.Errorf("Error creating the indexer: %s", err)
+	}
+	start := time.Now().UTC()
+	for _, document := range documents {
+		j, err := json.Marshal(document)
+		if err != nil {
+			return "", fmt.Errorf("Cannot encode document %s: %s", document, err)
+		}
+		hasher.Write(j)
+		err = bi.Add(
+			context.Background(),
+			esutil.BulkIndexerItem{
+				Action:     "index",
+				Body:       bytes.NewReader(j),
+				DocumentID: hex.EncodeToString(hasher.Sum(nil)),
+				OnSuccess: func(c context.Context, bii esutil.BulkIndexerItem, biri esutil.BulkIndexerResponseItem) {
+					indexerStatsLock.Lock()
+					defer indexerStatsLock.Unlock()
+					indexerStats[biri.Result]++
+				},
+			},
+		)
+		if err != nil {
+			return "", fmt.Errorf("Unexpected ES indexing error: %s", err)
+		}
+		hasher.Reset()
+	}
+	if err := bi.Close(context.Background()); err != nil {
+		return "", fmt.Errorf("Unexpected ES error: %s", err)
+	}
+	dur := time.Since(start)
+	for stat, val := range indexerStats {
+		statString += fmt.Sprintf(" %s=%d", stat, val)
+	}
+	return fmt.Sprintf("Indexing finished in %v:%v", dur.Truncate(time.Millisecond), statString), nil
+}
diff --git a/test/performance/performance/indexers/opensearch.go b/test/performance/performance/indexers/opensearch.go
new file mode 100644
index 000000000..d8e17bda9
--- /dev/null
+++ b/test/performance/performance/indexers/opensearch.go
@@ -0,0 +1,132 @@
+// Copyright 2023 The go-commons Authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package indexers
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"crypto/tls"
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"net/http"
+	"runtime"
+	"strings"
+	"sync"
+	"time"
+
+	opensearch "github.com/opensearch-project/opensearch-go"
+	opensearchutil "github.com/opensearch-project/opensearch-go/opensearchutil"
+)
+
+const indexer = "opensearch"
+
+// OSClient OpenSearch client instance
+var OSClient *opensearch.Client
+
+// OpenSearch OpenSearch instance
+type OpenSearch struct {
+	index string
+}
+
+// Init function
+func init() {
+	indexerMap[indexer] = &OpenSearch{}
+}
+
+// Returns new indexer for OpenSearch
+func (OpenSearchIndexer *OpenSearch) new(indexerConfig IndexerConfig) error {
+	var err error
+	if indexerConfig.Index == "" {
+		return fmt.Errorf("index name not specified")
+	}
+	OpenSearchIndex := strings.ToLower(indexerConfig.Index)
+	cfg := opensearch.Config{
+		Addresses: indexerConfig.Servers,
+		Transport: &http.Transport{TLSClientConfig: &tls.Config{InsecureSkipVerify: indexerConfig.InsecureSkipVerify}},
+	}
+	OSClient, err = opensearch.NewClient(cfg)
+	if err != nil {
+		return fmt.Errorf("error creating the OpenSearch client: %s", err)
+	}
+	r, err := OSClient.Cluster.Health()
+	if err != nil {
+		return fmt.Errorf("OpenSearch health check failed: %s", err)
+	}
+	if r.StatusCode != 200 {
+		return fmt.Errorf("unexpected OpenSearch status code: %d", r.StatusCode)
+	}
+	OpenSearchIndexer.index = OpenSearchIndex
+	r, _ = OSClient.Indices.Exists([]string{OpenSearchIndex})
+	if r.IsError() {
+		r, _ = OSClient.Indices.Create(OpenSearchIndex)
+		if r.IsError() {
+			return fmt.Errorf("error creating index %s on OpenSearch: %s", OpenSearchIndex, r.String())
+		}
+	}
+	return nil
+}
+
+// Index uses bulkIndexer to index the documents in the given index
+func (OpenSearchIndexer *OpenSearch) Index(documents []interface{}, opts IndexingOpts) (string, error) {
+	var statString string
+	var indexerStatsLock sync.Mutex
+	indexerStats := make(map[string]int)
+	hasher := sha256.New()
+	bi, err := opensearchutil.NewBulkIndexer(opensearchutil.BulkIndexerConfig{
+		Client:     OSClient,
+		Index:      OpenSearchIndexer.index,
+		FlushBytes: 5e+6,
+		NumWorkers: runtime.NumCPU(),
+		Timeout:    10 * time.Minute, // TODO: hardcoded
+	})
+	if err != nil {
+		return "", fmt.Errorf("Error creating the indexer: %s", err)
+	}
+	start := time.Now().UTC()
+	for _, document := range documents {
+		j, err := json.Marshal(document)
+		if err != nil {
+			return "", fmt.Errorf("Cannot encode document %s: %s", document, err)
+		}
+		hasher.Write(j)
+		err = bi.Add(
+			context.Background(),
+			opensearchutil.BulkIndexerItem{
+				Action:     "index",
+				Body:       bytes.NewReader(j),
+				DocumentID: hex.EncodeToString(hasher.Sum(nil)),
+				OnSuccess: func(c context.Context, bii opensearchutil.BulkIndexerItem, biri opensearchutil.BulkIndexerResponseItem) {
+					indexerStatsLock.Lock()
+					defer indexerStatsLock.Unlock()
+					indexerStats[biri.Result]++
+				},
+			},
+		)
+		if err != nil {
+			return "", fmt.Errorf("Unexpected OpenSearch indexing error: %s", err)
+		}
+		hasher.Reset()
+	}
+	if err := bi.Close(context.Background()); err != nil {
+		return "", fmt.Errorf("Unexpected OpenSearch error: %s", err)
+	}
+	dur := time.Since(start)
+	for stat, val := range indexerStats {
+		statString += fmt.Sprintf(" %s=%d", stat, val)
+	}
+	return fmt.Sprintf("Indexing finished in %v:%v", dur.Truncate(time.Millisecond), statString), nil
+}
diff --git a/test/performance/performance/indexers/types.go b/test/performance/performance/indexers/types.go
new file mode 100644
index 000000000..2317c746a
--- /dev/null
+++ b/test/performance/performance/indexers/types.go
@@ -0,0 +1,77 @@
+// Copyright 2023 The go-commons Authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package indexers
+
+import "fmt"
+
+// Types of indexers
+const (
+	// Elastic indexer that sends metrics to the configured ES instance
+	ElasticIndexer IndexerType = "elastic"
+	// OpenSearch indexer that sends metrics to the configured Search Instance
+	OpenSearchIndexer IndexerType = "opensearch"
+	// Local indexer that writes metrics to local directory
+	LocalIndexer IndexerType = "local"
+)
+
+var indexerMap = make(map[IndexerType]Indexer)
+
+// Indexer interface
+type Indexer interface {
+	Index([]interface{}, IndexingOpts) (string, error)
+	new(IndexerConfig) error
+}
+
+// Indexing options
+type IndexingOpts struct {
+	MetricName string // MetricName, required for local indexer
+}
+
+// IndexerType type of indexer
+type IndexerType string
+
+// IndexerConfig holds the indexer configuration
+type IndexerConfig struct {
+	// Type type of indexer
+	Type IndexerType `yaml:"type"`
+	// Servers List of ElasticSearch instances
+	Servers []string `yaml:"esServers"`
+	// Index index to send documents to server
+	Index string `yaml:"defaultIndex"`
+	// InsecureSkipVerify disable TLS ceriticate verification
+	InsecureSkipVerify bool `yaml:"insecureSkipVerify"`
+	// Directory to save metrics files in
+	MetricsDirectory string `yaml:"metricsDirectory"`
+	// Create tarball
+	CreateTarball bool `yaml:"createTarball"`
+	// TarBall name
+	TarballName string `yaml:"tarballName"`
+}
+
+// NewIndexer creates a new Indexer with the specified IndexerConfig
+func NewIndexer(indexerConfig IndexerConfig) (*Indexer, error) {
+	var indexer Indexer
+	var exists bool
+	cfg := indexerConfig
+	if indexer, exists = indexerMap[cfg.Type]; exists {
+		err := indexer.new(indexerConfig)
+		if err != nil {
+			return &indexer, err
+		}
+	} else {
+		return &indexer, fmt.Errorf("Indexer not found: %s", cfg.Type)
+	}
+	return &indexer, nil
+}
diff --git a/test/performance/performance/influx.go b/test/performance/performance/influx.go
new file mode 100644
index 000000000..65e9e65f3
--- /dev/null
+++ b/test/performance/performance/influx.go
@@ -0,0 +1,132 @@
+/*
+Copyright 2023 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package performance
+
+import (
+	"crypto/tls"
+	"fmt"
+	"log"
+	"os"
+	"time"
+
+	influxdb2 "github.com/influxdata/influxdb-client-go/v2"
+	"github.com/influxdata/influxdb-client-go/v2/api"
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+)
+
+const (
+	influxToken = "INFLUX_TOKEN"
+	influxURL   = "INFLUX_URL"
+	jobNameKey  = "JOB_NAME"
+	buildIDKey  = "BUILD_ID"
+	org         = "Knativetest"
+	bucket      = "knative-serving"
+)
+
+// InfluxReporter wraps a influxdb client
+type InfluxReporter struct {
+	client   influxdb2.Client
+	writeAPI api.WriteAPI
+	tags     map[string]string
+}
+
+// NewInfluxReporter creates a InfluxReporter
+// The method expects tags to be provided as a map. These are used to identify different runs.
+func NewInfluxReporter(tags map[string]string) (*InfluxReporter, error) {
+	url, err := getEnvVariable(influxURL)
+	if err != nil {
+		return nil, err
+	}
+
+	token, err := getEnvVariable(influxToken)
+	if err != nil {
+		return nil, err
+	}
+
+	client := influxdb2.NewClientWithOptions(url, token,
+		influxdb2.DefaultOptions().
+			SetUseGZip(true).
+			//nolint:gosec // We explicitly don't need to check certs here since this is test code.
+			SetTLSConfig(&tls.Config{InsecureSkipVerify: true}))
+
+	writeAPI := client.WriteAPI(org, bucket)
+
+	buildID, found := os.LookupEnv(buildIDKey)
+	if found {
+		tags[buildIDKey] = buildID
+	}
+	jobName, found := os.LookupEnv(jobNameKey)
+	if found {
+		tags[jobNameKey] = jobName
+	}
+
+	return &InfluxReporter{
+		client:   client,
+		writeAPI: writeAPI,
+		tags:     tags,
+	}, nil
+}
+
+// FlushAndShutdown flushes the data to influxdb and terminates the client.
+func (ir *InfluxReporter) FlushAndShutdown() {
+	log.Println("Shutting down InfluxReporter")
+	ir.writeAPI.Flush()
+	ir.client.Close()
+}
+
+// AddDataPoint asynchronously writes a new data-point to influxdb.
+func (ir *InfluxReporter) AddDataPoint(measurement string, fields map[string]interface{}) {
+	p := influxdb2.NewPoint(measurement,
+		ir.tags,
+		fields,
+		time.Now())
+
+	// Write point asynchronously
+	ir.writeAPI.WritePoint(p)
+}
+
+// AddDataPointsForMetrics reports vegeta.Metrics to influxdb
+func (ir *InfluxReporter) AddDataPointsForMetrics(m *vegeta.Metrics, benchmarkName string) {
+	metrics := []map[string]interface{}{
+		{
+			"requests":     float64(m.Requests),
+			"rate":         m.Rate,
+			"throughput":   m.Throughput,
+			"duration":     float64(m.Duration),
+			"latency-mean": float64(m.Latencies.Mean),
+			"latency-min":  float64(m.Latencies.Min),
+			"latency-max":  float64(m.Latencies.Max),
+			"latency-p95":  float64(m.Latencies.P95),
+			"success":      m.Success,
+			"errors":       float64(len(m.Errors)),
+			"bytes-in":     float64(m.BytesIn.Total),
+			"bytes-out":    float64(m.BytesOut.Total),
+		},
+	}
+
+	for _, m := range metrics {
+		ir.AddDataPoint(benchmarkName, m)
+	}
+}
+
+func getEnvVariable(envVarName string) (string, error) {
+	valueFromEnv, ok := os.LookupEnv(envVarName)
+	if !ok {
+		return "", fmt.Errorf("failed to get env variable: %s", envVarName)
+	}
+	return valueFromEnv, nil
+}
diff --git a/test/performance/performance.go b/test/performance/performance/performance.go
similarity index 96%
rename from test/performance/performance.go
rename to test/performance/performance/performance.go
index 1c235defe..22b096f7c 100644
--- a/test/performance/performance.go
+++ b/test/performance/performance/performance.go
@@ -73,7 +73,7 @@ func WaitForScaleToZero(ctx context.Context, namespace string, selector labels.S
 				return false, nil
 			}
 		}
-		log.Print("All pods are done or terminating after ", time.Since(begin))
+		log.Print("All pods are done (scaled to zero) or terminating after ", time.Since(begin))
 		return true, nil
 	})
 }
diff --git a/test/performance/performance/reporter.go b/test/performance/performance/reporter.go
new file mode 100644
index 000000000..6a3c41539
--- /dev/null
+++ b/test/performance/performance/reporter.go
@@ -0,0 +1,55 @@
+package performance
+
+import (
+	"os"
+	"strconv"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+	"knative.dev/serving/test/performance/performance/indexers"
+)
+
+type DataPointReporter interface {
+	AddDataPoint(measurement string, fields map[string]interface{})
+	AddDataPointsForMetrics(m *vegeta.Metrics, benchmarkName string)
+	FlushAndShutdown()
+}
+
+func NewDataPointReporterFactory(tags map[string]string, index string) (DataPointReporter, error) {
+	var reporter DataPointReporter
+	var err error
+	useDefaultReporter := true
+
+	if v, b := os.LookupEnv(UseESEnv); b {
+		if b, err = strconv.ParseBool(v); err == nil {
+			if b {
+				useDefaultReporter = false
+				reporter, err = NewESReporter(tags, indexers.ElasticIndexer, index)
+				if err != nil {
+					return nil, err
+				}
+			}
+		}
+	}
+
+	if v, b := os.LookupEnv(UseOpenSearcnEnv); b {
+		if b, err = strconv.ParseBool(v); err == nil {
+			if b {
+				useDefaultReporter = false
+				reporter, err = NewESReporter(tags, indexers.OpenSearchIndexer, index)
+				if err != nil {
+					return nil, err
+				}
+			}
+		}
+	}
+
+	if useDefaultReporter {
+		reporter, err = NewInfluxReporter(tags)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	rep := interface{}(reporter).(DataPointReporter)
+	return rep, nil
+}
diff --git a/test/performance/metrics/runtime.go b/test/performance/performance/runtime.go
similarity index 99%
rename from test/performance/metrics/runtime.go
rename to test/performance/performance/runtime.go
index 295f85097..699dd6973 100644
--- a/test/performance/metrics/runtime.go
+++ b/test/performance/performance/runtime.go
@@ -14,7 +14,7 @@ See the License for the specific language governing permissions and
 limitations under the License.
 */
 
-package metrics
+package performance
 
 import (
 	"context"
diff --git a/test/performance/performance/vegeta.go b/test/performance/performance/vegeta.go
new file mode 100644
index 000000000..e572c26dc
--- /dev/null
+++ b/test/performance/performance/vegeta.go
@@ -0,0 +1,69 @@
+/*
+Copyright 2023 The Knative Authors
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+*/
+
+package performance
+
+import (
+	"log"
+
+	vegeta "github.com/tsenart/vegeta/v12/lib"
+)
+
+// VegetaReporter wraps vegeta.Metrics to report metrics via channels.
+// Call StopAndCollectMetrics() to finish reporting and return the collected metrics.
+type VegetaReporter struct {
+	metrics *vegeta.Metrics
+	results chan *vegeta.Result
+	stop    chan bool
+}
+
+// NewVegetaReporter creates a new VegetaReporter.
+func NewVegetaReporter() *VegetaReporter {
+	reporter := &VegetaReporter{
+		metrics: &vegeta.Metrics{},
+		results: make(chan *vegeta.Result, 1000),
+		stop:    make(chan bool),
+	}
+
+	go reporter.run()
+
+	return reporter
+}
+
+// AddResult adds a results asynchronously.
+func (vr *VegetaReporter) AddResult(r *vegeta.Result) {
+	vr.results <- r
+}
+
+// StopAndCollectMetrics flushes the metrics and collects latency distribution,
+// shuts down the reporter and returns the collected metrics.
+func (vr *VegetaReporter) StopAndCollectMetrics() *vegeta.Metrics {
+	log.Println("Shutting down VegetaReporter")
+	vr.stop <- true
+	vr.metrics.Close()
+	return vr.metrics
+}
+
+func (vr *VegetaReporter) run() {
+	for {
+		select {
+		case r := <-vr.results:
+			vr.metrics.Add(r)
+		case <-vr.stop:
+			return
+		}
+	}
+}
diff --git a/test/performance/read_results.sh b/test/performance/read_results.sh
deleted file mode 100755
index 4fa533999..000000000
--- a/test/performance/read_results.sh
+++ /dev/null
@@ -1,83 +0,0 @@
-#!/usr/bin/env bash
-
-# Copyright 2022 The Knative Authors
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-# This script runs the end-to-end tests against Knative Serving built from source.
-# It is started by prow for each PR. For convenience, it can also be executed manually.
-
-# If you already have a Kubernetes cluster setup and kubectl pointing
-# to it, call this script with the --run-tests arguments and it will use
-# the cluster and run the tests.
-
-# Calling this script without arguments will create a new cluster in
-# project $PROJECT_ID, start knative in it, run the tests and delete the
-# cluster.
-
-check_command_exists() {
-  CMD_NAME=$1
-  command -v "$CMD_NAME" > /dev/null || {
-    echo "Command $CMD_NAME does not exist"
-    exit 1
-  }
-}
-
-check_command_exists kubectl
-check_command_exists curl
-
-if [[ $# -lt 7 ]]
-then
-  echo "Usage: $0 <mako_stub_pod_name> <mako_stub_namespace> <mako_stub_port> <timeout> <retries> <retries_interval> <out_file>"
-  exit 1
-fi
-
-MAKO_STUB_POD_NAME="$1"
-MAKO_STUB_NAMESPACE="$2"
-MAKO_STUB_PORT="$3"
-TIMEOUT="$4"
-RETRIES="$5"
-RETRIES_INTERVAL="$6"
-OUTPUT_FILE="$7"
-
-# Find port ready to use
-
-port=10000
-isfree=$(netstat -tapln | grep $port)
-
-while [[ -n "$isfree" ]]; do
-  port=$((port + 1))
-  isfree=$(netstat -tapln | grep $port)
-done
-
-for i in $(seq $RETRIES); do
-  kubectl port-forward -n "$MAKO_STUB_NAMESPACE" "$MAKO_STUB_POD_NAME" $port:$MAKO_STUB_PORT &
-  PORT_FORWARD_PID=$!
-
-  sleep 10
-
-  curl --connect-timeout $TIMEOUT "http://localhost:$port/results" > $OUTPUT_FILE
-  curl_exit_status=$?
-
-  kill $PORT_FORWARD_PID
-  wait $PORT_FORWARD_PID 2>/dev/null
-
-  if [ 0 -eq $curl_exit_status ]; then
-    exit 0
-  else
-    sleep $RETRIES_INTERVAL
-  fi
-
-done
-
-exit 1
diff --git a/test/performance/visualization/grafana-dashboard.json b/test/performance/visualization/grafana-dashboard.json
new file mode 100644
index 000000000..cbcc939be
--- /dev/null
+++ b/test/performance/visualization/grafana-dashboard.json
@@ -0,0 +1,3991 @@
+{
+  "annotations": {
+    "list": [
+      {
+        "builtIn": 1,
+        "datasource": {
+          "type": "datasource",
+          "uid": "grafana"
+        },
+        "enable": true,
+        "hide": true,
+        "iconColor": "rgba(0, 211, 255, 1)",
+        "name": "Annotations & Alerts",
+        "target": {
+          "limit": 100,
+          "matchAny": false,
+          "tags": [],
+          "type": "dashboard"
+        },
+        "type": "dashboard"
+      }
+    ]
+  },
+  "editable": true,
+  "fiscalYearStartMonth": 0,
+  "graphTooltip": 0,
+  "id": 6395,
+  "links": [],
+  "liveNow": false,
+  "panels": [
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 1,
+        "w": 24,
+        "x": 0,
+        "y": 0
+      },
+      "id": 15,
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "refId": "A"
+        }
+      ],
+      "title": "Dataplane probe results",
+      "type": "row"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 21,
+        "x": 0,
+        "y": 1
+      },
+      "id": 11,
+      "options": {
+        "content": "* Deployment = Vanilla k8s Deployment\n* Activator = Always hook the activator in\n* Queue = Only hook the activator in when scaled to zero.\n",
+        "mode": "markdown"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Dataplane probe results",
+      "type": "text"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 5
+      },
+      "id": 17,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"requests\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Requests",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 105
+              },
+              {
+                "color": "red",
+                "value": 115
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 5
+      },
+      "id": 19,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-min\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: min",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 115
+              },
+              {
+                "color": "red",
+                "value": 120
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 5
+      },
+      "id": 21,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-mean\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: mean",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 1
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 8
+      },
+      "id": 18,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"errors\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Errors",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA:\ndeployment = 105ms\nactivator = 110ms\nqueue = 110ms",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 105
+              },
+              {
+                "color": "red",
+                "value": 115
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 8
+      },
+      "id": 20,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-p95\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: p95 ",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 3500
+              },
+              {
+                "color": "red",
+                "value": 5000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 8
+      },
+      "id": 22,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-max\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: max",
+      "type": "stat"
+    },
+    {
+      "aliasColors": {},
+      "bars": false,
+      "dashLength": 10,
+      "dashes": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "displayName": "${__field.labels.target}",
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "fill": 1,
+      "fillGradient": 0,
+      "gridPos": {
+        "h": 5,
+        "w": 21,
+        "x": 0,
+        "y": 11
+      },
+      "hiddenSeries": false,
+      "id": 30,
+      "legend": {
+        "avg": false,
+        "current": false,
+        "max": false,
+        "min": false,
+        "show": true,
+        "total": false,
+        "values": false
+      },
+      "lines": true,
+      "linewidth": 1,
+      "nullPointMode": "null",
+      "options": {
+        "alertThreshold": true
+      },
+      "percentage": false,
+      "pluginVersion": "9.1.1",
+      "pointradius": 2,
+      "points": false,
+      "renderer": "flot",
+      "seriesOverrides": [],
+      "spaceLength": 10,
+      "stack": false,
+      "steppedLine": false,
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving dataplane probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"activator-pod-count\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "thresholds": [],
+      "timeRegions": [],
+      "title": "Activator pod count",
+      "tooltip": {
+        "shared": true,
+        "sort": 0,
+        "value_type": "individual"
+      },
+      "type": "graph",
+      "xaxis": {
+        "mode": "time",
+        "show": true,
+        "values": []
+      },
+      "yaxes": [
+        {
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        },
+        {
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        }
+      ],
+      "yaxis": {
+        "align": false
+      }
+    },
+    {
+      "collapsed": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 1,
+        "w": 24,
+        "x": 0,
+        "y": 16
+      },
+      "id": 2,
+      "panels": [],
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "refId": "A"
+        }
+      ],
+      "title": "Load test results",
+      "type": "row"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 21,
+        "x": 0,
+        "y": 17
+      },
+      "id": 23,
+      "options": {
+        "content": "* Zero = Only hook the activator in at zero\n* Always = Always hook the activator in.\n* 200 = Hook the activator in until we reach 200 concurrent requests",
+        "mode": "markdown"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Load test results",
+      "type": "text"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "flavor: ${__field.labels.flavor}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 21
+      },
+      "id": 4,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"requests\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Requests",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "flavor: ${__field.labels.flavor}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 105
+              },
+              {
+                "color": "red",
+                "value": 115
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 21
+      },
+      "id": 6,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-min\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: min",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "flavor: ${__field.labels.flavor}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 115
+              },
+              {
+                "color": "red",
+                "value": 120
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 21
+      },
+      "id": 8,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-mean\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: mean",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- 0 errors",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "flavor: ${__field.labels.flavor}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 1
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 24
+      },
+      "id": 5,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"errors\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Errors",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 115ms",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "flavor: ${__field.labels.flavor}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 116
+              },
+              {
+                "color": "red",
+                "value": 120
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 24
+      },
+      "id": 7,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-p95\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: p95",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "flavor: ${__field.labels.flavor}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 10001
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 24
+      },
+      "id": 37,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-max\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: max",
+      "type": "stat"
+    },
+    {
+      "aliasColors": {},
+      "bars": false,
+      "dashLength": 10,
+      "dashes": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "displayName": "${__field.name}, flavor: ${__field.labels.flavor}"
+        },
+        "overrides": []
+      },
+      "fill": 1,
+      "fillGradient": 0,
+      "gridPos": {
+        "h": 7,
+        "w": 21,
+        "x": 0,
+        "y": 27
+      },
+      "hiddenSeries": false,
+      "id": 31,
+      "legend": {
+        "avg": false,
+        "current": false,
+        "max": false,
+        "min": false,
+        "show": true,
+        "total": false,
+        "values": false
+      },
+      "lines": true,
+      "linewidth": 1,
+      "nullPointMode": "null",
+      "options": {
+        "alertThreshold": true
+      },
+      "percentage": false,
+      "pluginVersion": "9.1.1",
+      "pointradius": 2,
+      "points": false,
+      "renderer": "flot",
+      "seriesOverrides": [],
+      "spaceLength": 10,
+      "stack": false,
+      "steppedLine": false,
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"desired-replicas\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        },
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving load test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"ready-replicas\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "B",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "thresholds": [],
+      "timeRegions": [],
+      "title": "Ready pods",
+      "tooltip": {
+        "shared": true,
+        "sort": 0,
+        "value_type": "individual"
+      },
+      "type": "graph",
+      "xaxis": {
+        "mode": "time",
+        "show": true,
+        "values": []
+      },
+      "yaxes": [
+        {
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        },
+        {
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        }
+      ],
+      "yaxis": {
+        "align": false
+      }
+    },
+    {
+      "collapsed": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 1,
+        "w": 24,
+        "x": 0,
+        "y": 34
+      },
+      "id": 13,
+      "panels": [],
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "refId": "A"
+        }
+      ],
+      "title": "Rollout probe results",
+      "type": "row"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 21,
+        "x": 0,
+        "y": 35
+      },
+      "id": 16,
+      "options": {
+        "content": "* Activator Direct = Always hook the activator in: weightedExponential\n* Activator Direct Lin = Always hook the activator in: linear\n* Queue = Only hook the activator in when scaled to zero.\n",
+        "mode": "markdown"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Rollout probe results",
+      "type": "text"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 39
+      },
+      "id": 24,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"requests\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Requests",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 105
+              },
+              {
+                "color": "red",
+                "value": 115
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 39
+      },
+      "id": 26,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-min\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: min",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 115
+              },
+              {
+                "color": "red",
+                "value": 120
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 39
+      },
+      "id": 28,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-mean\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: mean",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 1
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 42
+      },
+      "id": 25,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"errors\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Errors",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA:\n- max: 110ms",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 110
+              },
+              {
+                "color": "red",
+                "value": 111
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 42
+      },
+      "id": 27,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-p95\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: p95",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "${__field.labels.target}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 3500
+              },
+              {
+                "color": "red",
+                "value": 5000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 42
+      },
+      "id": 29,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-max\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: max",
+      "type": "stat"
+    },
+    {
+      "aliasColors": {},
+      "bars": false,
+      "dashLength": 10,
+      "dashes": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fill": 1,
+      "fillGradient": 0,
+      "gridPos": {
+        "h": 7,
+        "w": 21,
+        "x": 0,
+        "y": 45
+      },
+      "hiddenSeries": false,
+      "id": 32,
+      "legend": {
+        "avg": false,
+        "current": false,
+        "max": false,
+        "min": false,
+        "show": true,
+        "total": false,
+        "values": false
+      },
+      "lines": true,
+      "linewidth": 1,
+      "nullPointMode": "null",
+      "options": {
+        "alertThreshold": true
+      },
+      "percentage": false,
+      "pluginVersion": "9.1.1",
+      "pointradius": 2,
+      "points": false,
+      "renderer": "flot",
+      "seriesOverrides": [],
+      "spaceLength": 10,
+      "stack": false,
+      "steppedLine": false,
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"desired-pods\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        },
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving rollout probe\")\n  |> filter(fn: (r) => r[\"_field\"] == \"available-pods\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "B",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "thresholds": [],
+      "timeRegions": [],
+      "title": "Desired/Available pods",
+      "tooltip": {
+        "shared": true,
+        "sort": 0,
+        "value_type": "individual"
+      },
+      "type": "graph",
+      "xaxis": {
+        "mode": "time",
+        "show": true,
+        "values": []
+      },
+      "yaxes": [
+        {
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        },
+        {
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        }
+      ],
+      "yaxis": {
+        "align": false
+      }
+    },
+    {
+      "collapsed": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 1,
+        "w": 24,
+        "x": 0,
+        "y": 52
+      },
+      "id": 34,
+      "panels": [],
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "refId": "A"
+        }
+      ],
+      "title": "Scale from zero results",
+      "type": "row"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 25s",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "parallel: ${__field.labels.parallel}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "yellow",
+                "value": 25000
+              },
+              {
+                "color": "red",
+                "value": 25001
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 7,
+        "x": 0,
+        "y": 53
+      },
+      "id": 35,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "min"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving scale from zero\")\n  |> filter(fn: (r) => r[\"_field\"] == \"service-ready-latency\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Service ready latency (min)",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 25s",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "parallel: ${__field.labels.parallel}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 25000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 7,
+        "x": 7,
+        "y": 53
+      },
+      "id": 40,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "mean"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving scale from zero\")\n  |> filter(fn: (r) => r[\"_field\"] == \"service-ready-latency\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Service ready latency (mean)",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 25s",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "parallel: ${__field.labels.parallel}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 25000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 7,
+        "x": 14,
+        "y": 53
+      },
+      "id": 38,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "max"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving scale from zero\")\n  |> filter(fn: (r) => r[\"_field\"] == \"service-ready-latency\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Service ready latency (max)",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 25s",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "parallel: ${__field.labels.parallel}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 25000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 7,
+        "x": 0,
+        "y": 57
+      },
+      "id": 36,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "min"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving scale from zero\")\n  |> filter(fn: (r) => r[\"_field\"] == \"deployment-updated-latency\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Deployment updated latency (min)",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 25s",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "parallel: ${__field.labels.parallel}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 25000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 7,
+        "x": 7,
+        "y": 57
+      },
+      "id": 41,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "mean"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving scale from zero\")\n  |> filter(fn: (r) => r[\"_field\"] == \"deployment-updated-latency\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Deployment updated latency (mean)",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA\n- max: 25s",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "displayName": "parallel: ${__field.labels.parallel}",
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 25000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 4,
+        "w": 7,
+        "x": 14,
+        "y": 57
+      },
+      "id": 39,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "none",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "max"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "hide": false,
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving scale from zero\")\n  |> filter(fn: (r) => r[\"_field\"] == \"deployment-updated-latency\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Deployment updated latency (max)",
+      "type": "stat"
+    },
+    {
+      "collapsed": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "gridPos": {
+        "h": 1,
+        "w": 24,
+        "x": 0,
+        "y": 61
+      },
+      "id": 43,
+      "panels": [],
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "refId": "A"
+        }
+      ],
+      "title": "Reconciliation delay",
+      "type": "row"
+    },
+    {
+      "aliasColors": {},
+      "bars": false,
+      "dashLength": 10,
+      "dashes": false,
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "SLA:\n- max: 5s",
+      "fieldConfig": {
+        "defaults": {
+          "displayName": "${__field.name}"
+        },
+        "overrides": []
+      },
+      "fill": 1,
+      "fillGradient": 0,
+      "gridPos": {
+        "h": 8,
+        "w": 21,
+        "x": 0,
+        "y": 62
+      },
+      "hiddenSeries": false,
+      "id": 45,
+      "legend": {
+        "avg": false,
+        "current": false,
+        "max": false,
+        "min": false,
+        "show": true,
+        "total": false,
+        "values": false
+      },
+      "lines": true,
+      "linewidth": 1,
+      "nullPointMode": "null",
+      "options": {
+        "alertThreshold": true
+      },
+      "percentage": false,
+      "pluginVersion": "9.1.1",
+      "pointradius": 2,
+      "points": false,
+      "renderer": "flot",
+      "seriesOverrides": [],
+      "spaceLength": 10,
+      "stack": false,
+      "steppedLine": false,
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving reconciliation delay\")\n  |> filter(fn: (r) => r[\"_field\"] == \"Configuration\" or r[\"_field\"] == \"Ingress\" or r[\"_field\"] == \"PodAutoscaler\" or r[\"_field\"] == \"Revision\" or r[\"_field\"] == \"Route\" or r[\"_field\"] == \"Service\" or r[\"_field\"] == \"ServerlessService\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |> yield(name: \"mean\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "thresholds": [
+        {
+          "$$hashKey": "object:127",
+          "colorMode": "critical",
+          "fill": true,
+          "line": true,
+          "op": "lt",
+          "value": 5,
+          "yaxis": "left"
+        }
+      ],
+      "timeRegions": [],
+      "title": "Reconciliation delay per CustomResource",
+      "tooltip": {
+        "shared": true,
+        "sort": 0,
+        "value_type": "individual"
+      },
+      "type": "graph",
+      "xaxis": {
+        "mode": "time",
+        "show": true,
+        "values": []
+      },
+      "yaxes": [
+        {
+          "$$hashKey": "object:60",
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        },
+        {
+          "$$hashKey": "object:61",
+          "format": "short",
+          "logBase": 1,
+          "show": true
+        }
+      ],
+      "yaxis": {
+        "align": false
+      }
+    },
+    {
+      "collapsed": false,
+      "gridPos": {
+        "h": 1,
+        "w": 24,
+        "x": 0,
+        "y": 70
+      },
+      "id": 47,
+      "panels": [],
+      "title": "Real traffic test",
+      "type": "row"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 71
+      },
+      "id": 49,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving real traffic test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"requests\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Requests",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 105
+              },
+              {
+                "color": "red",
+                "value": 115
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 71
+      },
+      "id": 52,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving real traffic test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-min\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: min",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 115
+              },
+              {
+                "color": "red",
+                "value": 120
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 71
+      },
+      "id": 55,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving real traffic test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-mean\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: mean",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "red",
+                "value": 1
+              }
+            ]
+          },
+          "unit": "short"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 0,
+        "y": 74
+      },
+      "id": 50,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving real traffic test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"errors\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Errors",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "description": "",
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 105
+              },
+              {
+                "color": "red",
+                "value": 115
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 7,
+        "y": 74
+      },
+      "id": 54,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving real traffic test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-p95\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: p95 ",
+      "type": "stat"
+    },
+    {
+      "datasource": {
+        "type": "influxdb",
+        "uid": "${ds}"
+      },
+      "fieldConfig": {
+        "defaults": {
+          "color": {
+            "mode": "thresholds"
+          },
+          "mappings": [],
+          "thresholds": {
+            "mode": "absolute",
+            "steps": [
+              {
+                "color": "green",
+                "value": null
+              },
+              {
+                "color": "#EAB839",
+                "value": 3500
+              },
+              {
+                "color": "red",
+                "value": 5000
+              }
+            ]
+          },
+          "unit": "ms"
+        },
+        "overrides": []
+      },
+      "gridPos": {
+        "h": 3,
+        "w": 7,
+        "x": 14,
+        "y": 74
+      },
+      "id": 56,
+      "options": {
+        "colorMode": "value",
+        "graphMode": "area",
+        "justifyMode": "auto",
+        "orientation": "auto",
+        "reduceOptions": {
+          "calcs": [
+            "lastNotNull"
+          ],
+          "fields": "",
+          "values": false
+        },
+        "text": {},
+        "textMode": "auto"
+      },
+      "pluginVersion": "9.1.1",
+      "targets": [
+        {
+          "datasource": {
+            "type": "influxdb",
+            "uid": "${ds}"
+          },
+          "groupBy": [
+            {
+              "params": [
+                "$__interval"
+              ],
+              "type": "time"
+            },
+            {
+              "params": [
+                "null"
+              ],
+              "type": "fill"
+            }
+          ],
+          "orderByTime": "ASC",
+          "policy": "default",
+          "query": "from(bucket: \"knative-serving\")\n  |> range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |> filter(fn: (r) => r[\"_measurement\"] == \"Knative Serving real traffic test\")\n  |> filter(fn: (r) => r[\"_field\"] == \"latency-max\")\n  |> filter(fn: (r) => r[\"BUILD_ID\"] == \"${buildid}\" or \"${buildid}\" == \"all\" )\n  |> filter(fn: (r) => r[\"JOB_NAME\"] == \"${jobname}\" or \"${jobname}\" == \"all\" )\n  |> map(fn: (r) => ({r with _value: r._value / 1000.0 / 1000.0}))  \n  |> aggregateWindow(every: v.windowPeriod, fn: last, createEmpty: false)\n  |> yield(name: \"last\")",
+          "refId": "A",
+          "resultFormat": "time_series",
+          "select": [
+            [
+              {
+                "params": [
+                  "value"
+                ],
+                "type": "field"
+              },
+              {
+                "params": [],
+                "type": "mean"
+              }
+            ]
+          ],
+          "tags": []
+        }
+      ],
+      "title": "Latency: max",
+      "type": "stat"
+    }
+  ],
+  "refresh": "10s",
+  "schemaVersion": 37,
+  "style": "dark",
+  "tags": [],
+  "templating": {
+    "list": [
+      {
+        "allValue": "all",
+        "current": {
+          "selected": false,
+          "text": "All",
+          "value": "$__all"
+        },
+        "datasource": {
+          "type": "influxdb",
+          "uid": "${ds}"
+        },
+        "definition": "import \"influxdata/influxdb/schema\"\nschema.tagValues(\n    bucket: \"knative-serving\",\n    tag: \"BUILD_ID\",\n    start: -1d\n)",
+        "description": "The builds ID according to https://prow.knative.dev/?repo=knative%2Fserving&job=*perf*",
+        "hide": 0,
+        "includeAll": true,
+        "label": "Build ID",
+        "multi": false,
+        "name": "buildid",
+        "options": [],
+        "query": "import \"influxdata/influxdb/schema\"\nschema.tagValues(\n    bucket: \"knative-serving\",\n    tag: \"BUILD_ID\",\n    start: -1d\n)",
+        "refresh": 1,
+        "regex": "",
+        "skipUrlSync": false,
+        "sort": 0,
+        "type": "query"
+      },
+      {
+        "allValue": "all",
+        "current": {
+          "selected": false,
+          "text": "All",
+          "value": "$__all"
+        },
+        "datasource": {
+          "type": "influxdb",
+          "uid": "${ds}"
+        },
+        "definition": "import \"influxdata/influxdb/schema\"\nschema.tagValues(\n    bucket: \"knative-serving\",\n    tag: \"JOB_NAME\",\n    start: -1d\n)",
+        "description": "The prow jobs name according to https://prow.knative.dev/?repo=knative%2Fserving&job=*perf*",
+        "hide": 0,
+        "includeAll": true,
+        "label": "Job name",
+        "multi": false,
+        "name": "jobname",
+        "options": [],
+        "query": "import \"influxdata/influxdb/schema\"\nschema.tagValues(\n    bucket: \"knative-serving\",\n    tag: \"JOB_NAME\",\n    start: -1d\n)",
+        "refresh": 1,
+        "regex": "",
+        "skipUrlSync": false,
+        "sort": 0,
+        "type": "query"
+      },
+      {
+        "current": {
+          "selected": false,
+          "text": "InfluxDB",
+          "value": "InfluxDB"
+        },
+        "description": "Data Source",
+        "hide": 0,
+        "includeAll": false,
+        "label": "DataSource",
+        "multi": false,
+        "name": "ds",
+        "options": [],
+        "query": "influxdb",
+        "queryValue": "",
+        "refresh": 1,
+        "regex": "",
+        "skipUrlSync": false,
+        "type": "datasource"
+      }
+    ]
+  },
+  "time": {
+    "from": "now-6h",
+    "to": "now"
+  },
+  "timepicker": {},
+  "timezone": "",
+  "title": "Knative Serving Performance Tests",
+  "uid": "igHJ5-fdk",
+  "version": 9,
+  "weekStart": ""
+}
diff --git a/test/performance/visualization/setup-influx-db.sh b/test/performance/visualization/setup-influx-db.sh
new file mode 100644
index 000000000..fc85c1381
--- /dev/null
+++ b/test/performance/visualization/setup-influx-db.sh
@@ -0,0 +1,75 @@
+#!/usr/bin/env bash
+
+# Copyright 2023 The Knative Authors
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+# This script configures an influx-db to the state that is expected by the tests
+# To run this, make sure you have set env:
+#   INFLUX_URL
+#   INFLUX_TOKEN
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+declare INFLUX_URL
+declare INFLUX_TOKEN
+
+orgname=Knativetest
+bucketname=knative-serving
+
+if [[ -z "${INFLUX_URL}" ]]; then
+  echo "env variable 'INFLUX_URL' not specified!"
+  exit 1
+fi
+if [[ -z "${INFLUX_TOKEN}" ]]; then
+  echo "env variable 'INFLUX_TOKEN' not specified!"
+  exit 1
+fi
+
+echo "# Checking login"
+curl -X GET "$INFLUX_URL/api/v2" -H "Authorization: Token $INFLUX_TOKEN"
+
+echo "# Setting up organization and bucket"
+status_code=$(curl -s --output /dev/null --write-out %{http_code} -X GET "$INFLUX_URL/api/v2/orgs?org=$orgname" -H "Authorization: Token $INFLUX_TOKEN")
+
+if [[ "$status_code" -ne 404 ]] ; then
+  echo "> Organization already exists"
+else
+  echo "> Creating organization $orgname"
+  curl -X POST "$INFLUX_URL/api/v2/orgs" -H "Authorization: Token $INFLUX_TOKEN" -H "Content-Type: application/json" \
+     --data "
+       {
+         \"name\": \"$orgname\",
+         \"description\": \"Knative Serving Performance tests\"
+       }"
+fi
+
+status_code=$(curl -s --output /dev/null --write-out %{http_code} -X GET "$INFLUX_URL/api/v2/buckets?org=$orgname&name=$bucketname" -H "Authorization: Token $INFLUX_TOKEN")
+
+if [[ "$status_code" -ne 404 ]] ; then
+  echo "> Bucket already exists"
+else
+  echo "> Creating bucket $bucketname"
+  orgID=$(curl -X GET "$INFLUX_URL/api/v2/orgs?org=$orgname" -H "Authorization: Token $INFLUX_TOKEN" | jq -r '.orgs[0].id')
+  curl -X POST "$INFLUX_URL/api/v2/buckets" -H "Authorization: Token $INFLUX_TOKEN" -H "Content-Type: application/json" \
+     --data "
+       {
+         \"name\": \"$bucketname\",
+         \"orgID\": \"$orgID\",
+         \"description\": \"Knative Serving Performance tests\"
+       }"
+fi
+
